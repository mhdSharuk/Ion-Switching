{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ion Switching_5_models",
      "provenance": [],
      "collapsed_sections": [
        "aWYXEdehkaRm",
        "fR6F9755kgDj",
        "ERzklCt6UI5g",
        "AP5vn_VT-ngC",
        "iJv1P2g2RNPw",
        "Q75NF6xTMheC",
        "pKRt1C0w-6MZ",
        "YozJbQObqyvS",
        "7jwFLQA2q9m7",
        "zene-auLQfag",
        "MW_Mdwgorxby",
        "J8DCrNUkU0uY",
        "suVlLZNZAiZE",
        "rn3bDnsFBz2W",
        "MwW365uZDxvK",
        "HqnledmPB4tv"
      ],
      "machine_shape": "hm",
      "mount_file_id": "1jpDwYksxiZ8BGcEO08l_qUKpZwHsgmtw",
      "authorship_tag": "ABX9TyOGRTXBhyGOGo8NZrhR4M8Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhdSharuk/Kaggle-Ion-Switching/blob/master/Ion_Switching_5_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSzABZZskQKH",
        "colab_type": "text"
      },
      "source": [
        "# Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_EIYHDkzlH4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b9dc2426-8d84-458a-de93-5129c60e95f6"
      },
      "source": [
        "import os\n",
        "import gc\n",
        "import pywt\n",
        "import copy\n",
        "import numba\n",
        "import random\n",
        "import logging\n",
        "import librosa\n",
        "import sklearn\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import lightgbm as lgb\n",
        "import tensorflow as tf\n",
        "from scipy import optimize\n",
        "from sklearn import metrics\n",
        "from fbprophet import Prophet\n",
        "from functools import partial\n",
        "from scipy import stats,signal\n",
        "from tqdm import tqdm_notebook\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score\n",
        "from IPython.display import display,HTML\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from keras.initializers import random_normal\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras import losses, models, optimizers\n",
        "from tensorflow.compat.v1.keras.layers import CuDNNLSTM \n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "#from tsfresh.feature_extraction import feature_calculators\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from sklearn.model_selection import KFold, train_test_split, GroupKFold\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler, Callback\n",
        "from tensorflow.keras.layers import Dense, Dropout, Add, Conv1D, Flatten,BatchNormalization, Activation, AveragePooling1D,Bidirectional\n",
        "from tensorflow.keras.layers import GlobalAveragePooling1D, Lambda,Input, Concatenate, UpSampling1D, Multiply,MaxPooling1D,LSTM,TimeDistributed"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wudkVIjxATcn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.set_option('display.max_rows',None)\n",
        "pd.set_option('display.max_columns',None)\n",
        "InteractiveShell.ast_node_interactivity = 'all'\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.rcParams['figure.figsize'] = (20,10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kh_MRWlUkWfj",
        "colab_type": "text"
      },
      "source": [
        "# Connect to Kaggle CLI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhE7o8r7EgA6",
        "colab_type": "code",
        "outputId": "c941c202-9285-4096-c9ee-3b36a9eb41ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "os.chdir('./drive/My Drive/Ion Switching')\n",
        "os.listdir()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sample_submission.csv',\n",
              " 'train.csv.zip',\n",
              " 'test.csv.zip',\n",
              " 'sample_submission.csv.zip',\n",
              " '.ipynb_checkpoints',\n",
              " 'test_clean.csv',\n",
              " 'train_clean.csv',\n",
              " 'kaggle.json',\n",
              " 'submission.csv',\n",
              " 'submission_1.csv',\n",
              " 'submission_2.csv',\n",
              " 'submission_5.csv',\n",
              " 'submission_6.csv',\n",
              " 'submission_7.csv',\n",
              " 'submission_8.csv',\n",
              " 'submission_9.csv',\n",
              " 'submission_10.csv',\n",
              " 'submission_11.csv',\n",
              " 'lgb_cv_0.93933.txt',\n",
              " 'submission_12.csv',\n",
              " 'submission_wavenet_13.csv',\n",
              " 'submission_wavenet_14.csv',\n",
              " 'submission_15.csv',\n",
              " 'submission_shift_rfc_16.csv',\n",
              " 'submission_shift_rfc_random_forest_17.csv',\n",
              " 'librmm.so',\n",
              " 'submission_shift_rfc_random_forest_18.csv',\n",
              " 'Y_test_proba.npy',\n",
              " 'Y_train_proba.npy',\n",
              " 'submission_lstm_19.csv',\n",
              " 'clean-kalman.zip',\n",
              " 'test_clean_kalman.csv',\n",
              " 'train_clean_kalman.csv',\n",
              " 'ion-shifted-rfc-proba.zip',\n",
              " 'submission_lstm_20.csv',\n",
              " 'submission_5_fold_lstm_21.csv']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pCT96Ny0Dhv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZuO25mjnIq9",
        "colab_type": "code",
        "outputId": "d5c64c57-3061-419a-afee-d7794b869f5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "#!kaggle datasets download -d ragnar123/clean-kalman\n",
        "!kaggle datasets download -d sggpls/ion-shifted-rfc-proba"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading ion-shifted-rfc-proba.zip to /content/drive/My Drive/Ion Switching\n",
            " 95% 211M/222M [00:02<00:00, 95.1MB/s]\n",
            "100% 222M/222M [00:02<00:00, 96.3MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6HW0XnDnIj7",
        "colab_type": "code",
        "outputId": "a40f8828-b811-4f1e-bd95-0f36b9abaf25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "#!unzip clean-kalman.zip\n",
        "!unzip ion-shifted-rfc-proba"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ion-shifted-rfc-proba.zip\n",
            "  inflating: Y_test_proba.npy        \n",
            "  inflating: Y_train_proba.npy       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWYXEdehkaRm",
        "colab_type": "text"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PWJYtuDnYWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_features(df):\n",
        "  for x in [1,2,3,4]:\n",
        "    df[f'sig_shift_pos_{x}'] = df['signal'].shift(x).bfill()\n",
        "    df[f'sig_shift_neg_{x}'] = df['signal'].shift(-1*x).ffill()\n",
        "    df[f'sig_pos_diff_{x}'] = df['signal'].diff(x).bfill()\n",
        "    df[f'sig_neg_diff_{x}'] = df['signal'].diff(-1*x).ffill()\n",
        "\n",
        "  df['sig_pct_change'] = df['signal'].pct_change().fillna(0)\n",
        "  for x in [1,2,3,4]:\n",
        "    df[f'sig_pct_change_pos_{x}'] = df['sig_pct_change'].shift(x).bfill()\n",
        "    df[f'sig_pct_change_neg_{x}'] = df['sig_pct_change'].shift(-1*x).ffill()\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wF5DBHuSoMbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(df):\n",
        "  df['batch'] =  df.groupby(df.index//50_000).agg('ngroup').astype(np.int16)\n",
        "  return df\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    tf.random.set_seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kf5JYpC3oMUT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def min_max(df, columns):\n",
        "  for x in columns:\n",
        "    df[x] = (df[x].values - np.min(df[x]))/(df[x].max() - df[x].min())\n",
        "  return df\n",
        "\n",
        "def std_sc(df,columns):\n",
        "  for x in columns:\n",
        "    df[x] = (df[x] - df[x].mean())/df[x].std()\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDUMPe7mzYmK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#For LightGBM\n",
        "def MacroF1Metric(preds, dtrain):\n",
        "    labels = dtrain.get_label()\n",
        "    preds = np.round(np.clip(preds, 0, 10)).astype(int)\n",
        "    score = f1_score(labels, preds, average = 'macro')\n",
        "    return ('MacroF1Metric', score, True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88TZ29RDv57f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_early_prediction(train,test):\n",
        "  dtrain = {'batch':[],\n",
        "     'min':[],\n",
        "     'max':[],\n",
        "     'range':[],\n",
        "     'std':[],\n",
        "     'mean':[],\n",
        "     'num_states':[],\n",
        "    }\n",
        "  for x in train['batch'].unique():\n",
        "    data = train[train['batch'] == x]\n",
        "    dtrain['batch'].append(x)\n",
        "    dtrain['min'].append(data['signal'].min())\n",
        "    dtrain['max'].append(data['signal'].max())\n",
        "    dtrain['range'].append(data['signal'].max() - data['signal'].min())\n",
        "    dtrain['mean'].append(data['signal'].mean())\n",
        "    dtrain['std'].append(data['signal'].std())\n",
        "    dtrain['num_states'].append(data['open_channels'].nunique())\n",
        "  data_train = pd.DataFrame(dtrain)\n",
        "  train['num_states'] = train['batch'].map({x:y for (x,y) in data_train[['batch','num_states']].values})\n",
        "  dtest = {'batch':[],\n",
        "     'min':[],\n",
        "     'max':[],\n",
        "     'range':[],\n",
        "     'std':[],\n",
        "     'mean':[]\n",
        "    }\n",
        "  for x in test['batch'].unique():\n",
        "    data = test[test['batch'] == x]\n",
        "    dtest['batch'].append(x)\n",
        "    dtest['min'].append(data['signal'].min())\n",
        "    dtest['max'].append(data['signal'].max())\n",
        "    dtest['mean'].append(data['signal'].mean())\n",
        "    dtest['std'].append(data['signal'].std())\n",
        "    dtest['range'].append(data['signal'].max() - data['signal'].min())\n",
        "  data_test = pd.DataFrame(dtest)\n",
        "  params = {'learning_rate': 0.09, \n",
        "          'max_depth': -1, \n",
        "          'num_leaves': 250,\n",
        "          'metric': 'rmse', \n",
        "          'random_state': 7, \n",
        "          'n_jobs':-1, \n",
        "          'sample_fraction':0.53}\n",
        "\n",
        "  gc.collect()\n",
        "  use_cols = ['min','max','range','std','mean']\n",
        "  x_train,x_val,y_train,y_val = train_test_split(data_train[use_cols],data_train['num_states'],test_size=0.01)\n",
        "  model = lgb.train(params, lgb.Dataset(x_train, y_train), 2000,  lgb.Dataset(x_val, y_val), verbose_eval=50, early_stopping_rounds=500)\n",
        "  pred = model.predict(data_test[use_cols])\n",
        "  round_pred = np.round(np.clip(pred, 2, 10)).astype(int)\n",
        "  data_test['num_states'] = round_pred\n",
        "  lgb.plot_importance(model,importance_type='split', max_num_features=30)\n",
        "  return_dict = {x:y for (x,y) in data_test[['batch','num_states']].values}\n",
        "  test['num_states'] = test['batch'].map(return_dict)\n",
        "  return train,test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGnjdU1RIePz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reduce_mem_usage(df: pd.DataFrame,verbose: bool = True) -> pd.DataFrame:\n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "\n",
        "        if col_type in numerics:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if (c_min > np.iinfo(np.int8).min\n",
        "                        and c_max < np.iinfo(np.int8).max):\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif (c_min > np.iinfo(np.int16).min\n",
        "                      and c_max < np.iinfo(np.int16).max):\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif (c_min > np.iinfo(np.int32).min\n",
        "                      and c_max < np.iinfo(np.int32).max):\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif (c_min > np.iinfo(np.int64).min\n",
        "                      and c_max < np.iinfo(np.int64).max):\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            else:\n",
        "                if (c_min > np.finfo(np.float16).min\n",
        "                        and c_max < np.finfo(np.float16).max):\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif (c_min > np.finfo(np.float32).min\n",
        "                      and c_max < np.finfo(np.float32).max):\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    reduction = (start_mem - end_mem) / start_mem\n",
        "\n",
        "    msg = f'Mem. usage decreased to {end_mem:5.2f} MB ({reduction * 100:.1f} % reduction)'\n",
        "    if verbose:\n",
        "        print(msg)\n",
        "\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnVw8a63YEGm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check(df,a,b):\n",
        "  return (df[a].values == df[b].values).all()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di4ZSyL2kTKF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def denoise_signal_simple(x, wavelet='db4', level=1):\n",
        "    coeff = pywt.wavedec(x, wavelet, mode=\"per\")\n",
        "    uthresh = 10\n",
        "    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])\n",
        "    return pywt.waverec(coeff, wavelet, mode='per')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrfgzFI-LPUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mean_abs_dev(val):\n",
        "  return np.mean(np.absolute(val - np.mean(val)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YL04GQxNoTBp",
        "colab_type": "text"
      },
      "source": [
        "# Helper Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANhgACF_UPG1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Eval Metric for Deep Neural Network\n",
        "class MacroF1(Callback):\n",
        "    def __init__(self, model, inputs, targets):\n",
        "        self.model = model\n",
        "        self.inputs = inputs\n",
        "        self.targets = np.argmax(targets, axis = 2).reshape(-1)\n",
        "        \n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        pred = np.argmax(self.model.predict(self.inputs), axis = 2).reshape(-1)\n",
        "        score = f1_score(self.targets, pred, average = 'macro')\n",
        "        print(f'   F1 Macro Score: {score:.5f}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJqmqJH2JUz8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(tf.keras.layers.Layer):\n",
        "    def __init__(self, hidden_size, \n",
        "                 num_heads = 8, \n",
        "                 attention_dropout=.1,\n",
        "                 trainable=True,\n",
        "                 name='Attention'):\n",
        "        \n",
        "        if hidden_size % num_heads != 0:\n",
        "            raise ValueError(\"Hidden size must be evenly divisible by the number of heads.\")\n",
        "            \n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_heads = num_heads\n",
        "        self.trainable = trainable\n",
        "        self.attention_dropout = attention_dropout\n",
        "        self.dense = tf.keras.layers.Dense(self.hidden_size, use_bias=False)\n",
        "        super(Attention, self).__init__(name=name)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        with tf.name_scope(\"split_heads\"):\n",
        "            batch_size = tf.shape(x)[0]\n",
        "            length = tf.shape(x)[1]\n",
        "\n",
        "            # Calculate depth of last dimension after it has been split.\n",
        "            depth = (self.hidden_size // self.num_heads)\n",
        "\n",
        "            # Split the last dimension\n",
        "            x = tf.reshape(x, [batch_size, length, self.num_heads, depth])\n",
        "\n",
        "            # Transpose the result\n",
        "            return tf.transpose(x, [0, 2, 1, 3])\n",
        "    \n",
        "    def combine_heads(self, x):\n",
        "        \"\"\"Combine tensor that has been split.\n",
        "        Args:\n",
        "          x: A tensor [batch_size, num_heads, length, hidden_size/num_heads]\n",
        "        Returns:\n",
        "          A tensor with shape [batch_size, length, hidden_size]\n",
        "        \"\"\"\n",
        "        with tf.name_scope(\"combine_heads\"):\n",
        "            batch_size = tf.shape(x)[0]\n",
        "            length = tf.shape(x)[2]\n",
        "            x = tf.transpose(x, [0, 2, 1, 3])  # --> [batch, length, num_heads, depth]\n",
        "            return tf.reshape(x, [batch_size, length, self.hidden_size])        \n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Apply attention mechanism to inputs.\n",
        "        Args:\n",
        "          inputs: a tensor with shape [batch_size, length_x, hidden_size]\n",
        "        Returns:\n",
        "          Attention layer output with shape [batch_size, length_x, hidden_size]\n",
        "        \"\"\"\n",
        "        # Google developper use tf.layer.Dense to linearly project the queries, keys, and values.\n",
        "        q = self.dense(inputs)\n",
        "        k = self.dense(inputs)\n",
        "        v = self.dense(inputs)\n",
        "\n",
        "        q = self.split_heads(q)\n",
        "        k = self.split_heads(k)\n",
        "        v = self.split_heads(v)\n",
        "        \n",
        "        # Scale q to prevent the dot product between q and k from growing too large.\n",
        "        depth = (self.hidden_size // self.num_heads)\n",
        "        q *= depth ** -0.5\n",
        "        \n",
        "        logits = tf.matmul(q, k, transpose_b=True)\n",
        "        # logits += self.bias\n",
        "        weights = tf.nn.softmax(logits, name=\"attention_weights\")\n",
        "        \n",
        "        if self.trainable:\n",
        "            weights = tf.nn.dropout(weights, 1.0 - self.attention_dropout)\n",
        "        \n",
        "        attention_output = tf.matmul(weights, v)\n",
        "        attention_output = self.combine_heads(attention_output)\n",
        "        attention_output = self.dense(attention_output)\n",
        "        return attention_output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tf.TensorShape(input_shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fR6F9755kgDj",
        "colab_type": "text"
      },
      "source": [
        "# Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvenkcI20DOe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('train_clean_kalman.csv')\n",
        "test = pd.read_csv('test_clean_kalman.csv')\n",
        "submission = pd.read_csv('sample_submission.csv.zip')\n",
        "\n",
        "test.reset_index(drop=True,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxkucbSk4fHH",
        "colab_type": "text"
      },
      "source": [
        "#### Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4J90WKxb_sl_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = get_batches(train)\n",
        "test = get_batches(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj7Z-_CYxeYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_groups = {\n",
        "    0:[(0,1000000)],\n",
        "    1:[(1500000,2000000),(3500000,4000000)],\n",
        "    2:[(2500000,3000000),(4000000,4500000)],\n",
        "    3:[(1000000,1500000),(3000000,3500000)],\n",
        "    4:[(2000000,2500000),(4500000,5000000)]\n",
        "}\n",
        "train['group'] = -1\n",
        "for k,v in train_groups.items():\n",
        "    for l,r in v:\n",
        "        train.iloc[l:r,-1] = k\n",
        "\n",
        "\n",
        "test['group'] = -1\n",
        "test_groups = {\n",
        "    0:[(0,100000),(300000,400000),(800000,900000),(1000000,2000000)],\n",
        "    1:[(100000,200000),(900000,1000000)],\n",
        "    2:[(200000,300000),(600000,700000)],\n",
        "    3:[(400000,500000)],\n",
        "    4:[(500000,600000),(700000,800000)]\n",
        "}\n",
        "for k,v in test_groups.items():\n",
        "    for l,r in v:\n",
        "        test.iloc[l:r,-1] = k"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-Pr3QjZDNuT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "append = False\n",
        "if append:\n",
        "  test_num_states = {100.0: 2.0,\n",
        "  101.0: 2.0,\n",
        "  102.0: 4.0,\n",
        "  103.0: 4.0,\n",
        "  104.0: 6.0,\n",
        "  105.0: 6.0,\n",
        "  106.0: 2.0,\n",
        "  107.0: 2.0,\n",
        "  108.0: 2.0,\n",
        "  109.0: 2.0,\n",
        "  110.0: 10.0,\n",
        "  111.0: 10.0,\n",
        "  112.0: 6.0,\n",
        "  113.0: 9.0,\n",
        "  114.0: 10.0,\n",
        "  115.0: 10.0,\n",
        "  116.0: 2.0,\n",
        "  117.0: 2.0,\n",
        "  118.0: 4.0,\n",
        "  119.0: 4.0,\n",
        "  120.0: 2.0,\n",
        "  121.0: 2.0,\n",
        "  122.0: 2.0,\n",
        "  123.0: 2.0,\n",
        "  124.0: 3.0,\n",
        "  125.0: 2.0,\n",
        "  126.0: 2.0,\n",
        "  127.0: 2.0,\n",
        "  128.0: 2.0,\n",
        "  129.0: 2.0,\n",
        "  130.0: 2.0,\n",
        "  131.0: 2.0,\n",
        "  132.0: 2.0,\n",
        "  133.0: 2.0,\n",
        "  134.0: 2.0,\n",
        "  135.0: 2.0,\n",
        "  136.0: 3.0,\n",
        "  137.0: 2.0,\n",
        "  138.0: 2.0,\n",
        "  139.0: 2.0}\n",
        "  test['num_states'] = test['batch'].map(test_num_states)\n",
        "  train,_ = get_early_prediction(train,test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5yMP1CvWuSW",
        "colab_type": "code",
        "outputId": "8fb6afe4-6794-4d3b-fc04-d22f6d86aff6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "train.shape\n",
        "test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000000, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000000, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN3fVXqFTwi4",
        "colab_type": "code",
        "outputId": "2463e695-f207-4326-a868-58fe482d15de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3855"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERzklCt6UI5g",
        "colab_type": "text"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l94YF9MnTEun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = get_features(train)\n",
        "test = get_features(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWLZbTUNppgk",
        "colab_type": "code",
        "outputId": "b074e98a-1db4-4f04-fd85-9a530b2b00e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "indicator = []\n",
        "for x in train.batch.unique():\n",
        "  if(train[train['batch'] == x]['open_channels'].nunique())>=9:\n",
        "    indicator.append(x)\n",
        "len(indicator)\n",
        "\n",
        "train_indicator = {}\n",
        "for x in train.batch.unique():\n",
        "  if x in indicator:\n",
        "    train_indicator[x] = 1\n",
        "  else:\n",
        "    train_indicator[x] = 0\n",
        "\n",
        "test_indicator = {}\n",
        "for x in test.batch.unique():\n",
        "  if x in [10,11,14,15]:\n",
        "    test_indicator[x] = 1\n",
        "  else:\n",
        "    test_indicator[x] = 0\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnQZBNospfKL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train['has_10_states'] = 0\n",
        "test['has_10_states'] = 0\n",
        "\n",
        "train['has_10_states'] = train['batch'].map(train_indicator)\n",
        "test['has_10_states'] = test['batch'].map(test_indicator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3D4Nkw-9qZHu",
        "colab_type": "code",
        "outputId": "060f2b45-d8ba-4936-8a7f-3c93b8473ebb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        }
      },
      "source": [
        "train.head()\n",
        "test.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>signal</th>\n",
              "      <th>open_channels</th>\n",
              "      <th>batch</th>\n",
              "      <th>group</th>\n",
              "      <th>sig_shift_pos_1</th>\n",
              "      <th>sig_shift_neg_1</th>\n",
              "      <th>sig_pos_diff_1</th>\n",
              "      <th>sig_neg_diff_1</th>\n",
              "      <th>sig_shift_pos_2</th>\n",
              "      <th>sig_shift_neg_2</th>\n",
              "      <th>sig_pos_diff_2</th>\n",
              "      <th>sig_neg_diff_2</th>\n",
              "      <th>sig_shift_pos_3</th>\n",
              "      <th>sig_shift_neg_3</th>\n",
              "      <th>sig_pos_diff_3</th>\n",
              "      <th>sig_neg_diff_3</th>\n",
              "      <th>sig_shift_pos_4</th>\n",
              "      <th>sig_shift_neg_4</th>\n",
              "      <th>sig_pos_diff_4</th>\n",
              "      <th>sig_neg_diff_4</th>\n",
              "      <th>sig_pct_change</th>\n",
              "      <th>sig_pct_change_pos_1</th>\n",
              "      <th>sig_pct_change_neg_1</th>\n",
              "      <th>sig_pct_change_pos_2</th>\n",
              "      <th>sig_pct_change_neg_2</th>\n",
              "      <th>sig_pct_change_pos_3</th>\n",
              "      <th>sig_pct_change_neg_3</th>\n",
              "      <th>sig_pct_change_pos_4</th>\n",
              "      <th>sig_pct_change_neg_4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0001</td>\n",
              "      <td>-2.760655</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-2.760655</td>\n",
              "      <td>-2.848034</td>\n",
              "      <td>-0.087379</td>\n",
              "      <td>0.087379</td>\n",
              "      <td>-2.760655</td>\n",
              "      <td>-2.424341</td>\n",
              "      <td>0.336315</td>\n",
              "      <td>-0.336315</td>\n",
              "      <td>-2.760655</td>\n",
              "      <td>-3.130037</td>\n",
              "      <td>-0.369381</td>\n",
              "      <td>0.369381</td>\n",
              "      <td>-2.760655</td>\n",
              "      <td>-3.144854</td>\n",
              "      <td>-0.384199</td>\n",
              "      <td>0.384199</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.031651</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.148767</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.291088</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.004734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0002</td>\n",
              "      <td>-2.848034</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-2.760655</td>\n",
              "      <td>-2.424341</td>\n",
              "      <td>-0.087379</td>\n",
              "      <td>-0.423693</td>\n",
              "      <td>-2.760655</td>\n",
              "      <td>-3.130037</td>\n",
              "      <td>0.336315</td>\n",
              "      <td>0.282003</td>\n",
              "      <td>-2.760655</td>\n",
              "      <td>-3.144854</td>\n",
              "      <td>-0.369381</td>\n",
              "      <td>0.296820</td>\n",
              "      <td>-2.760655</td>\n",
              "      <td>-2.649931</td>\n",
              "      <td>-0.384199</td>\n",
              "      <td>-0.198103</td>\n",
              "      <td>0.031651</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.148767</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.291088</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004734</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.157375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0003</td>\n",
              "      <td>-2.424341</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-2.848034</td>\n",
              "      <td>-3.130037</td>\n",
              "      <td>0.423693</td>\n",
              "      <td>0.705696</td>\n",
              "      <td>-2.760655</td>\n",
              "      <td>-3.144854</td>\n",
              "      <td>0.336315</td>\n",
              "      <td>0.720513</td>\n",
              "      <td>-2.760655</td>\n",
              "      <td>-2.649931</td>\n",
              "      <td>-0.369381</td>\n",
              "      <td>0.225590</td>\n",
              "      <td>-2.760655</td>\n",
              "      <td>-2.697078</td>\n",
              "      <td>-0.384199</td>\n",
              "      <td>0.272737</td>\n",
              "      <td>-0.148767</td>\n",
              "      <td>0.031651</td>\n",
              "      <td>0.291088</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004734</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.157375</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.017792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0004</td>\n",
              "      <td>-3.130037</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-2.424341</td>\n",
              "      <td>-3.144854</td>\n",
              "      <td>-0.705696</td>\n",
              "      <td>0.014817</td>\n",
              "      <td>-2.848034</td>\n",
              "      <td>-2.649931</td>\n",
              "      <td>-0.282003</td>\n",
              "      <td>-0.480106</td>\n",
              "      <td>-2.760655</td>\n",
              "      <td>-2.697078</td>\n",
              "      <td>-0.369381</td>\n",
              "      <td>-0.432959</td>\n",
              "      <td>-2.760655</td>\n",
              "      <td>-2.596101</td>\n",
              "      <td>-0.384199</td>\n",
              "      <td>-0.533936</td>\n",
              "      <td>0.291088</td>\n",
              "      <td>-0.148767</td>\n",
              "      <td>0.004734</td>\n",
              "      <td>0.031651</td>\n",
              "      <td>-0.157375</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.017792</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.037439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0005</td>\n",
              "      <td>-3.144854</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-3.130037</td>\n",
              "      <td>-2.649931</td>\n",
              "      <td>-0.014817</td>\n",
              "      <td>-0.494923</td>\n",
              "      <td>-2.424341</td>\n",
              "      <td>-2.697078</td>\n",
              "      <td>-0.720513</td>\n",
              "      <td>-0.447776</td>\n",
              "      <td>-2.848034</td>\n",
              "      <td>-2.596101</td>\n",
              "      <td>-0.296820</td>\n",
              "      <td>-0.548753</td>\n",
              "      <td>-2.760655</td>\n",
              "      <td>-2.668520</td>\n",
              "      <td>-0.384199</td>\n",
              "      <td>-0.476334</td>\n",
              "      <td>0.004734</td>\n",
              "      <td>0.291088</td>\n",
              "      <td>-0.157375</td>\n",
              "      <td>-0.148767</td>\n",
              "      <td>0.017792</td>\n",
              "      <td>0.031651</td>\n",
              "      <td>-0.037439</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.027895</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     time    signal  open_channels  batch  group  sig_shift_pos_1  \\\n",
              "0  0.0001 -2.760655              0      0      0        -2.760655   \n",
              "1  0.0002 -2.848034              0      0      0        -2.760655   \n",
              "2  0.0003 -2.424341              0      0      0        -2.848034   \n",
              "3  0.0004 -3.130037              0      0      0        -2.424341   \n",
              "4  0.0005 -3.144854              0      0      0        -3.130037   \n",
              "\n",
              "   sig_shift_neg_1  sig_pos_diff_1  sig_neg_diff_1  sig_shift_pos_2  \\\n",
              "0        -2.848034       -0.087379        0.087379        -2.760655   \n",
              "1        -2.424341       -0.087379       -0.423693        -2.760655   \n",
              "2        -3.130037        0.423693        0.705696        -2.760655   \n",
              "3        -3.144854       -0.705696        0.014817        -2.848034   \n",
              "4        -2.649931       -0.014817       -0.494923        -2.424341   \n",
              "\n",
              "   sig_shift_neg_2  sig_pos_diff_2  sig_neg_diff_2  sig_shift_pos_3  \\\n",
              "0        -2.424341        0.336315       -0.336315        -2.760655   \n",
              "1        -3.130037        0.336315        0.282003        -2.760655   \n",
              "2        -3.144854        0.336315        0.720513        -2.760655   \n",
              "3        -2.649931       -0.282003       -0.480106        -2.760655   \n",
              "4        -2.697078       -0.720513       -0.447776        -2.848034   \n",
              "\n",
              "   sig_shift_neg_3  sig_pos_diff_3  sig_neg_diff_3  sig_shift_pos_4  \\\n",
              "0        -3.130037       -0.369381        0.369381        -2.760655   \n",
              "1        -3.144854       -0.369381        0.296820        -2.760655   \n",
              "2        -2.649931       -0.369381        0.225590        -2.760655   \n",
              "3        -2.697078       -0.369381       -0.432959        -2.760655   \n",
              "4        -2.596101       -0.296820       -0.548753        -2.760655   \n",
              "\n",
              "   sig_shift_neg_4  sig_pos_diff_4  sig_neg_diff_4  sig_pct_change  \\\n",
              "0        -3.144854       -0.384199        0.384199        0.000000   \n",
              "1        -2.649931       -0.384199       -0.198103        0.031651   \n",
              "2        -2.697078       -0.384199        0.272737       -0.148767   \n",
              "3        -2.596101       -0.384199       -0.533936        0.291088   \n",
              "4        -2.668520       -0.384199       -0.476334        0.004734   \n",
              "\n",
              "   sig_pct_change_pos_1  sig_pct_change_neg_1  sig_pct_change_pos_2  \\\n",
              "0              0.000000              0.031651              0.000000   \n",
              "1              0.000000             -0.148767              0.000000   \n",
              "2              0.031651              0.291088              0.000000   \n",
              "3             -0.148767              0.004734              0.031651   \n",
              "4              0.291088             -0.157375             -0.148767   \n",
              "\n",
              "   sig_pct_change_neg_2  sig_pct_change_pos_3  sig_pct_change_neg_3  \\\n",
              "0             -0.148767              0.000000              0.291088   \n",
              "1              0.291088              0.000000              0.004734   \n",
              "2              0.004734              0.000000             -0.157375   \n",
              "3             -0.157375              0.000000              0.017792   \n",
              "4              0.017792              0.031651             -0.037439   \n",
              "\n",
              "   sig_pct_change_pos_4  sig_pct_change_neg_4  \n",
              "0                   0.0              0.004734  \n",
              "1                   0.0             -0.157375  \n",
              "2                   0.0              0.017792  \n",
              "3                   0.0             -0.037439  \n",
              "4                   0.0              0.027895  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>signal</th>\n",
              "      <th>batch</th>\n",
              "      <th>group</th>\n",
              "      <th>sig_shift_pos_1</th>\n",
              "      <th>sig_shift_neg_1</th>\n",
              "      <th>sig_pos_diff_1</th>\n",
              "      <th>sig_neg_diff_1</th>\n",
              "      <th>sig_shift_pos_2</th>\n",
              "      <th>sig_shift_neg_2</th>\n",
              "      <th>sig_pos_diff_2</th>\n",
              "      <th>sig_neg_diff_2</th>\n",
              "      <th>sig_shift_pos_3</th>\n",
              "      <th>sig_shift_neg_3</th>\n",
              "      <th>sig_pos_diff_3</th>\n",
              "      <th>sig_neg_diff_3</th>\n",
              "      <th>sig_shift_pos_4</th>\n",
              "      <th>sig_shift_neg_4</th>\n",
              "      <th>sig_pos_diff_4</th>\n",
              "      <th>sig_neg_diff_4</th>\n",
              "      <th>sig_pct_change</th>\n",
              "      <th>sig_pct_change_pos_1</th>\n",
              "      <th>sig_pct_change_neg_1</th>\n",
              "      <th>sig_pct_change_pos_2</th>\n",
              "      <th>sig_pct_change_neg_2</th>\n",
              "      <th>sig_pct_change_pos_3</th>\n",
              "      <th>sig_pct_change_neg_3</th>\n",
              "      <th>sig_pct_change_pos_4</th>\n",
              "      <th>sig_pct_change_neg_4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>500.0001</td>\n",
              "      <td>-2.651296</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-2.651296</td>\n",
              "      <td>-2.846641</td>\n",
              "      <td>-0.195344</td>\n",
              "      <td>0.195344</td>\n",
              "      <td>-2.651296</td>\n",
              "      <td>-2.853836</td>\n",
              "      <td>-0.202539</td>\n",
              "      <td>0.202539</td>\n",
              "      <td>-2.651296</td>\n",
              "      <td>-2.443806</td>\n",
              "      <td>0.207490</td>\n",
              "      <td>-0.207490</td>\n",
              "      <td>-2.651296</td>\n",
              "      <td>-2.612478</td>\n",
              "      <td>0.038818</td>\n",
              "      <td>-0.038818</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.073679</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002528</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.143677</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.069020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>500.0002</td>\n",
              "      <td>-2.846641</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-2.651296</td>\n",
              "      <td>-2.853836</td>\n",
              "      <td>-0.195344</td>\n",
              "      <td>0.007195</td>\n",
              "      <td>-2.651296</td>\n",
              "      <td>-2.443806</td>\n",
              "      <td>-0.202539</td>\n",
              "      <td>-0.402834</td>\n",
              "      <td>-2.651296</td>\n",
              "      <td>-2.612478</td>\n",
              "      <td>0.207490</td>\n",
              "      <td>-0.234162</td>\n",
              "      <td>-2.651296</td>\n",
              "      <td>-2.569241</td>\n",
              "      <td>0.038818</td>\n",
              "      <td>-0.277400</td>\n",
              "      <td>0.073679</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002528</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.143677</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.069020</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.016550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>500.0003</td>\n",
              "      <td>-2.853836</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-2.846641</td>\n",
              "      <td>-2.443806</td>\n",
              "      <td>-0.007195</td>\n",
              "      <td>-0.410030</td>\n",
              "      <td>-2.651296</td>\n",
              "      <td>-2.612478</td>\n",
              "      <td>-0.202539</td>\n",
              "      <td>-0.241357</td>\n",
              "      <td>-2.651296</td>\n",
              "      <td>-2.569241</td>\n",
              "      <td>0.207490</td>\n",
              "      <td>-0.284595</td>\n",
              "      <td>-2.651296</td>\n",
              "      <td>-2.736163</td>\n",
              "      <td>0.038818</td>\n",
              "      <td>-0.117672</td>\n",
              "      <td>0.002528</td>\n",
              "      <td>0.073679</td>\n",
              "      <td>-0.143677</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.069020</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.016550</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.064970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>500.0004</td>\n",
              "      <td>-2.443806</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-2.853836</td>\n",
              "      <td>-2.612478</td>\n",
              "      <td>0.410030</td>\n",
              "      <td>0.168672</td>\n",
              "      <td>-2.846641</td>\n",
              "      <td>-2.569241</td>\n",
              "      <td>0.402834</td>\n",
              "      <td>0.125435</td>\n",
              "      <td>-2.651296</td>\n",
              "      <td>-2.736163</td>\n",
              "      <td>0.207490</td>\n",
              "      <td>0.292357</td>\n",
              "      <td>-2.651296</td>\n",
              "      <td>-2.779321</td>\n",
              "      <td>0.038818</td>\n",
              "      <td>0.335514</td>\n",
              "      <td>-0.143677</td>\n",
              "      <td>0.002528</td>\n",
              "      <td>0.069020</td>\n",
              "      <td>0.073679</td>\n",
              "      <td>-0.016550</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.064970</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.015773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>500.0005</td>\n",
              "      <td>-2.612478</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-2.443806</td>\n",
              "      <td>-2.569241</td>\n",
              "      <td>-0.168672</td>\n",
              "      <td>-0.043238</td>\n",
              "      <td>-2.853836</td>\n",
              "      <td>-2.736163</td>\n",
              "      <td>0.241357</td>\n",
              "      <td>0.123685</td>\n",
              "      <td>-2.846641</td>\n",
              "      <td>-2.779321</td>\n",
              "      <td>0.234162</td>\n",
              "      <td>0.166842</td>\n",
              "      <td>-2.651296</td>\n",
              "      <td>-2.840423</td>\n",
              "      <td>0.038818</td>\n",
              "      <td>0.227945</td>\n",
              "      <td>0.069020</td>\n",
              "      <td>-0.143677</td>\n",
              "      <td>-0.016550</td>\n",
              "      <td>0.002528</td>\n",
              "      <td>0.064970</td>\n",
              "      <td>0.073679</td>\n",
              "      <td>0.015773</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.021985</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       time    signal  batch  group  sig_shift_pos_1  sig_shift_neg_1  \\\n",
              "0  500.0001 -2.651296      0      0        -2.651296        -2.846641   \n",
              "1  500.0002 -2.846641      0      0        -2.651296        -2.853836   \n",
              "2  500.0003 -2.853836      0      0        -2.846641        -2.443806   \n",
              "3  500.0004 -2.443806      0      0        -2.853836        -2.612478   \n",
              "4  500.0005 -2.612478      0      0        -2.443806        -2.569241   \n",
              "\n",
              "   sig_pos_diff_1  sig_neg_diff_1  sig_shift_pos_2  sig_shift_neg_2  \\\n",
              "0       -0.195344        0.195344        -2.651296        -2.853836   \n",
              "1       -0.195344        0.007195        -2.651296        -2.443806   \n",
              "2       -0.007195       -0.410030        -2.651296        -2.612478   \n",
              "3        0.410030        0.168672        -2.846641        -2.569241   \n",
              "4       -0.168672       -0.043238        -2.853836        -2.736163   \n",
              "\n",
              "   sig_pos_diff_2  sig_neg_diff_2  sig_shift_pos_3  sig_shift_neg_3  \\\n",
              "0       -0.202539        0.202539        -2.651296        -2.443806   \n",
              "1       -0.202539       -0.402834        -2.651296        -2.612478   \n",
              "2       -0.202539       -0.241357        -2.651296        -2.569241   \n",
              "3        0.402834        0.125435        -2.651296        -2.736163   \n",
              "4        0.241357        0.123685        -2.846641        -2.779321   \n",
              "\n",
              "   sig_pos_diff_3  sig_neg_diff_3  sig_shift_pos_4  sig_shift_neg_4  \\\n",
              "0        0.207490       -0.207490        -2.651296        -2.612478   \n",
              "1        0.207490       -0.234162        -2.651296        -2.569241   \n",
              "2        0.207490       -0.284595        -2.651296        -2.736163   \n",
              "3        0.207490        0.292357        -2.651296        -2.779321   \n",
              "4        0.234162        0.166842        -2.651296        -2.840423   \n",
              "\n",
              "   sig_pos_diff_4  sig_neg_diff_4  sig_pct_change  sig_pct_change_pos_1  \\\n",
              "0        0.038818       -0.038818        0.000000              0.000000   \n",
              "1        0.038818       -0.277400        0.073679              0.000000   \n",
              "2        0.038818       -0.117672        0.002528              0.073679   \n",
              "3        0.038818        0.335514       -0.143677              0.002528   \n",
              "4        0.038818        0.227945        0.069020             -0.143677   \n",
              "\n",
              "   sig_pct_change_neg_1  sig_pct_change_pos_2  sig_pct_change_neg_2  \\\n",
              "0              0.073679              0.000000              0.002528   \n",
              "1              0.002528              0.000000             -0.143677   \n",
              "2             -0.143677              0.000000              0.069020   \n",
              "3              0.069020              0.073679             -0.016550   \n",
              "4             -0.016550              0.002528              0.064970   \n",
              "\n",
              "   sig_pct_change_pos_3  sig_pct_change_neg_3  sig_pct_change_pos_4  \\\n",
              "0              0.000000             -0.143677                   0.0   \n",
              "1              0.000000              0.069020                   0.0   \n",
              "2              0.000000             -0.016550                   0.0   \n",
              "3              0.000000              0.064970                   0.0   \n",
              "4              0.073679              0.015773                   0.0   \n",
              "\n",
              "   sig_pct_change_neg_4  \n",
              "0              0.069020  \n",
              "1             -0.016550  \n",
              "2              0.064970  \n",
              "3              0.015773  \n",
              "4              0.021985  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP5vn_VT-ngC",
        "colab_type": "text"
      },
      "source": [
        "# Data Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vE-A1jS_kJgR",
        "colab": {}
      },
      "source": [
        "split_value = train.shape[0] - train.shape[0]*0.1\n",
        "split_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0xdpTt6hAFG",
        "colab_type": "code",
        "outputId": "22b8b543-ca4a-4ef9-ec75-641b3e4e5f36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "source": [
        "use_cols = [x for x in train.columns if x not in ['time', 'open_channels','batch', 'batch_slices', 'group']]\n",
        "use_cols"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['signal',\n",
              " 'sig_shift_lead_1',\n",
              " 'sig_shift_lag_1',\n",
              " 'sig_pct_change_lag_1',\n",
              " 'sig_pct_change_lead_1',\n",
              " 'sig_shift_lead_2',\n",
              " 'sig_shift_lag_2',\n",
              " 'sig_pct_change_lag_2',\n",
              " 'sig_pct_change_lead_2',\n",
              " 'sig_shift_lead_3',\n",
              " 'sig_shift_lag_3',\n",
              " 'sig_pct_change_lag_3',\n",
              " 'sig_pct_change_lead_3',\n",
              " 'sig_shift_lead_4',\n",
              " 'sig_shift_lag_4',\n",
              " 'sig_pct_change_lag_4',\n",
              " 'sig_pct_change_lead_4',\n",
              " 'sig_power',\n",
              " 'batch_sig_power_mean',\n",
              " 'batch_sig_power_mean_msig',\n",
              " 'has_10_states']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZfj6oXVVMv6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train,x_val,y_train,y_val = train_test_split(train[use_cols],train['open_channels'],test_size=0.15)\n",
        "gc.collect()\n",
        "print(f'x_train shape => {x_train.shape}, y_val shape => {y_val.shape}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1xxbHSjtKAy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t9A0L_DQQON",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WCTNKTuxk24",
        "colab_type": "text"
      },
      "source": [
        "## Long Short Term Memory (LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDcB6Nlmdxei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_cols = [x for x in train.columns if x not in ['time', 'open_channels','batch', 'batch_slices', 'group']]\n",
        "norm_cols = [x for x in use_cols if x not in ['open_channels','has_10_states']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reUKhIIWsxj2",
        "colab_type": "code",
        "outputId": "8521f547-5c5e-483f-af50-9c69723bbaf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 886
        }
      },
      "source": [
        "SCALE = True\n",
        "if SCALE:\n",
        "  train = std_sc(train,norm_cols)\n",
        "  test = std_sc(test,norm_cols)\n",
        "for x in norm_cols:\n",
        "  print('train',x,train[x].mean(),train[x].std())\n",
        "  print('test',x,test[x].mean(),test[x].std())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train signal 3.493587792391395e-14 0.999999999999989\n",
            "test signal 8.659270744892922e-15 0.9999999999999813\n",
            "train sig_shift_pos_1 2.2886475736783042e-14 0.9999999999999851\n",
            "test sig_shift_pos_1 2.7500393351420628e-14 0.9999999999999947\n",
            "train sig_shift_neg_1 -1.3087271710787718e-15 1.0000000000000488\n",
            "test sig_shift_neg_1 -9.440610515554227e-16 0.999999999999995\n",
            "train sig_pos_diff_1 1.7821522035887937e-18 1.000000000000001\n",
            "test sig_pos_diff_1 -1.793948323225436e-17 0.9999999999999462\n",
            "train sig_neg_diff_1 -2.9183322425296865e-18 0.9999999999999843\n",
            "test sig_neg_diff_1 4.742206627383894e-18 1.0000000000000344\n",
            "train sig_shift_pos_2 -2.7765500965060142e-14 0.9999999999999669\n",
            "test sig_shift_pos_2 1.3757074812659197e-14 1.0000000000000042\n",
            "train sig_shift_neg_2 -8.80271056047377e-15 0.9999999999999597\n",
            "test sig_shift_neg_2 3.2518650439072874e-14 0.9999999999999286\n",
            "train sig_pos_diff_2 9.786549348689278e-18 1.0000000000000588\n",
            "test sig_pos_diff_2 -3.6358416277693095e-19 0.999999999999981\n",
            "train sig_neg_diff_2 -3.446354313041411e-18 1.0000000000000104\n",
            "test sig_neg_diff_2 1.4353421229351682e-17 0.9999999999999442\n",
            "train sig_shift_pos_3 -3.91239855179748e-14 1.000000000000024\n",
            "test sig_shift_pos_3 1.0214761203553024e-14 0.9999999999999861\n",
            "train sig_shift_neg_3 -2.285656002243286e-14 1.0000000000000098\n",
            "test sig_shift_neg_3 2.054655434680086e-14 0.9999999999999967\n",
            "train sig_pos_diff_3 5.917488721252084e-18 1.0000000000000253\n",
            "test sig_pos_diff_3 7.111530114789488e-18 0.9999999999999544\n",
            "train sig_neg_diff_3 -6.1808336226931714e-18 1.000000000000034\n",
            "test sig_neg_diff_3 -1.2068301002629566e-17 0.9999999999999948\n",
            "train sig_shift_pos_4 -1.0751965939803654e-14 0.9999999999999146\n",
            "test sig_shift_pos_4 2.60488179915086e-14 0.9999999999999297\n",
            "train sig_shift_neg_4 -5.987720719247136e-15 1.0000000000000426\n",
            "test sig_shift_neg_4 3.439730977827793e-14 1.000000000000044\n",
            "train sig_pos_diff_4 -1.347668643347788e-17 0.9999999999999958\n",
            "test sig_pos_diff_4 9.635847675326658e-18 0.9999999999999346\n",
            "train sig_neg_diff_4 -4.560263278108323e-18 1.0000000000000118\n",
            "test sig_neg_diff_4 1.9358958880388856e-18 1.0000000000000209\n",
            "train sig_pct_change -9.505203004897544e-18 0.999999999999992\n",
            "test sig_pct_change 7.541276370731653e-17 1.000000000000019\n",
            "train sig_pct_change_pos_1 -1.8633822523178578e-17 0.9999999999999931\n",
            "test sig_pct_change_pos_1 7.807558706539667e-17 1.000000000000019\n",
            "train sig_pct_change_neg_1 -2.0786977607961667e-17 0.9999999999999879\n",
            "test sig_pct_change_neg_1 8.244292336485881e-17 1.000000000000019\n",
            "train sig_pct_change_pos_2 -1.386763984895234e-17 0.9999999999999886\n",
            "test sig_pct_change_pos_2 3.5584728197944615e-17 1.0000000000000198\n",
            "train sig_pct_change_neg_2 -9.014677199159493e-18 0.9999999999999832\n",
            "test sig_pct_change_neg_2 7.861720616956363e-17 1.0000000000000173\n",
            "train sig_pct_change_pos_3 -1.120443506854793e-17 0.9999999999999983\n",
            "test sig_pct_change_pos_3 8.013000636283851e-17 1.000000000000019\n",
            "train sig_pct_change_neg_3 6.4032400475672045e-18 0.9999999999999901\n",
            "test sig_pct_change_neg_3 3.6423357277319544e-17 1.0000000000000167\n",
            "train sig_pct_change_pos_4 2.5871874954896958e-18 1.0000000000000016\n",
            "test sig_pct_change_pos_4 6.651370784444694e-17 1.000000000000019\n",
            "train sig_pct_change_neg_4 -1.76442094875634e-17 0.9999999999999944\n",
            "test sig_pct_change_neg_4 6.29464600018434e-17 1.0000000000000178\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-lVoJaW6SbM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "\"\"\"CHECK = False\n",
        "if CHECK:\n",
        "  train_data = train.loc[train.batch.isin(indicator)][use_cols].drop(columns=['has_10_states']).copy()\n",
        "  train_data_target = pd.get_dummies(train.loc[train.batch.isin(indicator)]['open_channels'])\n",
        "  test_data = test[use_cols].drop(columns=['has_10_states']).copy()\n",
        "else:\n",
        "  train_data = train[use_cols].copy()\n",
        "  train_data_target = pd.get_dummies(train['open_channels'])\n",
        "  test_data = test[use_cols].copy()\n",
        "\n",
        "\n",
        "train_data['time_slice'] = train_data.groupby(train_data.index//50).agg('ngroup').astype(np.int32) \n",
        "train_data_target['time_slice'] = train_data_target.groupby(train_data_target.index//50).agg('ngroup').astype(np.int32)\n",
        "test_data['time_slice'] = test_data.groupby(test_data.index//50).agg('ngroup').astype(np.int32)\n",
        "\n",
        "train_data = np.array(list(train_data.groupby('time_slice').apply(lambda x:x[norm_cols].values)))\n",
        "train_data_target = np.array(list(train_data_target.groupby('time_slice').apply(lambda x:x[np.linspace(0,10,11)].values)))\n",
        "test_data = np.array(list(test_data.groupby('time_slice').apply(lambda x:x[norm_cols].values)))\n",
        "\n",
        "train_data = np.array(train_data).reshape((-1,100,27))\n",
        "train_data_target = np.array(train_data_target).reshape(-1,100,11)\n",
        "test_data = np.array(test_data).reshape((-1,100,27))\n",
        "\n",
        "train_data.shape, train_data_target.shape, test_data.shape\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9y03AofQyhLl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "K.clear_session()\n",
        "config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\n",
        "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\n",
        "tf.compat.v1.keras.backend.set_session(sess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7JHn7HCXgjB",
        "colab_type": "code",
        "outputId": "933b9b90-4575-46ec-b622-bd2a31a6f9e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "train['group'] = train.groupby(train.index//4000).agg('ngroup').astype(np.int32)\n",
        "test['group'] = test.groupby(test.index//4000).agg('ngroup').astype(np.int32)\n",
        "group = train['group']\n",
        "\n",
        "kf = GroupKFold(n_splits=5)\n",
        "splits = [x for x in kf.split(train, train['open_channels'], group)]\n",
        "new_splits = []\n",
        "for sp in splits:\n",
        "    new_split = []\n",
        "    new_split.append(np.unique(group[sp[0]]))\n",
        "    new_split.append(np.unique(group[sp[1]]))\n",
        "    new_split.append(sp[1])    \n",
        "    new_splits.append(new_split)\n",
        "\n",
        "\n",
        "cols = [x for x in train.columns if x not in ['group','time','open_channels','batch']]\n",
        "tr = pd.concat([pd.get_dummies(train.open_channels), train[['group']]], axis=1)\n",
        "tr.columns = ['target_'+str(i) for i in range(11)] + ['group']\n",
        "target_cols = ['target_'+str(i) for i in range(11)]\n",
        "\n",
        "train_data_target = np.array(list(tr.groupby('group').apply(lambda x: x[target_cols].values))).astype(np.float32)\n",
        "train_data = np.array(list(train.groupby('group').apply(lambda x: x[use_cols].values)))\n",
        "test_data = np.array(list(test.groupby('group').apply(lambda x: x[use_cols].values)))\n",
        "\n",
        "train_data.shape, train_data_target.shape, test_data.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1250, 4000, 26), (1250, 4000, 11), (500, 4000, 26))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qCmuCunnVeh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',min_delta=0.0001,patience=10,verbose=1)\n",
        "lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',factor=0.25,patience=10,verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82Ct0XlAPpa_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    if epoch < 40:\n",
        "        lr = LR\n",
        "    elif epoch < 50:\n",
        "        lr = LR / 3\n",
        "    elif epoch < 60:\n",
        "        lr = LR / 6\n",
        "    elif epoch < 75:\n",
        "        lr = LR / 9\n",
        "    elif epoch < 85:\n",
        "        lr = LR / 12\n",
        "    elif epoch < 100:\n",
        "        lr = LR / 15\n",
        "    else:\n",
        "        lr = LR / 50\n",
        "    return lr\n",
        "\n",
        "LR = 0.001\n",
        "cb_lr_schedule = LearningRateScheduler(lr_schedule)\n",
        "opt = optimizers.Adam(lr=LR,decay=1e-6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsrpGVAexyBA",
        "colab_type": "code",
        "outputId": "b4956319-be86-49e2-8b80-13cfd6b4ddd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        }
      },
      "source": [
        "LR = 0.001\n",
        "opt = optimizers.Adam(lr=LR,decay=1e-6)\n",
        "model = Sequential()\n",
        "model.add(CuDNNLSTM(200, return_sequences=True, input_shape=train_data.shape[1:]))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(CuDNNLSTM(100, return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(CuDNNLSTM(50, return_sequences=True))\n",
        "model.add(TimeDistributed(Dense(25, activation='relu')))\n",
        "model.add(BatchNormalization())\n",
        "model.add(TimeDistributed(Dense(11,activation='softmax')))\n",
        "model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "cu_dnnlstm (CuDNNLSTM)       (None, 4000, 200)         182400    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 4000, 200)         0         \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_1 (CuDNNLSTM)     (None, 4000, 100)         120800    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 4000, 100)         0         \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_2 (CuDNNLSTM)     (None, 4000, 50)          30400     \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 4000, 25)          1275      \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 4000, 25)          100       \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 4000, 11)          286       \n",
            "=================================================================\n",
            "Total params: 335,261\n",
            "Trainable params: 335,211\n",
            "Non-trainable params: 50\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-OVByfim9S_",
        "colab_type": "code",
        "outputId": "34d8955a-6345-4e16-8750-1568f0790308",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "seed_everything(42)\n",
        "oof_ = np.zeros((len(train), 11)) \n",
        "preds_ = np.zeros((len(test), 11))\n",
        "for n_fold, (tr_idx, val_idx, val_orig_idx) in enumerate(new_splits[0:], start=0):\n",
        "  print(f'On Fold :{n_fold}')\n",
        "  train_x, train_y = train_data[tr_idx], train_data_target[tr_idx]\n",
        "  valid_x, valid_y = train_data[val_idx], train_data_target[val_idx]\n",
        "  print(f'Our training dataset shape is {train_x.shape}')\n",
        "  print(f'Our validation dataset shape is {valid_x.shape}')\n",
        "  model.fit(train_x,train_y,\n",
        "            epochs=120,\n",
        "            callbacks=[early_stopping,lr_schedule,MacroF1(model,valid_x,valid_y)],\n",
        "            batch_size=8,\n",
        "            validation_data=(valid_x,valid_y))\n",
        "  preds_f = model.predict(valid_x)\n",
        "  f1_score_ = f1_score(np.argmax(valid_y, axis=2).reshape(-1),  np.argmax(preds_f, axis=2).reshape(-1), average = 'macro')\n",
        "  print(f'Training fold {n_fold + 1} completed. macro f1 score : {f1_score_ :1.5f}')\n",
        "  preds_f = preds_f.reshape(-1, preds_f.shape[-1])\n",
        "  oof_[val_orig_idx,:] += preds_f\n",
        "  te_preds = model.predict(test_data)\n",
        "  te_preds = te_preds.reshape(-1, te_preds.shape[-1])\n",
        "  preds_ += te_preds\n",
        "preds_ = preds_ / 5\n",
        "f1_score_ = f1_score(np.argmax(train_data_target, axis = 2).reshape(-1),  np.argmax(oof_, axis = 1), average = 'macro') \n",
        "print(f'Training completed. oof macro f1 score : {f1_score_:1.5f}')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On Fold :0\n",
            "Our training dataset shape is (1000, 4000, 26)\n",
            "Our validation dataset shape is (250, 4000, 26)\n",
            "Epoch 1/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.5398 - accuracy: 0.8211   F1 Macro Score: 0.76652\n",
            "125/125 [==============================] - 46s 366ms/step - loss: 0.5398 - accuracy: 0.8211 - val_loss: 0.3963 - val_accuracy: 0.8969 - lr: 0.0010\n",
            "Epoch 2/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.2657 - accuracy: 0.9181   F1 Macro Score: 0.88053\n",
            "125/125 [==============================] - 41s 331ms/step - loss: 0.2657 - accuracy: 0.9181 - val_loss: 0.2120 - val_accuracy: 0.9339 - lr: 0.0010\n",
            "Epoch 3/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.4494 - accuracy: 0.8514   F1 Macro Score: 0.58353\n",
            "125/125 [==============================] - 41s 331ms/step - loss: 0.4494 - accuracy: 0.8514 - val_loss: 0.5318 - val_accuracy: 0.8087 - lr: 0.0010\n",
            "Epoch 4/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.3281 - accuracy: 0.8929   F1 Macro Score: 0.81156\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.3281 - accuracy: 0.8929 - val_loss: 0.2131 - val_accuracy: 0.9325 - lr: 0.0010\n",
            "Epoch 5/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1919 - accuracy: 0.9367   F1 Macro Score: 0.85304\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.1919 - accuracy: 0.9367 - val_loss: 0.2011 - val_accuracy: 0.9306 - lr: 0.0010\n",
            "Epoch 6/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1672 - accuracy: 0.9453   F1 Macro Score: 0.90234\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.1672 - accuracy: 0.9453 - val_loss: 0.1299 - val_accuracy: 0.9560 - lr: 0.0010\n",
            "Epoch 7/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1653 - accuracy: 0.9450   F1 Macro Score: 0.92216\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.1653 - accuracy: 0.9450 - val_loss: 0.1199 - val_accuracy: 0.9592 - lr: 0.0010\n",
            "Epoch 8/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1374 - accuracy: 0.9524   F1 Macro Score: 0.92007\n",
            "125/125 [==============================] - 41s 331ms/step - loss: 0.1374 - accuracy: 0.9524 - val_loss: 0.1179 - val_accuracy: 0.9592 - lr: 0.0010\n",
            "Epoch 9/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1357 - accuracy: 0.9532   F1 Macro Score: 0.86941\n",
            "125/125 [==============================] - 41s 331ms/step - loss: 0.1357 - accuracy: 0.9532 - val_loss: 0.1562 - val_accuracy: 0.9443 - lr: 0.0010\n",
            "Epoch 10/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1279 - accuracy: 0.9546   F1 Macro Score: 0.92744\n",
            "125/125 [==============================] - 41s 331ms/step - loss: 0.1279 - accuracy: 0.9546 - val_loss: 0.1127 - val_accuracy: 0.9599 - lr: 0.0010\n",
            "Epoch 11/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1531 - accuracy: 0.9482   F1 Macro Score: 0.92378\n",
            "125/125 [==============================] - 41s 331ms/step - loss: 0.1531 - accuracy: 0.9482 - val_loss: 0.1197 - val_accuracy: 0.9577 - lr: 0.0010\n",
            "Epoch 12/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1220 - accuracy: 0.9564   F1 Macro Score: 0.93170\n",
            "125/125 [==============================] - 41s 331ms/step - loss: 0.1220 - accuracy: 0.9564 - val_loss: 0.1059 - val_accuracy: 0.9622 - lr: 0.0010\n",
            "Epoch 13/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1192 - accuracy: 0.9576   F1 Macro Score: 0.92733\n",
            "125/125 [==============================] - 41s 331ms/step - loss: 0.1192 - accuracy: 0.9576 - val_loss: 0.1218 - val_accuracy: 0.9570 - lr: 0.0010\n",
            "Epoch 14/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1461 - accuracy: 0.9490   F1 Macro Score: 0.93209\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.1461 - accuracy: 0.9490 - val_loss: 0.1063 - val_accuracy: 0.9620 - lr: 0.0010\n",
            "Epoch 15/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1198 - accuracy: 0.9569   F1 Macro Score: 0.93221\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.1198 - accuracy: 0.9569 - val_loss: 0.1064 - val_accuracy: 0.9618 - lr: 0.0010\n",
            "Epoch 16/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1155 - accuracy: 0.9579   F1 Macro Score: 0.93082\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.1155 - accuracy: 0.9579 - val_loss: 0.1052 - val_accuracy: 0.9619 - lr: 0.0010\n",
            "Epoch 17/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1135 - accuracy: 0.9585   F1 Macro Score: 0.93409\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.1135 - accuracy: 0.9585 - val_loss: 0.1000 - val_accuracy: 0.9640 - lr: 0.0010\n",
            "Epoch 18/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1198 - accuracy: 0.9566   F1 Macro Score: 0.93431\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.1198 - accuracy: 0.9566 - val_loss: 0.1007 - val_accuracy: 0.9639 - lr: 0.0010\n",
            "Epoch 19/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1174 - accuracy: 0.9566   F1 Macro Score: 0.93143\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.1174 - accuracy: 0.9566 - val_loss: 0.1025 - val_accuracy: 0.9630 - lr: 0.0010\n",
            "Epoch 20/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1073 - accuracy: 0.9604   F1 Macro Score: 0.93438\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.1073 - accuracy: 0.9604 - val_loss: 0.1003 - val_accuracy: 0.9636 - lr: 0.0010\n",
            "Epoch 21/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1086 - accuracy: 0.9601   F1 Macro Score: 0.93347\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.1086 - accuracy: 0.9601 - val_loss: 0.1022 - val_accuracy: 0.9629 - lr: 0.0010\n",
            "Epoch 22/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1044 - accuracy: 0.9613   F1 Macro Score: 0.93362\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.1044 - accuracy: 0.9613 - val_loss: 0.0992 - val_accuracy: 0.9641 - lr: 0.0010\n",
            "Epoch 23/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1055 - accuracy: 0.9609   F1 Macro Score: 0.92990\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.1055 - accuracy: 0.9609 - val_loss: 0.1054 - val_accuracy: 0.9614 - lr: 0.0010\n",
            "Epoch 24/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1062 - accuracy: 0.9606   F1 Macro Score: 0.93198\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.1062 - accuracy: 0.9606 - val_loss: 0.1036 - val_accuracy: 0.9628 - lr: 0.0010\n",
            "Epoch 25/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1027 - accuracy: 0.9616   F1 Macro Score: 0.93295\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.1027 - accuracy: 0.9616 - val_loss: 0.1003 - val_accuracy: 0.9636 - lr: 0.0010\n",
            "Epoch 26/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1049 - accuracy: 0.9611   F1 Macro Score: 0.93285\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.1049 - accuracy: 0.9611 - val_loss: 0.1012 - val_accuracy: 0.9633 - lr: 0.0010\n",
            "Epoch 27/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1019 - accuracy: 0.9619   F1 Macro Score: 0.93530\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.1019 - accuracy: 0.9619 - val_loss: 0.0972 - val_accuracy: 0.9647 - lr: 0.0010\n",
            "Epoch 28/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1043 - accuracy: 0.9611   F1 Macro Score: 0.93416\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.1043 - accuracy: 0.9611 - val_loss: 0.0986 - val_accuracy: 0.9642 - lr: 0.0010\n",
            "Epoch 29/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1027 - accuracy: 0.9617   F1 Macro Score: 0.92530\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.1027 - accuracy: 0.9617 - val_loss: 0.1049 - val_accuracy: 0.9620 - lr: 0.0010\n",
            "Epoch 30/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1022 - accuracy: 0.9617   F1 Macro Score: 0.93168\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.1022 - accuracy: 0.9617 - val_loss: 0.0992 - val_accuracy: 0.9641 - lr: 0.0010\n",
            "Epoch 31/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1007 - accuracy: 0.9622   F1 Macro Score: 0.92764\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.1007 - accuracy: 0.9622 - val_loss: 0.1076 - val_accuracy: 0.9607 - lr: 0.0010\n",
            "Epoch 32/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1900 - accuracy: 0.9359   F1 Macro Score: 0.92221\n",
            "125/125 [==============================] - 42s 332ms/step - loss: 0.1900 - accuracy: 0.9359 - val_loss: 0.1071 - val_accuracy: 0.9614 - lr: 0.0010\n",
            "Epoch 33/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1095 - accuracy: 0.9596   F1 Macro Score: 0.93120\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.1095 - accuracy: 0.9596 - val_loss: 0.1042 - val_accuracy: 0.9622 - lr: 0.0010\n",
            "Epoch 34/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1048 - accuracy: 0.9608   F1 Macro Score: 0.93257\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.1048 - accuracy: 0.9608 - val_loss: 0.1014 - val_accuracy: 0.9631 - lr: 0.0010\n",
            "Epoch 35/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1035 - accuracy: 0.9617   F1 Macro Score: 0.93337\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.1035 - accuracy: 0.9617 - val_loss: 0.0986 - val_accuracy: 0.9643 - lr: 0.0010\n",
            "Epoch 36/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0996 - accuracy: 0.9628   F1 Macro Score: 0.93515\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0996 - accuracy: 0.9628 - val_loss: 0.0974 - val_accuracy: 0.9648 - lr: 0.0010\n",
            "Epoch 37/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1008 - accuracy: 0.9624   F1 Macro Score: 0.93596\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.1008 - accuracy: 0.9624 - val_loss: 0.0963 - val_accuracy: 0.9652 - lr: 0.0010\n",
            "Epoch 38/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0999 - accuracy: 0.9628   F1 Macro Score: 0.93325\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0999 - accuracy: 0.9628 - val_loss: 0.0977 - val_accuracy: 0.9645 - lr: 0.0010\n",
            "Epoch 39/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0976 - accuracy: 0.9631   F1 Macro Score: 0.93392\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0976 - accuracy: 0.9631 - val_loss: 0.0986 - val_accuracy: 0.9643 - lr: 0.0010\n",
            "Epoch 40/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0984 - accuracy: 0.9631   F1 Macro Score: 0.92818\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0984 - accuracy: 0.9631 - val_loss: 0.1011 - val_accuracy: 0.9632 - lr: 0.0010\n",
            "Epoch 41/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0993 - accuracy: 0.9627   F1 Macro Score: 0.92762\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0993 - accuracy: 0.9627 - val_loss: 0.1035 - val_accuracy: 0.9621 - lr: 0.0010\n",
            "Epoch 42/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0975 - accuracy: 0.9631   F1 Macro Score: 0.93307\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0975 - accuracy: 0.9631 - val_loss: 0.1009 - val_accuracy: 0.9635 - lr: 0.0010\n",
            "Epoch 43/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0968 - accuracy: 0.9635   F1 Macro Score: 0.93114\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0968 - accuracy: 0.9635 - val_loss: 0.1024 - val_accuracy: 0.9628 - lr: 0.0010\n",
            "Epoch 44/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0990 - accuracy: 0.9627   F1 Macro Score: 0.92996\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0990 - accuracy: 0.9627 - val_loss: 0.1000 - val_accuracy: 0.9639 - lr: 0.0010\n",
            "Epoch 45/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0966 - accuracy: 0.9637   F1 Macro Score: 0.93506\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0966 - accuracy: 0.9637 - val_loss: 0.0966 - val_accuracy: 0.9650 - lr: 0.0010\n",
            "Epoch 46/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0954 - accuracy: 0.9640   F1 Macro Score: 0.93392\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0954 - accuracy: 0.9640 - val_loss: 0.0988 - val_accuracy: 0.9640 - lr: 0.0010\n",
            "Epoch 47/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0949 - accuracy: 0.9641\n",
            "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "   F1 Macro Score: 0.93246\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0949 - accuracy: 0.9641 - val_loss: 0.0976 - val_accuracy: 0.9646 - lr: 0.0010\n",
            "Epoch 00047: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe74bf7ad68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        },
        {
          "output_type": "stream",
          "text": [
            "Training fold 1 completed. macro f1 score : 0.93246\n",
            "On Fold :1\n",
            "Our training dataset shape is (1000, 4000, 26)\n",
            "Our validation dataset shape is (250, 4000, 26)\n",
            "Epoch 1/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0951 - accuracy: 0.9642   F1 Macro Score: 0.93852\n",
            "125/125 [==============================] - 41s 331ms/step - loss: 0.0951 - accuracy: 0.9642 - val_loss: 0.0857 - val_accuracy: 0.9691 - lr: 2.5000e-04\n",
            "Epoch 2/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0938 - accuracy: 0.9645   F1 Macro Score: 0.93870\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0938 - accuracy: 0.9645 - val_loss: 0.0856 - val_accuracy: 0.9691 - lr: 2.5000e-04\n",
            "Epoch 3/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0944 - accuracy: 0.9642   F1 Macro Score: 0.93794\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0944 - accuracy: 0.9642 - val_loss: 0.0857 - val_accuracy: 0.9690 - lr: 2.5000e-04\n",
            "Epoch 4/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0941 - accuracy: 0.9645   F1 Macro Score: 0.93856\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0941 - accuracy: 0.9645 - val_loss: 0.0861 - val_accuracy: 0.9689 - lr: 2.5000e-04\n",
            "Epoch 5/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0932 - accuracy: 0.9646   F1 Macro Score: 0.93879\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0932 - accuracy: 0.9646 - val_loss: 0.0857 - val_accuracy: 0.9690 - lr: 2.5000e-04\n",
            "Epoch 6/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0943 - accuracy: 0.9644   F1 Macro Score: 0.93860\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0943 - accuracy: 0.9644 - val_loss: 0.0858 - val_accuracy: 0.9690 - lr: 2.5000e-04\n",
            "Epoch 7/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0935 - accuracy: 0.9646   F1 Macro Score: 0.93879\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0935 - accuracy: 0.9646 - val_loss: 0.0857 - val_accuracy: 0.9691 - lr: 2.5000e-04\n",
            "Epoch 8/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0933 - accuracy: 0.9645   F1 Macro Score: 0.93854\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0933 - accuracy: 0.9645 - val_loss: 0.0859 - val_accuracy: 0.9690 - lr: 2.5000e-04\n",
            "Epoch 9/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0936 - accuracy: 0.9645   F1 Macro Score: 0.93834\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0936 - accuracy: 0.9645 - val_loss: 0.0859 - val_accuracy: 0.9689 - lr: 2.5000e-04\n",
            "Epoch 10/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0934 - accuracy: 0.9646   F1 Macro Score: 0.93754\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0934 - accuracy: 0.9646 - val_loss: 0.0866 - val_accuracy: 0.9687 - lr: 2.5000e-04\n",
            "Epoch 11/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0934 - accuracy: 0.9646\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "   F1 Macro Score: 0.93826\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0934 - accuracy: 0.9646 - val_loss: 0.0867 - val_accuracy: 0.9687 - lr: 2.5000e-04\n",
            "Epoch 00011: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe82e06ec18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        },
        {
          "output_type": "stream",
          "text": [
            "Training fold 2 completed. macro f1 score : 0.93826\n",
            "On Fold :2\n",
            "Our training dataset shape is (1000, 4000, 26)\n",
            "Our validation dataset shape is (250, 4000, 26)\n",
            "Epoch 1/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0924 - accuracy: 0.9650   F1 Macro Score: 0.93629\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0924 - accuracy: 0.9650 - val_loss: 0.0876 - val_accuracy: 0.9681 - lr: 6.2500e-05\n",
            "Epoch 2/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0918 - accuracy: 0.9651   F1 Macro Score: 0.93648\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0918 - accuracy: 0.9651 - val_loss: 0.0876 - val_accuracy: 0.9682 - lr: 6.2500e-05\n",
            "Epoch 3/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0920 - accuracy: 0.9652   F1 Macro Score: 0.93644\n",
            "125/125 [==============================] - 42s 332ms/step - loss: 0.0920 - accuracy: 0.9652 - val_loss: 0.0878 - val_accuracy: 0.9681 - lr: 6.2500e-05\n",
            "Epoch 4/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0920 - accuracy: 0.9651   F1 Macro Score: 0.93653\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0920 - accuracy: 0.9651 - val_loss: 0.0881 - val_accuracy: 0.9681 - lr: 6.2500e-05\n",
            "Epoch 5/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0915 - accuracy: 0.9651   F1 Macro Score: 0.93638\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0915 - accuracy: 0.9651 - val_loss: 0.0876 - val_accuracy: 0.9682 - lr: 6.2500e-05\n",
            "Epoch 6/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0923 - accuracy: 0.9652   F1 Macro Score: 0.93650\n",
            "125/125 [==============================] - 42s 332ms/step - loss: 0.0923 - accuracy: 0.9652 - val_loss: 0.0877 - val_accuracy: 0.9681 - lr: 6.2500e-05\n",
            "Epoch 7/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0921 - accuracy: 0.9651   F1 Macro Score: 0.93634\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0921 - accuracy: 0.9651 - val_loss: 0.0877 - val_accuracy: 0.9682 - lr: 6.2500e-05\n",
            "Epoch 8/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0914 - accuracy: 0.9652   F1 Macro Score: 0.93635\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0914 - accuracy: 0.9652 - val_loss: 0.0878 - val_accuracy: 0.9681 - lr: 6.2500e-05\n",
            "Epoch 9/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0920 - accuracy: 0.9651   F1 Macro Score: 0.93634\n",
            "125/125 [==============================] - 42s 332ms/step - loss: 0.0920 - accuracy: 0.9651 - val_loss: 0.0877 - val_accuracy: 0.9681 - lr: 6.2500e-05\n",
            "Epoch 10/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0916 - accuracy: 0.9652   F1 Macro Score: 0.93618\n",
            "125/125 [==============================] - 42s 332ms/step - loss: 0.0916 - accuracy: 0.9652 - val_loss: 0.0880 - val_accuracy: 0.9681 - lr: 6.2500e-05\n",
            "Epoch 11/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0916 - accuracy: 0.9652\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "   F1 Macro Score: 0.93629\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0916 - accuracy: 0.9652 - val_loss: 0.0879 - val_accuracy: 0.9681 - lr: 6.2500e-05\n",
            "Epoch 00011: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe7d6556e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        },
        {
          "output_type": "stream",
          "text": [
            "Training fold 3 completed. macro f1 score : 0.93629\n",
            "On Fold :3\n",
            "Our training dataset shape is (1000, 4000, 26)\n",
            "Our validation dataset shape is (250, 4000, 26)\n",
            "Epoch 1/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0913 - accuracy: 0.9654   F1 Macro Score: 0.93747\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0913 - accuracy: 0.9654 - val_loss: 0.0888 - val_accuracy: 0.9676 - lr: 1.5625e-05\n",
            "Epoch 2/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0907 - accuracy: 0.9655   F1 Macro Score: 0.93740\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0907 - accuracy: 0.9655 - val_loss: 0.0887 - val_accuracy: 0.9675 - lr: 1.5625e-05\n",
            "Epoch 3/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0911 - accuracy: 0.9655   F1 Macro Score: 0.93731\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0911 - accuracy: 0.9655 - val_loss: 0.0889 - val_accuracy: 0.9675 - lr: 1.5625e-05\n",
            "Epoch 4/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0911 - accuracy: 0.9655   F1 Macro Score: 0.93744\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0911 - accuracy: 0.9655 - val_loss: 0.0889 - val_accuracy: 0.9675 - lr: 1.5625e-05\n",
            "Epoch 5/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0904 - accuracy: 0.9656   F1 Macro Score: 0.93738\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0904 - accuracy: 0.9656 - val_loss: 0.0887 - val_accuracy: 0.9675 - lr: 1.5625e-05\n",
            "Epoch 6/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0911 - accuracy: 0.9655   F1 Macro Score: 0.93727\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0911 - accuracy: 0.9655 - val_loss: 0.0889 - val_accuracy: 0.9675 - lr: 1.5625e-05\n",
            "Epoch 7/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0908 - accuracy: 0.9655   F1 Macro Score: 0.93739\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0908 - accuracy: 0.9655 - val_loss: 0.0888 - val_accuracy: 0.9675 - lr: 1.5625e-05\n",
            "Epoch 8/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0906 - accuracy: 0.9656   F1 Macro Score: 0.93723\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0906 - accuracy: 0.9656 - val_loss: 0.0889 - val_accuracy: 0.9674 - lr: 1.5625e-05\n",
            "Epoch 9/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0908 - accuracy: 0.9656   F1 Macro Score: 0.93758\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0908 - accuracy: 0.9656 - val_loss: 0.0890 - val_accuracy: 0.9674 - lr: 1.5625e-05\n",
            "Epoch 10/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0909 - accuracy: 0.9654   F1 Macro Score: 0.93716\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0909 - accuracy: 0.9654 - val_loss: 0.0889 - val_accuracy: 0.9674 - lr: 1.5625e-05\n",
            "Epoch 11/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0907 - accuracy: 0.9655\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "   F1 Macro Score: 0.93709\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0907 - accuracy: 0.9655 - val_loss: 0.0890 - val_accuracy: 0.9674 - lr: 1.5625e-05\n",
            "Epoch 00011: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe7d6498128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        },
        {
          "output_type": "stream",
          "text": [
            "Training fold 4 completed. macro f1 score : 0.93709\n",
            "On Fold :4\n",
            "Our training dataset shape is (1000, 4000, 26)\n",
            "Our validation dataset shape is (250, 4000, 26)\n",
            "Epoch 1/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0913 - accuracy: 0.9654   F1 Macro Score: 0.93768\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0913 - accuracy: 0.9654 - val_loss: 0.0887 - val_accuracy: 0.9678 - lr: 3.9063e-06\n",
            "Epoch 2/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0908 - accuracy: 0.9654   F1 Macro Score: 0.93760\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0908 - accuracy: 0.9654 - val_loss: 0.0887 - val_accuracy: 0.9677 - lr: 3.9063e-06\n",
            "Epoch 3/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0909 - accuracy: 0.9655   F1 Macro Score: 0.93762\n",
            "125/125 [==============================] - 42s 332ms/step - loss: 0.0909 - accuracy: 0.9655 - val_loss: 0.0887 - val_accuracy: 0.9677 - lr: 3.9063e-06\n",
            "Epoch 4/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0910 - accuracy: 0.9654   F1 Macro Score: 0.93762\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0910 - accuracy: 0.9654 - val_loss: 0.0887 - val_accuracy: 0.9677 - lr: 3.9063e-06\n",
            "Epoch 5/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0907 - accuracy: 0.9655   F1 Macro Score: 0.93769\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0907 - accuracy: 0.9655 - val_loss: 0.0887 - val_accuracy: 0.9677 - lr: 3.9063e-06\n",
            "Epoch 6/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0911 - accuracy: 0.9655   F1 Macro Score: 0.93765\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0911 - accuracy: 0.9655 - val_loss: 0.0887 - val_accuracy: 0.9677 - lr: 3.9063e-06\n",
            "Epoch 7/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0911 - accuracy: 0.9655   F1 Macro Score: 0.93770\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0911 - accuracy: 0.9655 - val_loss: 0.0887 - val_accuracy: 0.9677 - lr: 3.9063e-06\n",
            "Epoch 8/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0905 - accuracy: 0.9656   F1 Macro Score: 0.93765\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0905 - accuracy: 0.9656 - val_loss: 0.0887 - val_accuracy: 0.9677 - lr: 3.9063e-06\n",
            "Epoch 9/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0908 - accuracy: 0.9656   F1 Macro Score: 0.93765\n",
            "125/125 [==============================] - 42s 332ms/step - loss: 0.0908 - accuracy: 0.9656 - val_loss: 0.0887 - val_accuracy: 0.9677 - lr: 3.9063e-06\n",
            "Epoch 10/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0910 - accuracy: 0.9654   F1 Macro Score: 0.93770\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0910 - accuracy: 0.9654 - val_loss: 0.0887 - val_accuracy: 0.9677 - lr: 3.9063e-06\n",
            "Epoch 11/120\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0908 - accuracy: 0.9655\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "   F1 Macro Score: 0.93765\n",
            "125/125 [==============================] - 41s 332ms/step - loss: 0.0908 - accuracy: 0.9655 - val_loss: 0.0887 - val_accuracy: 0.9677 - lr: 3.9063e-06\n",
            "Epoch 00011: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe7d64f0be0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        },
        {
          "output_type": "stream",
          "text": [
            "Training fold 5 completed. macro f1 score : 0.93765\n",
            "Training completed. oof macro f1 score : 0.93635\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZQcJA1eIHIE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "5805eb3b-32e7-42c4-9487-56bac097e036"
      },
      "source": [
        "submission['open_channels'] = np.argmax(preds_, axis = 1).astype(int)\n",
        "submission['open_channels'].value_counts()\n",
        "submission.to_csv('submission_5_fold_lstm_22.csv', index=False, float_format='%.4f')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1     1213516\n",
              "2      204899\n",
              "8       96614\n",
              "3       91210\n",
              "7       80363\n",
              "9       77447\n",
              "4       73058\n",
              "10      62556\n",
              "6       52118\n",
              "5       43671\n",
              "0        4548\n",
              "Name: open_channels, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XXjrwgvITXU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "outputId": "89b0439f-3c69-40e1-9948-f92b12b1f2a8"
      },
      "source": [
        "plt.plot(test['signal'])\n",
        "for x in np.sort(submission['open_channels'].unique()):\n",
        "  _=plt.plot(submission[submission['open_channels'] == x]['open_channels'],'.',label=x)\n",
        "_=plt.legend()\n",
        "_=plt.yticks(np.linspace(0,10,11))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fe74cfaf5f8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONzoLMEL4YKK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "4f709d04-ee43-434d-d094-7039929e81ab"
      },
      "source": [
        "!kaggle competitions submit -c liverpool-ion-switching -f submission_5_fold_lstm_22.csv -m \"0.93635\""
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "100% 21.0M/21.0M [00:02<00:00, 9.12MB/s]\n",
            "Successfully submitted to University of Liverpool - Ion Switching"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhU6xAsw2VPz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#opt = optimizers.Adam(lr=LR,decay=1e-5)\n",
        "#model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "#history = model.fit(train_data,train_data_target,epochs=120,batch_size=8,validation_split=0.2,callbacks=[cb_lr_schedule])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R__B-BEJBvKY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Conv1D, Dense, Dropout, Input, Concatenate, GlobalMaxPooling1D\n",
        "from keras.models import Model\n",
        "\n",
        "def get_base_model(input_len, fsize):\n",
        "  input_seq = Input(shape=(input_len, 19))\n",
        "  nb_filters = 10\n",
        "  convolved = Conv1D(nb_filters, fsize, padding=\"same\", activation=\"tanh\")(input_seq)\n",
        "  processed = GlobalMaxPooling1D()(convolved)\n",
        "  compressed = Dense(50, activation=\"tanh\")(processed)\n",
        "  compressed = Dropout(0.3)(compressed)\n",
        "  model = Model(inputs=input_seq, outputs=compressed)\n",
        "  return model\n",
        "  \n",
        "def main_model(inputs_lens = [512, 1024, 3480], fsizes = [8,16,24]):\n",
        "  input_smallseq = Input(shape=(inputs_lens[0], 19))\n",
        "  input_medseq = Input(shape=(inputs_lens[1] , 19))\n",
        "  input_origseq = Input(shape=(inputs_lens[2], 19))\n",
        "  base_net_small = get_base_model(inputs_lens[0], fsizes[0])\n",
        "  base_net_med = get_base_model(inputs_lens[1], fsizes[1])\n",
        "  base_net_original = get_base_model(inputs_lens[2], fsizes[2])\n",
        "  embedding_small = base_net_small(input_smallseq)\n",
        "  embedding_med = base_net_med(input_medseq)\n",
        "  embedding_original = base_net_original(input_origseq)\n",
        "  merged = Concatenate()([embedding_small, embedding_med, embedding_original])\n",
        "  out = Dense(1, activation='sigmoid')(merged)\n",
        "  model = Model(inputs=[input_smallseq, input_medseq, input_origseq], outputs=out)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJv1P2g2RNPw",
        "colab_type": "text"
      },
      "source": [
        "## Facebook Prophet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9sUfks1RRPG",
        "colab_type": "code",
        "outputId": "b3880b88-454a-4b71-a328-ca81560b08b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "prophet = Prophet()\n",
        "prophet_data = train.rename(columns={'signal':'ds','open_channels':'y'})\n",
        "prophet.fit(prophet_data[['ds','y']])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<fbprophet.forecaster.Prophet at 0x7f89e93f5588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_cBbK5mWx4P",
        "colab_type": "code",
        "outputId": "ab413dad-a372-4e09-a992-c6b49fe8f0be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "source": [
        ";prophet_test = train[train['batch'] == 47].rename(columns={'signal':'ds'}).drop(columns=['open_channels']).head(200).tail(100)\n",
        "prophet_test.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>ds</th>\n",
              "      <th>batch</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2350100</th>\n",
              "      <td>235.0101</td>\n",
              "      <td>5.457536</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2350101</th>\n",
              "      <td>235.0102</td>\n",
              "      <td>5.873407</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2350102</th>\n",
              "      <td>235.0103</td>\n",
              "      <td>4.103076</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2350103</th>\n",
              "      <td>235.0104</td>\n",
              "      <td>2.964446</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2350104</th>\n",
              "      <td>235.0105</td>\n",
              "      <td>1.162220</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             time        ds  batch\n",
              "2350100  235.0101  5.457536     47\n",
              "2350101  235.0102  5.873407     47\n",
              "2350102  235.0103  4.103076     47\n",
              "2350103  235.0104  2.964446     47\n",
              "2350104  235.0105  1.162220     47"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65Bjhm4MVs9E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = prophet.predict(prophet_test)['yhat']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_qyo3JNXjkR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = np.round(np.clip(0,10,preds)).astype(np.int32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0B-Fvn-XyiK",
        "colab_type": "code",
        "outputId": "481203f6-8253-49df-fbcf-3effa639a0fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "true_ = train[train['batch'] == 47].rename(columns={'signal':'ds'}).head(200).tail(100)['open_channels']\n",
        "f1_score(preds,true_,average='macro')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.11281483374506629"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q75NF6xTMheC",
        "colab_type": "text"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iE4h3Nr6AfMr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = random_forest_feat(train)\n",
        "test = random_forest_feat(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OLqFm0SBAqU",
        "colab_type": "code",
        "outputId": "d5e4daae-c32e-46be-b02e-ccc2c5aec64e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "test_pred = np.zeros((test.shape[0]),dtype=np.int8)\n",
        "for g in [0,1,2,3,4]:\n",
        "    print('Infering group %i'%g)\n",
        "    \n",
        "    # TRAIN DATA\n",
        "    data = train.loc[train.group==g]\n",
        "    x_train = np.zeros((len(data)-6,7))\n",
        "    x_train[:,0] = 0.25*data.signal[:-6]\n",
        "    x_train[:,1] = 0.5*data.signal[1:-5]\n",
        "    x_train[:,2] = 1.0*data.signal[2:-4]\n",
        "    x_train[:,3] = 4.0*data.signal[3:-3]\n",
        "    x_train[:,4] = 1.0*data.signal[4:-2]\n",
        "    x_train[:,5] = 0.5*data.signal[5:-1]\n",
        "    x_train[:,6] = 0.25*data.signal[6:]\n",
        "    y_train = data.open_channels[3:].values\n",
        "\n",
        "    # TEST DATA\n",
        "    data = test.loc[test.group==g]\n",
        "    x_test = np.zeros((len(data)-6,7))\n",
        "    x_test[:,0] = 0.25*data.signal[:-6]\n",
        "    x_test[:,1] = 0.5*data.signal[1:-5]\n",
        "    x_test[:,2] = 1.0*data.signal[2:-4]\n",
        "    x_test[:,3] = 4.0*data.signal[3:-3]\n",
        "    x_test[:,4] = 1.0*data.signal[4:-2]\n",
        "    x_test[:,5] = 0.5*data.signal[5:-1]\n",
        "    x_test[:,6] = 0.25*data.signal[6:]\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Infering group 0\n",
            "Infering group 1\n",
            "Infering group 2\n",
            "Infering group 3\n",
            "Infering group 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO8fk3dtB9I9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.fillna(0,inplace=True)\n",
        "test.fillna(0,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7Onx9gzMg9P",
        "colab_type": "code",
        "outputId": "94c274c3-77ee-4e17-bf3e-73a89152061d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier(\n",
        "            n_estimators=150,\n",
        "            max_depth=19,\n",
        "            max_features=train[use_cols].shape[1],\n",
        "            random_state=42,\n",
        "            n_jobs=10,\n",
        "            verbose=2\n",
        "        )\n",
        "model = rf.fit(train[use_cols],train['open_channels'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "building tree 1 of 150building tree 2 of 150building tree 3 of 150building tree 4 of 150\n",
            "\n",
            "building tree 5 of 150\n",
            "building tree 6 of 150\n",
            "\n",
            "building tree 7 of 150\n",
            "building tree 8 of 150\n",
            "building tree 9 of 150\n",
            "building tree 10 of 150\n",
            "\n",
            "building tree 11 of 150\n",
            "building tree 12 of 150\n",
            "building tree 13 of 150\n",
            "building tree 14 of 150\n",
            "building tree 15 of 150\n",
            "building tree 16 of 150\n",
            "building tree 17 of 150\n",
            "building tree 18 of 150\n",
            "building tree 19 of 150\n",
            "building tree 20 of 150\n",
            "building tree 21 of 150\n",
            "building tree 22 of 150\n",
            "building tree 23 of 150\n",
            "building tree 24 of 150\n",
            "building tree 25 of 150\n",
            "building tree 26 of 150\n",
            "building tree 27 of 150\n",
            "building tree 28 of 150\n",
            "building tree 29 of 150\n",
            "building tree 30 of 150\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=10)]: Done  21 tasks      | elapsed: 16.8min\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "building tree 31 of 150\n",
            "building tree 32 of 150\n",
            "building tree 33 of 150\n",
            "building tree 34 of 150\n",
            "building tree 35 of 150\n",
            "building tree 36 of 150\n",
            "building tree 37 of 150\n",
            "building tree 38 of 150\n",
            "building tree 39 of 150\n",
            "building tree 40 of 150\n",
            "building tree 41 of 150\n",
            "building tree 42 of 150\n",
            "building tree 43 of 150\n",
            "building tree 44 of 150\n",
            "building tree 45 of 150\n",
            "building tree 46 of 150\n",
            "building tree 47 of 150\n",
            "building tree 48 of 150\n",
            "building tree 49 of 150\n",
            "building tree 50 of 150\n",
            "building tree 51 of 150\n",
            "building tree 52 of 150\n",
            "building tree 53 of 150\n",
            "building tree 54 of 150\n",
            "building tree 55 of 150\n",
            "building tree 56 of 150\n",
            "building tree 57 of 150\n",
            "building tree 58 of 150\n",
            "building tree 59 of 150\n",
            "building tree 60 of 150\n",
            "building tree 61 of 150\n",
            "building tree 62 of 150\n",
            "building tree 63 of 150\n",
            "building tree 64 of 150\n",
            "building tree 65 of 150\n",
            "building tree 66 of 150\n",
            "building tree 67 of 150\n",
            "building tree 68 of 150\n",
            "building tree 69 of 150\n",
            "building tree 70 of 150\n",
            "building tree 71 of 150\n",
            "building tree 72 of 150\n",
            "building tree 73 of 150\n",
            "building tree 74 of 150\n",
            "building tree 75 of 150\n",
            "building tree 76 of 150\n",
            "building tree 77 of 150\n",
            "building tree 78 of 150\n",
            "building tree 79 of 150\n",
            "building tree 80 of 150\n",
            "building tree 81 of 150\n",
            "building tree 82 of 150\n",
            "building tree 83 of 150\n",
            "building tree 84 of 150\n",
            "building tree 85 of 150\n",
            "building tree 86 of 150\n",
            "building tree 87 of 150\n",
            "building tree 88 of 150\n",
            "building tree 89 of 150\n",
            "building tree 90 of 150\n",
            "building tree 91 of 150\n",
            "building tree 92 of 150\n",
            "building tree 93 of 150\n",
            "building tree 94 of 150\n",
            "building tree 95 of 150\n",
            "building tree 96 of 150\n",
            "building tree 97 of 150\n",
            "building tree 98 of 150\n",
            "building tree 99 of 150\n",
            "building tree 100 of 150\n",
            "building tree 101 of 150\n",
            "building tree 102 of 150\n",
            "building tree 103 of 150\n",
            "building tree 104 of 150\n",
            "building tree 105 of 150\n",
            "building tree 106 of 150\n",
            "building tree 107 of 150\n",
            "building tree 108 of 150\n",
            "building tree 109 of 150\n",
            "building tree 110 of 150\n",
            "building tree 111 of 150\n",
            "building tree 112 of 150\n",
            "building tree 113 of 150\n",
            "building tree 114 of 150\n",
            "building tree 115 of 150\n",
            "building tree 116 of 150\n",
            "building tree 117 of 150\n",
            "building tree 118 of 150\n",
            "building tree 119 of 150\n",
            "building tree 120 of 150\n",
            "building tree 121 of 150\n",
            "building tree 122 of 150\n",
            "building tree 123 of 150\n",
            "building tree 124 of 150\n",
            "building tree 125 of 150\n",
            "building tree 126 of 150\n",
            "building tree 127 of 150\n",
            "building tree 128 of 150\n",
            "building tree 129 of 150\n",
            "building tree 130 of 150\n",
            "building tree 131 of 150\n",
            "building tree 132 of 150\n",
            "building tree 133 of 150\n",
            "building tree 134 of 150\n",
            "building tree 135 of 150\n",
            "building tree 136 of 150\n",
            "building tree 137 of 150\n",
            "building tree 138 of 150\n",
            "building tree 139 of 150\n",
            "building tree 140 of 150\n",
            "building tree 141 of 150\n",
            "building tree 142 of 150\n",
            "building tree 143 of 150\n",
            "building tree 144 of 150\n",
            "building tree 145 of 150\n",
            "building tree 146 of 150\n",
            "building tree 147 of 150\n",
            "building tree 148 of 150\n",
            "building tree 149 of 150\n",
            "building tree 150 of 150\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=10)]: Done 150 out of 150 | elapsed: 83.4min finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KyM8cRsM3YG",
        "colab_type": "code",
        "outputId": "4bfa92ec-4abd-46c3-9332-fc361fe65d87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "preds = model.predict(test[use_cols])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
            "[Parallel(n_jobs=10)]: Done  21 tasks      | elapsed:    3.3s\n",
            "[Parallel(n_jobs=10)]: Done 150 out of 150 | elapsed:   19.4s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKRt1C0w-6MZ",
        "colab_type": "text"
      },
      "source": [
        "## LightGBM Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YozJbQObqyvS",
        "colab_type": "text"
      },
      "source": [
        "### GroupKFold "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "km8BENbvacP2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = 0\n",
        "splits = GroupKFold(n_splits = 5)\n",
        "for i, (train_idx, val_idx) in enumerate(splits.split(train[use_cols],train['open_channels'],groups=train['batch'])):\n",
        "  print(f'On fold {i}')\n",
        "  cols = [x for x in use_cols if x not in ['batch']]\n",
        "  \n",
        "  x_train,x_val = train[cols].iloc[train_idx],train[cols].iloc[val_idx]\n",
        "  y_train,y_val = train['open_channels'].iloc[train_idx],train['open_channels'].iloc[val_idx]\n",
        "  \n",
        "  train_data = lgb.Dataset(x_train,y_train)\n",
        "  val_data = lgb.Dataset(x_val, y_val)\n",
        "\n",
        "  model = lgb.train(params, train_data, 2000,  val_data, verbose_eval=50, early_stopping_rounds=500, feval=MacroF1Metric)\n",
        "  preds = model.predict(test[cols],num_iteration=model.best_iteration)\n",
        "  predictions += preds\n",
        "\n",
        "  del x_train,x_val,y_train,y_val,train_data,val_data\n",
        "  gc.collect()\n",
        "predictions /= 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jwFLQA2q9m7",
        "colab_type": "text"
      },
      "source": [
        "### One Fold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZruqklUnoctw",
        "colab_type": "code",
        "outputId": "27ffd159-005d-4f01-b32b-738938fc73af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "params = {'learning_rate': 0.09, \n",
        "          'max_depth': -1, \n",
        "          'num_leaves': 250,\n",
        "          'metric': 'rmse', \n",
        "          'random_state': 7, \n",
        "          'n_jobs':-1, \n",
        "          'sample_fraction':0.53}\n",
        "\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihkW6w_68uVp",
        "colab_type": "code",
        "outputId": "47768f5e-dd79-4b8a-8545-f3b7ec6b035e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "model = lgb.train(params, lgb.Dataset(x_train, y_train), 2000,  lgb.Dataset(x_val, y_val), verbose_eval=50, early_stopping_rounds=500, feval=MacroF1Metric)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 500 rounds.\n",
            "[50]\tvalid_0's rmse: 0.159627\tvalid_0's MacroF1Metric: 0.933547\n",
            "[100]\tvalid_0's rmse: 0.157512\tvalid_0's MacroF1Metric: 0.93458\n",
            "[150]\tvalid_0's rmse: 0.157538\tvalid_0's MacroF1Metric: 0.934386\n",
            "[200]\tvalid_0's rmse: 0.157576\tvalid_0's MacroF1Metric: 0.934521\n",
            "[250]\tvalid_0's rmse: 0.157592\tvalid_0's MacroF1Metric: 0.934466\n",
            "[300]\tvalid_0's rmse: 0.157621\tvalid_0's MacroF1Metric: 0.934504\n",
            "[350]\tvalid_0's rmse: 0.157653\tvalid_0's MacroF1Metric: 0.934774\n",
            "[400]\tvalid_0's rmse: 0.157685\tvalid_0's MacroF1Metric: 0.934684\n",
            "[450]\tvalid_0's rmse: 0.157731\tvalid_0's MacroF1Metric: 0.934726\n",
            "[500]\tvalid_0's rmse: 0.157776\tvalid_0's MacroF1Metric: 0.934665\n",
            "[550]\tvalid_0's rmse: 0.157799\tvalid_0's MacroF1Metric: 0.934665\n",
            "Early stopping, best iteration is:\n",
            "[88]\tvalid_0's rmse: 0.157497\tvalid_0's MacroF1Metric: 0.93452\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPuHvYBrOxox",
        "colab_type": "code",
        "outputId": "6ce01be8-edea-4185-c015-a7e9f392a229",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "SAVE_MODEL = True\n",
        "if SAVE_MODEL:\n",
        "  model.save_model('lgb_cv_0.93933.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<lightgbm.basic.Booster at 0x7fabc7f7cda0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sz6htk0j_neE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = model.predict(test[use_cols], num_iteration=model.best_iteration)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zene-auLQfag",
        "colab_type": "text"
      },
      "source": [
        "## Unet Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_sQdhs_QjHP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "K.clear_session()\n",
        "model = Unet()\n",
        "print(model.summary())\n",
        "\n",
        "learning_rate=0.005\n",
        "n_epoch=60\n",
        "batch_size=32\n",
        "\n",
        "lr_schedule = LearningRateScheduler(lrs)\n",
        "\n",
        "#classifier\n",
        "model.compile(loss=categorical_crossentropy, \n",
        "              optimizer=Adam(lr=learning_rate), \n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "hist = model_fit(model, train_input, train_target, val_input, val_target, n_epoch, batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MW_Mdwgorxby",
        "colab_type": "text"
      },
      "source": [
        "## WaveNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9JjwOTwIV1p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Classifier(shape_):\n",
        "    \n",
        "    def wave_block(x, filters, kernel_size, n):\n",
        "        dilation_rates = [2**i for i in range(n)]\n",
        "        x = Conv1D(filters = filters,\n",
        "                   kernel_size = 1,\n",
        "                   padding = 'same')(x)\n",
        "        res_x = x\n",
        "        for dilation_rate in dilation_rates:\n",
        "            tanh_out = Conv1D(filters = filters,\n",
        "                              kernel_size = kernel_size,\n",
        "                              padding = 'same', \n",
        "                              activation = 'tanh', \n",
        "                              dilation_rate = dilation_rate)(x)\n",
        "            sigm_out = Conv1D(filters = filters,\n",
        "                              kernel_size = kernel_size,\n",
        "                              padding = 'same',\n",
        "                              activation = 'sigmoid', \n",
        "                              dilation_rate = dilation_rate)(x)\n",
        "            x = Multiply()([tanh_out, sigm_out])\n",
        "            x = Conv1D(filters = filters,\n",
        "                       kernel_size = 1,\n",
        "                       padding = 'same')(x)\n",
        "            res_x = Add()([res_x, x])\n",
        "        return res_x\n",
        "    \n",
        "    inp = Input(shape = (shape_))\n",
        "    print(f'inp : {inp}')\n",
        "    x = wave_block(inp, 16, 3, 12)\n",
        "    x = wave_block(x, 32, 3, 8)\n",
        "    x = wave_block(x, 64, 3, 4)\n",
        "    x = wave_block(x, 128, 3, 1)\n",
        "    \n",
        "    out = Dense(11, activation = 'softmax', name = 'out')(x)\n",
        "    \n",
        "    model = models.Model(inputs = inp, outputs = out)\n",
        "    print(f'Input Shape : {model.input_shape}')\n",
        "    opt = Adam(lr = LR)\n",
        "    #opt = tfa.optimizers.SWA(opt)\n",
        "    model.compile(loss = losses.CategoricalCrossentropy(), optimizer = opt, metrics = ['accuracy'])\n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n",
        "# function that decrease the learning as epochs increase (i also change this part of the code)\n",
        "def lr_schedule(epoch):\n",
        "    if epoch < 30:\n",
        "        lr = LR\n",
        "    elif epoch < 40:\n",
        "        lr = LR / 3\n",
        "    elif epoch < 50:\n",
        "        lr = LR / 5\n",
        "    elif epoch < 60:\n",
        "        lr = LR / 7\n",
        "    elif epoch < 70:\n",
        "        lr = LR / 9\n",
        "    elif epoch < 80:\n",
        "        lr = LR / 11\n",
        "    elif epoch < 90:\n",
        "        lr = LR / 13\n",
        "    else:\n",
        "        lr = LR / 100\n",
        "    return lr\n",
        "\n",
        "# class to get macro f1 score. This is not entirely necessary but it's fun to check f1 score of each epoch (be carefull, if you use this function early stopping callback will not work)\n",
        "class MacroF1(Callback):\n",
        "    def __init__(self, model, inputs, targets):\n",
        "        self.model = model\n",
        "        self.inputs = inputs\n",
        "        self.targets = np.argmax(targets, axis = 2).reshape(-1)\n",
        "        \n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        pred = np.argmax(self.model.predict(self.inputs), axis = 2).reshape(-1)\n",
        "        score = f1_score(self.targets, pred, average = 'macro')\n",
        "        print(f'F1 Macro Score: {score:.5f}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7Do85ZL1Z0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def grouping(df):\n",
        "  df['group'] = df.groupby(df.index//4000,sort=False)['signal'].agg(['ngroup']).values\n",
        "  df['group'] = df.group.astype(np.uint16)\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pH-bMmQmE-6w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_features(df):\n",
        "  df['batch_slice_roll10_std'] = df.groupby('batch_slices')['signal'].transform(lambda x:pd.Series(x).rolling(10).std())\n",
        "  df['batch_slice_roll20_std'] = df.groupby('batch_slices')['signal'].transform(lambda x:pd.Series(x).rolling(20).std())\n",
        "  df['batch_slice_roll30_std'] = df.groupby('batch_slices')['signal'].transform(lambda x:pd.Series(x).rolling(30).std())\n",
        "  df['batch_slice_roll10_var'] = df.groupby('batch_slices')['signal'].transform(lambda x:pd.Series(x).rolling(10).var())\n",
        "  df['batch_slice_roll20_var'] = df.groupby('batch_slices')['signal'].transform(lambda x:pd.Series(x).rolling(20).var())\n",
        "  df['batch_slice_roll30_var'] = df.groupby('batch_slices')['signal'].transform(lambda x:pd.Series(x).rolling(30).var())\n",
        "  df['batch_slice_roll10_mean'] = df.groupby('batch_slices')['signal'].transform(lambda x:pd.Series(x).rolling(10).mean())\n",
        "  df['batch_slice_roll20_mean'] = df.groupby('batch_slices')['signal'].transform(lambda x:pd.Series(x).rolling(20).mean())\n",
        "  df['batch_slice_roll30_mean'] = df.groupby('batch_slices')['signal'].transform(lambda x:pd.Series(x).rolling(30).mean())\n",
        "  df['batch_slice_roll10_max'] = df.groupby('batch_slices')['signal'].transform(lambda x:pd.Series(x).rolling(10).max())\n",
        "  df['batch_slice_roll20_max'] = df.groupby('batch_slices')['signal'].transform(lambda x:pd.Series(x).rolling(20).max())\n",
        "  df['batch_slice_roll30_max'] = df.groupby('batch_slices')['signal'].transform(lambda x:pd.Series(x).rolling(30).max())\n",
        "  df['batch_slice_roll10_min'] = df.groupby('batch_slices')['signal'].transform(lambda x:pd.Series(x).rolling(10).min())\n",
        "  df['batch_slice_roll20_min'] = df.groupby('batch_slices')['signal'].transform(lambda x:pd.Series(x).rolling(20).min())\n",
        "  df['batch_slice_roll30_min'] = df.groupby('batch_slices')['signal'].transform(lambda x:pd.Series(x).rolling(30).min())\n",
        "  df['batch_slice_roll10_range'] = df['batch_slice_roll10_max'] - df['batch_slice_roll10_min']\n",
        "  df['batch_slice_roll20_range'] = df['batch_slice_roll20_max'] - df['batch_slice_roll20_min']\n",
        "  df['batch_slice_roll30_range'] = df['batch_slice_roll30_max'] - df['batch_slice_roll30_min']\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHaWos2tIYC9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def norm(df):\n",
        "  for x in [x for x in df.columns if x not in ['group','open_channels','time','num_states','batch','batch_slices']]:\n",
        "    min_ = df[x].min(axis=0)\n",
        "    max_ = df[x].max(axis=0)\n",
        "    df[x] = (df[x] - min_)/(max_ - min_)\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dE_sdFf1aEA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('train_clean_kalman.csv')\n",
        "test = pd.read_csv('test_clean_kalman.csv')\n",
        "submission = pd.read_csv('sample_submission.csv.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5eSgV1dG7XW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipe = DataPipeLine()\n",
        "train = pipe.get_batch(train)\n",
        "test = pipe.get_batch(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkWHcvRsG9Ox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = grouping(train)\n",
        "test = grouping(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnoQHRCvGwDG",
        "colab_type": "code",
        "outputId": "a3728eae-4370-4d44-e09f-126e90e0e020",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        }
      },
      "source": [
        "train,test = get_early_prediction(train,test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 500 rounds.\n",
            "[50]\tvalid_0's rmse: 0.00948205\n",
            "[100]\tvalid_0's rmse: 0.00912135\n",
            "[150]\tvalid_0's rmse: 0.0317013\n",
            "[200]\tvalid_0's rmse: 0.0550278\n",
            "[250]\tvalid_0's rmse: 0.0707167\n",
            "[300]\tvalid_0's rmse: 0.0826354\n",
            "[350]\tvalid_0's rmse: 0.0938618\n",
            "[400]\tvalid_0's rmse: 0.0980254\n",
            "[450]\tvalid_0's rmse: 0.105739\n",
            "[500]\tvalid_0's rmse: 0.110259\n",
            "[550]\tvalid_0's rmse: 0.112986\n",
            "Early stopping, best iteration is:\n",
            "[87]\tvalid_0's rmse: 0.0001708\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1368x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2ZKzedL8Hi7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = get_features(train)\n",
        "test = get_features(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGNTagZ4H3HG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = norm(train)\n",
        "test = norm(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyW1bcfoPd1U",
        "colab_type": "code",
        "outputId": "ef1f53b3-be06-4127-ba74-40edff0a2c68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        }
      },
      "source": [
        "train.describe()\n",
        "test.describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>signal</th>\n",
              "      <th>open_channels</th>\n",
              "      <th>batch</th>\n",
              "      <th>group</th>\n",
              "      <th>num_states</th>\n",
              "      <th>batch_slice_roll10_std</th>\n",
              "      <th>batch_slice_roll20_std</th>\n",
              "      <th>batch_slice_roll30_std</th>\n",
              "      <th>batch_slice_roll10_var</th>\n",
              "      <th>batch_slice_roll20_var</th>\n",
              "      <th>batch_slice_roll30_var</th>\n",
              "      <th>batch_slice_roll10_mean</th>\n",
              "      <th>batch_slice_roll20_mean</th>\n",
              "      <th>batch_slice_roll30_mean</th>\n",
              "      <th>batch_slice_roll10_max</th>\n",
              "      <th>batch_slice_roll20_max</th>\n",
              "      <th>batch_slice_roll30_max</th>\n",
              "      <th>batch_slice_roll10_min</th>\n",
              "      <th>batch_slice_roll20_min</th>\n",
              "      <th>batch_slice_roll30_min</th>\n",
              "      <th>batch_slice_roll10_range</th>\n",
              "      <th>batch_slice_roll20_range</th>\n",
              "      <th>batch_slice_roll30_range</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5.000000e+06</td>\n",
              "      <td>5.000000e+06</td>\n",
              "      <td>5.000000e+06</td>\n",
              "      <td>5.000000e+06</td>\n",
              "      <td>5.000000e+06</td>\n",
              "      <td>5.000000e+06</td>\n",
              "      <td>4.991000e+06</td>\n",
              "      <td>4.981000e+06</td>\n",
              "      <td>4.971000e+06</td>\n",
              "      <td>4.991000e+06</td>\n",
              "      <td>4.981000e+06</td>\n",
              "      <td>4.971000e+06</td>\n",
              "      <td>4.991000e+06</td>\n",
              "      <td>4.981000e+06</td>\n",
              "      <td>4.971000e+06</td>\n",
              "      <td>4.991000e+06</td>\n",
              "      <td>4.981000e+06</td>\n",
              "      <td>4.971000e+06</td>\n",
              "      <td>4.991000e+06</td>\n",
              "      <td>4.981000e+06</td>\n",
              "      <td>4.971000e+06</td>\n",
              "      <td>4.991000e+06</td>\n",
              "      <td>4.981000e+06</td>\n",
              "      <td>4.971000e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2.500001e+02</td>\n",
              "      <td>4.080354e-01</td>\n",
              "      <td>2.726043e+00</td>\n",
              "      <td>4.949999e+01</td>\n",
              "      <td>6.245000e+02</td>\n",
              "      <td>4.820000e+00</td>\n",
              "      <td>1.175916e-01</td>\n",
              "      <td>1.363476e-01</td>\n",
              "      <td>1.471268e-01</td>\n",
              "      <td>2.242574e-02</td>\n",
              "      <td>3.125659e-02</td>\n",
              "      <td>3.828431e-02</td>\n",
              "      <td>3.541544e-01</td>\n",
              "      <td>3.223954e-01</td>\n",
              "      <td>3.015726e-01</td>\n",
              "      <td>3.460113e-01</td>\n",
              "      <td>3.575005e-01</td>\n",
              "      <td>3.649569e-01</td>\n",
              "      <td>3.864966e-01</td>\n",
              "      <td>3.680960e-01</td>\n",
              "      <td>3.598078e-01</td>\n",
              "      <td>1.748988e-01</td>\n",
              "      <td>2.109475e-01</td>\n",
              "      <td>2.282431e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.443376e+02</td>\n",
              "      <td>1.731953e-01</td>\n",
              "      <td>2.665051e+00</td>\n",
              "      <td>2.886607e+01</td>\n",
              "      <td>3.608438e+02</td>\n",
              "      <td>2.987909e+00</td>\n",
              "      <td>8.464416e-02</td>\n",
              "      <td>9.671180e-02</td>\n",
              "      <td>1.062616e-01</td>\n",
              "      <td>3.398883e-02</td>\n",
              "      <td>4.055737e-02</td>\n",
              "      <td>4.639604e-02</td>\n",
              "      <td>2.101878e-01</td>\n",
              "      <td>2.202706e-01</td>\n",
              "      <td>2.302172e-01</td>\n",
              "      <td>2.456687e-01</td>\n",
              "      <td>2.597327e-01</td>\n",
              "      <td>2.675085e-01</td>\n",
              "      <td>1.614062e-01</td>\n",
              "      <td>1.520208e-01</td>\n",
              "      <td>1.471322e-01</td>\n",
              "      <td>1.254165e-01</td>\n",
              "      <td>1.500132e-01</td>\n",
              "      <td>1.661437e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000e-04</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.250001e+02</td>\n",
              "      <td>2.560607e-01</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>2.475000e+01</td>\n",
              "      <td>3.120000e+02</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>4.292514e-02</td>\n",
              "      <td>4.209613e-02</td>\n",
              "      <td>3.840646e-02</td>\n",
              "      <td>2.452140e-03</td>\n",
              "      <td>3.004358e-03</td>\n",
              "      <td>3.204240e-03</td>\n",
              "      <td>1.644060e-01</td>\n",
              "      <td>1.227190e-01</td>\n",
              "      <td>9.302422e-02</td>\n",
              "      <td>1.404848e-01</td>\n",
              "      <td>1.362806e-01</td>\n",
              "      <td>1.352456e-01</td>\n",
              "      <td>2.266545e-01</td>\n",
              "      <td>2.199298e-01</td>\n",
              "      <td>2.187609e-01</td>\n",
              "      <td>6.725083e-02</td>\n",
              "      <td>7.371293e-02</td>\n",
              "      <td>7.221630e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2.500000e+02</td>\n",
              "      <td>3.806188e-01</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>4.950000e+01</td>\n",
              "      <td>6.245000e+02</td>\n",
              "      <td>4.000000e+00</td>\n",
              "      <td>1.088345e-01</td>\n",
              "      <td>1.262821e-01</td>\n",
              "      <td>1.336942e-01</td>\n",
              "      <td>1.328405e-02</td>\n",
              "      <td>1.931892e-02</td>\n",
              "      <td>2.329700e-02</td>\n",
              "      <td>3.327503e-01</td>\n",
              "      <td>3.025729e-01</td>\n",
              "      <td>2.823018e-01</td>\n",
              "      <td>3.457822e-01</td>\n",
              "      <td>3.509961e-01</td>\n",
              "      <td>3.537368e-01</td>\n",
              "      <td>3.396161e-01</td>\n",
              "      <td>3.239426e-01</td>\n",
              "      <td>3.212072e-01</td>\n",
              "      <td>1.521841e-01</td>\n",
              "      <td>1.736753e-01</td>\n",
              "      <td>1.822765e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>3.750000e+02</td>\n",
              "      <td>5.403648e-01</td>\n",
              "      <td>4.000000e+00</td>\n",
              "      <td>7.400000e+01</td>\n",
              "      <td>9.370000e+02</td>\n",
              "      <td>6.000000e+00</td>\n",
              "      <td>1.658441e-01</td>\n",
              "      <td>1.980436e-01</td>\n",
              "      <td>2.170580e-01</td>\n",
              "      <td>2.955693e-02</td>\n",
              "      <td>4.407476e-02</td>\n",
              "      <td>5.507120e-02</td>\n",
              "      <td>5.203709e-01</td>\n",
              "      <td>4.990732e-01</td>\n",
              "      <td>4.878719e-01</td>\n",
              "      <td>5.554434e-01</td>\n",
              "      <td>5.708526e-01</td>\n",
              "      <td>5.770584e-01</td>\n",
              "      <td>5.064888e-01</td>\n",
              "      <td>4.873768e-01</td>\n",
              "      <td>4.601131e-01</td>\n",
              "      <td>2.499022e-01</td>\n",
              "      <td>3.003234e-01</td>\n",
              "      <td>3.389818e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>5.000000e+02</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+01</td>\n",
              "      <td>9.900000e+01</td>\n",
              "      <td>1.249000e+03</td>\n",
              "      <td>1.100000e+01</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               time        signal  open_channels         batch         group  \\\n",
              "count  5.000000e+06  5.000000e+06   5.000000e+06  5.000000e+06  5.000000e+06   \n",
              "mean   2.500001e+02  4.080354e-01   2.726043e+00  4.949999e+01  6.245000e+02   \n",
              "std    1.443376e+02  1.731953e-01   2.665051e+00  2.886607e+01  3.608438e+02   \n",
              "min    1.000000e-04  0.000000e+00   0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "25%    1.250001e+02  2.560607e-01   1.000000e+00  2.475000e+01  3.120000e+02   \n",
              "50%    2.500000e+02  3.806188e-01   2.000000e+00  4.950000e+01  6.245000e+02   \n",
              "75%    3.750000e+02  5.403648e-01   4.000000e+00  7.400000e+01  9.370000e+02   \n",
              "max    5.000000e+02  1.000000e+00   1.000000e+01  9.900000e+01  1.249000e+03   \n",
              "\n",
              "         num_states  batch_slice_roll10_std  batch_slice_roll20_std  \\\n",
              "count  5.000000e+06            4.991000e+06            4.981000e+06   \n",
              "mean   4.820000e+00            1.175916e-01            1.363476e-01   \n",
              "std    2.987909e+00            8.464416e-02            9.671180e-02   \n",
              "min    2.000000e+00            0.000000e+00            0.000000e+00   \n",
              "25%    2.000000e+00            4.292514e-02            4.209613e-02   \n",
              "50%    4.000000e+00            1.088345e-01            1.262821e-01   \n",
              "75%    6.000000e+00            1.658441e-01            1.980436e-01   \n",
              "max    1.100000e+01            1.000000e+00            1.000000e+00   \n",
              "\n",
              "       batch_slice_roll30_std  batch_slice_roll10_var  batch_slice_roll20_var  \\\n",
              "count            4.971000e+06            4.991000e+06            4.981000e+06   \n",
              "mean             1.471268e-01            2.242574e-02            3.125659e-02   \n",
              "std              1.062616e-01            3.398883e-02            4.055737e-02   \n",
              "min              0.000000e+00            0.000000e+00            0.000000e+00   \n",
              "25%              3.840646e-02            2.452140e-03            3.004358e-03   \n",
              "50%              1.336942e-01            1.328405e-02            1.931892e-02   \n",
              "75%              2.170580e-01            2.955693e-02            4.407476e-02   \n",
              "max              1.000000e+00            1.000000e+00            1.000000e+00   \n",
              "\n",
              "       batch_slice_roll30_var  batch_slice_roll10_mean  \\\n",
              "count            4.971000e+06             4.991000e+06   \n",
              "mean             3.828431e-02             3.541544e-01   \n",
              "std              4.639604e-02             2.101878e-01   \n",
              "min              0.000000e+00             0.000000e+00   \n",
              "25%              3.204240e-03             1.644060e-01   \n",
              "50%              2.329700e-02             3.327503e-01   \n",
              "75%              5.507120e-02             5.203709e-01   \n",
              "max              1.000000e+00             1.000000e+00   \n",
              "\n",
              "       batch_slice_roll20_mean  batch_slice_roll30_mean  \\\n",
              "count             4.981000e+06             4.971000e+06   \n",
              "mean              3.223954e-01             3.015726e-01   \n",
              "std               2.202706e-01             2.302172e-01   \n",
              "min               0.000000e+00             0.000000e+00   \n",
              "25%               1.227190e-01             9.302422e-02   \n",
              "50%               3.025729e-01             2.823018e-01   \n",
              "75%               4.990732e-01             4.878719e-01   \n",
              "max               1.000000e+00             1.000000e+00   \n",
              "\n",
              "       batch_slice_roll10_max  batch_slice_roll20_max  batch_slice_roll30_max  \\\n",
              "count            4.991000e+06            4.981000e+06            4.971000e+06   \n",
              "mean             3.460113e-01            3.575005e-01            3.649569e-01   \n",
              "std              2.456687e-01            2.597327e-01            2.675085e-01   \n",
              "min              0.000000e+00            0.000000e+00            0.000000e+00   \n",
              "25%              1.404848e-01            1.362806e-01            1.352456e-01   \n",
              "50%              3.457822e-01            3.509961e-01            3.537368e-01   \n",
              "75%              5.554434e-01            5.708526e-01            5.770584e-01   \n",
              "max              1.000000e+00            1.000000e+00            1.000000e+00   \n",
              "\n",
              "       batch_slice_roll10_min  batch_slice_roll20_min  batch_slice_roll30_min  \\\n",
              "count            4.991000e+06            4.981000e+06            4.971000e+06   \n",
              "mean             3.864966e-01            3.680960e-01            3.598078e-01   \n",
              "std              1.614062e-01            1.520208e-01            1.471322e-01   \n",
              "min              0.000000e+00            0.000000e+00            0.000000e+00   \n",
              "25%              2.266545e-01            2.199298e-01            2.187609e-01   \n",
              "50%              3.396161e-01            3.239426e-01            3.212072e-01   \n",
              "75%              5.064888e-01            4.873768e-01            4.601131e-01   \n",
              "max              1.000000e+00            1.000000e+00            1.000000e+00   \n",
              "\n",
              "       batch_slice_roll10_range  batch_slice_roll20_range  \\\n",
              "count              4.991000e+06              4.981000e+06   \n",
              "mean               1.748988e-01              2.109475e-01   \n",
              "std                1.254165e-01              1.500132e-01   \n",
              "min                0.000000e+00              0.000000e+00   \n",
              "25%                6.725083e-02              7.371293e-02   \n",
              "50%                1.521841e-01              1.736753e-01   \n",
              "75%                2.499022e-01              3.003234e-01   \n",
              "max                1.000000e+00              1.000000e+00   \n",
              "\n",
              "       batch_slice_roll30_range  \n",
              "count              4.971000e+06  \n",
              "mean               2.282431e-01  \n",
              "std                1.661437e-01  \n",
              "min                0.000000e+00  \n",
              "25%                7.221630e-02  \n",
              "50%                1.822765e-01  \n",
              "75%                3.389818e-01  \n",
              "max                1.000000e+00  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>signal</th>\n",
              "      <th>batch</th>\n",
              "      <th>group</th>\n",
              "      <th>num_states</th>\n",
              "      <th>batch_slice_roll10_std</th>\n",
              "      <th>batch_slice_roll20_std</th>\n",
              "      <th>batch_slice_roll30_std</th>\n",
              "      <th>batch_slice_roll10_var</th>\n",
              "      <th>batch_slice_roll20_var</th>\n",
              "      <th>batch_slice_roll30_var</th>\n",
              "      <th>batch_slice_roll10_mean</th>\n",
              "      <th>batch_slice_roll20_mean</th>\n",
              "      <th>batch_slice_roll30_mean</th>\n",
              "      <th>batch_slice_roll10_max</th>\n",
              "      <th>batch_slice_roll20_max</th>\n",
              "      <th>batch_slice_roll30_max</th>\n",
              "      <th>batch_slice_roll10_min</th>\n",
              "      <th>batch_slice_roll20_min</th>\n",
              "      <th>batch_slice_roll30_min</th>\n",
              "      <th>batch_slice_roll10_range</th>\n",
              "      <th>batch_slice_roll20_range</th>\n",
              "      <th>batch_slice_roll30_range</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>1.996400e+06</td>\n",
              "      <td>1.992400e+06</td>\n",
              "      <td>1.988400e+06</td>\n",
              "      <td>1.996400e+06</td>\n",
              "      <td>1.992400e+06</td>\n",
              "      <td>1.988400e+06</td>\n",
              "      <td>1.996400e+06</td>\n",
              "      <td>1.992400e+06</td>\n",
              "      <td>1.988400e+06</td>\n",
              "      <td>1.996400e+06</td>\n",
              "      <td>1.992400e+06</td>\n",
              "      <td>1.988400e+06</td>\n",
              "      <td>1.996400e+06</td>\n",
              "      <td>1.992400e+06</td>\n",
              "      <td>1.988400e+06</td>\n",
              "      <td>1.996400e+06</td>\n",
              "      <td>1.992400e+06</td>\n",
              "      <td>1.988400e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>6.000001e+02</td>\n",
              "      <td>3.012103e-01</td>\n",
              "      <td>1.195000e+02</td>\n",
              "      <td>2.495000e+02</td>\n",
              "      <td>3.750000e+00</td>\n",
              "      <td>1.191040e-01</td>\n",
              "      <td>1.369275e-01</td>\n",
              "      <td>1.290481e-01</td>\n",
              "      <td>2.666212e-02</td>\n",
              "      <td>4.114670e-02</td>\n",
              "      <td>3.908565e-02</td>\n",
              "      <td>2.130505e-01</td>\n",
              "      <td>1.696633e-01</td>\n",
              "      <td>1.679608e-01</td>\n",
              "      <td>2.036175e-01</td>\n",
              "      <td>2.058649e-01</td>\n",
              "      <td>2.111060e-01</td>\n",
              "      <td>2.794275e-01</td>\n",
              "      <td>2.740865e-01</td>\n",
              "      <td>2.679150e-01</td>\n",
              "      <td>1.276733e-01</td>\n",
              "      <td>1.345486e-01</td>\n",
              "      <td>1.455808e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>5.773504e+01</td>\n",
              "      <td>1.620091e-01</td>\n",
              "      <td>1.154340e+01</td>\n",
              "      <td>1.443373e+02</td>\n",
              "      <td>2.467287e+00</td>\n",
              "      <td>1.027773e-01</td>\n",
              "      <td>1.298545e-01</td>\n",
              "      <td>1.277912e-01</td>\n",
              "      <td>4.593586e-02</td>\n",
              "      <td>6.757486e-02</td>\n",
              "      <td>6.337742e-02</td>\n",
              "      <td>1.995798e-01</td>\n",
              "      <td>2.137186e-01</td>\n",
              "      <td>2.140694e-01</td>\n",
              "      <td>2.306383e-01</td>\n",
              "      <td>2.442853e-01</td>\n",
              "      <td>2.506702e-01</td>\n",
              "      <td>1.453750e-01</td>\n",
              "      <td>1.387967e-01</td>\n",
              "      <td>1.323803e-01</td>\n",
              "      <td>1.088038e-01</td>\n",
              "      <td>1.261947e-01</td>\n",
              "      <td>1.419849e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>5.000001e+02</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000e+02</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>5.500001e+02</td>\n",
              "      <td>1.944610e-01</td>\n",
              "      <td>1.097500e+02</td>\n",
              "      <td>1.247500e+02</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>5.097809e-02</td>\n",
              "      <td>4.826944e-02</td>\n",
              "      <td>4.051324e-02</td>\n",
              "      <td>3.579730e-03</td>\n",
              "      <td>4.839826e-03</td>\n",
              "      <td>4.110326e-03</td>\n",
              "      <td>8.111174e-02</td>\n",
              "      <td>2.785994e-02</td>\n",
              "      <td>2.556016e-02</td>\n",
              "      <td>4.655078e-02</td>\n",
              "      <td>3.903943e-02</td>\n",
              "      <td>3.975648e-02</td>\n",
              "      <td>1.893534e-01</td>\n",
              "      <td>1.899024e-01</td>\n",
              "      <td>1.885436e-01</td>\n",
              "      <td>5.620643e-02</td>\n",
              "      <td>5.039760e-02</td>\n",
              "      <td>5.036889e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>6.000000e+02</td>\n",
              "      <td>2.140872e-01</td>\n",
              "      <td>1.195000e+02</td>\n",
              "      <td>2.495000e+02</td>\n",
              "      <td>3.000000e+00</td>\n",
              "      <td>6.824371e-02</td>\n",
              "      <td>6.364135e-02</td>\n",
              "      <td>5.282093e-02</td>\n",
              "      <td>5.946517e-03</td>\n",
              "      <td>7.305959e-03</td>\n",
              "      <td>5.967830e-03</td>\n",
              "      <td>9.139691e-02</td>\n",
              "      <td>3.675213e-02</td>\n",
              "      <td>3.358518e-02</td>\n",
              "      <td>6.392428e-02</td>\n",
              "      <td>5.546235e-02</td>\n",
              "      <td>5.565808e-02</td>\n",
              "      <td>2.013682e-01</td>\n",
              "      <td>2.008356e-01</td>\n",
              "      <td>1.988580e-01</td>\n",
              "      <td>7.696584e-02</td>\n",
              "      <td>6.980326e-02</td>\n",
              "      <td>6.977516e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.500000e+02</td>\n",
              "      <td>3.743221e-01</td>\n",
              "      <td>1.292500e+02</td>\n",
              "      <td>3.742500e+02</td>\n",
              "      <td>4.000000e+00</td>\n",
              "      <td>1.716970e-01</td>\n",
              "      <td>2.022147e-01</td>\n",
              "      <td>1.905940e-01</td>\n",
              "      <td>3.236352e-02</td>\n",
              "      <td>4.970464e-02</td>\n",
              "      <td>4.612461e-02</td>\n",
              "      <td>3.186608e-01</td>\n",
              "      <td>2.865749e-01</td>\n",
              "      <td>2.871187e-01</td>\n",
              "      <td>3.516052e-01</td>\n",
              "      <td>3.571113e-01</td>\n",
              "      <td>3.623799e-01</td>\n",
              "      <td>3.242982e-01</td>\n",
              "      <td>3.130148e-01</td>\n",
              "      <td>3.069200e-01</td>\n",
              "      <td>1.700034e-01</td>\n",
              "      <td>1.743993e-01</td>\n",
              "      <td>1.876201e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>7.000000e+02</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.390000e+02</td>\n",
              "      <td>4.990000e+02</td>\n",
              "      <td>1.000000e+01</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               time        signal         batch         group    num_states  \\\n",
              "count  2.000000e+06  2.000000e+06  2.000000e+06  2.000000e+06  2.000000e+06   \n",
              "mean   6.000001e+02  3.012103e-01  1.195000e+02  2.495000e+02  3.750000e+00   \n",
              "std    5.773504e+01  1.620091e-01  1.154340e+01  1.443373e+02  2.467287e+00   \n",
              "min    5.000001e+02  0.000000e+00  1.000000e+02  0.000000e+00  2.000000e+00   \n",
              "25%    5.500001e+02  1.944610e-01  1.097500e+02  1.247500e+02  2.000000e+00   \n",
              "50%    6.000000e+02  2.140872e-01  1.195000e+02  2.495000e+02  3.000000e+00   \n",
              "75%    6.500000e+02  3.743221e-01  1.292500e+02  3.742500e+02  4.000000e+00   \n",
              "max    7.000000e+02  1.000000e+00  1.390000e+02  4.990000e+02  1.000000e+01   \n",
              "\n",
              "       batch_slice_roll10_std  batch_slice_roll20_std  batch_slice_roll30_std  \\\n",
              "count            1.996400e+06            1.992400e+06            1.988400e+06   \n",
              "mean             1.191040e-01            1.369275e-01            1.290481e-01   \n",
              "std              1.027773e-01            1.298545e-01            1.277912e-01   \n",
              "min              0.000000e+00            0.000000e+00            0.000000e+00   \n",
              "25%              5.097809e-02            4.826944e-02            4.051324e-02   \n",
              "50%              6.824371e-02            6.364135e-02            5.282093e-02   \n",
              "75%              1.716970e-01            2.022147e-01            1.905940e-01   \n",
              "max              1.000000e+00            1.000000e+00            1.000000e+00   \n",
              "\n",
              "       batch_slice_roll10_var  batch_slice_roll20_var  batch_slice_roll30_var  \\\n",
              "count            1.996400e+06            1.992400e+06            1.988400e+06   \n",
              "mean             2.666212e-02            4.114670e-02            3.908565e-02   \n",
              "std              4.593586e-02            6.757486e-02            6.337742e-02   \n",
              "min              0.000000e+00            0.000000e+00            0.000000e+00   \n",
              "25%              3.579730e-03            4.839826e-03            4.110326e-03   \n",
              "50%              5.946517e-03            7.305959e-03            5.967830e-03   \n",
              "75%              3.236352e-02            4.970464e-02            4.612461e-02   \n",
              "max              1.000000e+00            1.000000e+00            1.000000e+00   \n",
              "\n",
              "       batch_slice_roll10_mean  batch_slice_roll20_mean  \\\n",
              "count             1.996400e+06             1.992400e+06   \n",
              "mean              2.130505e-01             1.696633e-01   \n",
              "std               1.995798e-01             2.137186e-01   \n",
              "min               0.000000e+00             0.000000e+00   \n",
              "25%               8.111174e-02             2.785994e-02   \n",
              "50%               9.139691e-02             3.675213e-02   \n",
              "75%               3.186608e-01             2.865749e-01   \n",
              "max               1.000000e+00             1.000000e+00   \n",
              "\n",
              "       batch_slice_roll30_mean  batch_slice_roll10_max  \\\n",
              "count             1.988400e+06            1.996400e+06   \n",
              "mean              1.679608e-01            2.036175e-01   \n",
              "std               2.140694e-01            2.306383e-01   \n",
              "min               0.000000e+00            0.000000e+00   \n",
              "25%               2.556016e-02            4.655078e-02   \n",
              "50%               3.358518e-02            6.392428e-02   \n",
              "75%               2.871187e-01            3.516052e-01   \n",
              "max               1.000000e+00            1.000000e+00   \n",
              "\n",
              "       batch_slice_roll20_max  batch_slice_roll30_max  batch_slice_roll10_min  \\\n",
              "count            1.992400e+06            1.988400e+06            1.996400e+06   \n",
              "mean             2.058649e-01            2.111060e-01            2.794275e-01   \n",
              "std              2.442853e-01            2.506702e-01            1.453750e-01   \n",
              "min              0.000000e+00            0.000000e+00            0.000000e+00   \n",
              "25%              3.903943e-02            3.975648e-02            1.893534e-01   \n",
              "50%              5.546235e-02            5.565808e-02            2.013682e-01   \n",
              "75%              3.571113e-01            3.623799e-01            3.242982e-01   \n",
              "max              1.000000e+00            1.000000e+00            1.000000e+00   \n",
              "\n",
              "       batch_slice_roll20_min  batch_slice_roll30_min  \\\n",
              "count            1.992400e+06            1.988400e+06   \n",
              "mean             2.740865e-01            2.679150e-01   \n",
              "std              1.387967e-01            1.323803e-01   \n",
              "min              0.000000e+00            0.000000e+00   \n",
              "25%              1.899024e-01            1.885436e-01   \n",
              "50%              2.008356e-01            1.988580e-01   \n",
              "75%              3.130148e-01            3.069200e-01   \n",
              "max              1.000000e+00            1.000000e+00   \n",
              "\n",
              "       batch_slice_roll10_range  batch_slice_roll20_range  \\\n",
              "count              1.996400e+06              1.992400e+06   \n",
              "mean               1.276733e-01              1.345486e-01   \n",
              "std                1.088038e-01              1.261947e-01   \n",
              "min                0.000000e+00              0.000000e+00   \n",
              "25%                5.620643e-02              5.039760e-02   \n",
              "50%                7.696584e-02              6.980326e-02   \n",
              "75%                1.700034e-01              1.743993e-01   \n",
              "max                1.000000e+00              1.000000e+00   \n",
              "\n",
              "       batch_slice_roll30_range  \n",
              "count              1.988400e+06  \n",
              "mean               1.455808e-01  \n",
              "std                1.419849e-01  \n",
              "min                0.000000e+00  \n",
              "25%                5.036889e-02  \n",
              "50%                6.977516e-02  \n",
              "75%                1.876201e-01  \n",
              "max                1.000000e+00  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjVU1EdLIWKU",
        "colab_type": "code",
        "outputId": "96f27619-5735-4b9e-c274-081ec1b73b82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "source": [
        "feats"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['signal',\n",
              " 'num_states',\n",
              " 'batch_slice_roll10_std',\n",
              " 'batch_slice_roll20_std',\n",
              " 'batch_slice_roll30_std',\n",
              " 'batch_slice_roll10_var',\n",
              " 'batch_slice_roll20_var',\n",
              " 'batch_slice_roll30_var',\n",
              " 'batch_slice_roll10_mean',\n",
              " 'batch_slice_roll20_mean',\n",
              " 'batch_slice_roll30_mean',\n",
              " 'batch_slice_roll10_max',\n",
              " 'batch_slice_roll20_max',\n",
              " 'batch_slice_roll30_max',\n",
              " 'batch_slice_roll10_min',\n",
              " 'batch_slice_roll20_min',\n",
              " 'batch_slice_roll30_min',\n",
              " 'batch_slice_roll10_range',\n",
              " 'batch_slice_roll20_range',\n",
              " 'batch_slice_roll30_range']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nmie7vyv91x",
        "colab_type": "code",
        "outputId": "178fa068-4a9d-4dc1-98b1-1c79a6dd5140",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "feats = ['signal'] #[x for x in test.columns if x not in ['time','group','batch','batch_slices','batch','batch_slices']]\n",
        "tr = pd.concat([pd.get_dummies(train.open_channels), train[['group']]], axis=1)\n",
        "tr.columns = ['target_'+str(i) for i in range(11)] + ['group']\n",
        "target_cols = ['target_'+str(i) for i in range(11)]\n",
        "train_tr = np.array(list(tr.groupby('group').apply(lambda x: x[target_cols].values)))\n",
        "train_ = np.array(list(train.groupby('group').apply(lambda x: x[feats].values)))\n",
        "test_ = np.array(list(test.groupby('group').apply(lambda x: x[feats].values)))\n",
        "print(train_tr.shape)\n",
        "print(train_.shape)\n",
        "print(test_.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1250,)\n",
            "(1250,)\n",
            "(500,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GcPOSshIzCj",
        "colab_type": "code",
        "outputId": "51226200-ef52-4da9-b185-4f9998775094",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "train_.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1250,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3OfcJ7uIRTy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x,val_x,train_y,val_y = train_test_split(train_,train_tr,test_size=0.15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PZHxnGKIRRV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LR = 0.001\n",
        "nn_epochs = 100\n",
        "nn_batch_size = 16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlKgF320r0S0",
        "colab_type": "code",
        "outputId": "ad9f78b1-d609-45e6-e64c-d35866dbae12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "K.clear_session()\n",
        "gc.collect()\n",
        "shape_ = (None, train_x.shape[2]) \n",
        "print(f'Shape : {shape_}')\n",
        "model = Classifier(shape_)\n",
        "print(f'Model Input shape : {model.input_shape}')\n",
        "print(f'Model Output shape : {model.output_shape}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "382889"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        },
        {
          "output_type": "stream",
          "text": [
            "Shape : (None, 3)\n",
            "inp : Tensor(\"input_1:0\", shape=(None, None, 3), dtype=float32)\n",
            "Input Shape : (None, None, 3)\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None, 3)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d (Conv1D)                 (None, None, 16)     64          input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, None, 16)     784         conv1d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, None, 16)     784         conv1d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "multiply (Multiply)             (None, None, 16)     0           conv1d_1[0][0]                   \n",
            "                                                                 conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, None, 16)     272         multiply[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, None, 16)     784         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_5 (Conv1D)               (None, None, 16)     784         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "multiply_1 (Multiply)           (None, None, 16)     0           conv1d_4[0][0]                   \n",
            "                                                                 conv1d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, None, 16)     272         multiply_1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_7 (Conv1D)               (None, None, 16)     784         conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_8 (Conv1D)               (None, None, 16)     784         conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "multiply_2 (Multiply)           (None, None, 16)     0           conv1d_7[0][0]                   \n",
            "                                                                 conv1d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_9 (Conv1D)               (None, None, 16)     272         multiply_2[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_10 (Conv1D)              (None, None, 16)     784         conv1d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_11 (Conv1D)              (None, None, 16)     784         conv1d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "multiply_3 (Multiply)           (None, None, 16)     0           conv1d_10[0][0]                  \n",
            "                                                                 conv1d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_12 (Conv1D)              (None, None, 16)     272         multiply_3[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_13 (Conv1D)              (None, None, 16)     784         conv1d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_14 (Conv1D)              (None, None, 16)     784         conv1d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_4 (Multiply)           (None, None, 16)     0           conv1d_13[0][0]                  \n",
            "                                                                 conv1d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_15 (Conv1D)              (None, None, 16)     272         multiply_4[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_16 (Conv1D)              (None, None, 16)     784         conv1d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_17 (Conv1D)              (None, None, 16)     784         conv1d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_5 (Multiply)           (None, None, 16)     0           conv1d_16[0][0]                  \n",
            "                                                                 conv1d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_18 (Conv1D)              (None, None, 16)     272         multiply_5[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_19 (Conv1D)              (None, None, 16)     784         conv1d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_20 (Conv1D)              (None, None, 16)     784         conv1d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_6 (Multiply)           (None, None, 16)     0           conv1d_19[0][0]                  \n",
            "                                                                 conv1d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_21 (Conv1D)              (None, None, 16)     272         multiply_6[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_22 (Conv1D)              (None, None, 16)     784         conv1d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_23 (Conv1D)              (None, None, 16)     784         conv1d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_7 (Multiply)           (None, None, 16)     0           conv1d_22[0][0]                  \n",
            "                                                                 conv1d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_24 (Conv1D)              (None, None, 16)     272         multiply_7[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_25 (Conv1D)              (None, None, 16)     784         conv1d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_26 (Conv1D)              (None, None, 16)     784         conv1d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, None, 16)     0           conv1d[0][0]                     \n",
            "                                                                 conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "multiply_8 (Multiply)           (None, None, 16)     0           conv1d_25[0][0]                  \n",
            "                                                                 conv1d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, None, 16)     0           add[0][0]                        \n",
            "                                                                 conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_27 (Conv1D)              (None, None, 16)     272         multiply_8[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, None, 16)     0           add_1[0][0]                      \n",
            "                                                                 conv1d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_28 (Conv1D)              (None, None, 16)     784         conv1d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_29 (Conv1D)              (None, None, 16)     784         conv1d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, None, 16)     0           add_2[0][0]                      \n",
            "                                                                 conv1d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_9 (Multiply)           (None, None, 16)     0           conv1d_28[0][0]                  \n",
            "                                                                 conv1d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, None, 16)     0           add_3[0][0]                      \n",
            "                                                                 conv1d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_30 (Conv1D)              (None, None, 16)     272         multiply_9[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, None, 16)     0           add_4[0][0]                      \n",
            "                                                                 conv1d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_31 (Conv1D)              (None, None, 16)     784         conv1d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_32 (Conv1D)              (None, None, 16)     784         conv1d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, None, 16)     0           add_5[0][0]                      \n",
            "                                                                 conv1d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_10 (Multiply)          (None, None, 16)     0           conv1d_31[0][0]                  \n",
            "                                                                 conv1d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, None, 16)     0           add_6[0][0]                      \n",
            "                                                                 conv1d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_33 (Conv1D)              (None, None, 16)     272         multiply_10[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, None, 16)     0           add_7[0][0]                      \n",
            "                                                                 conv1d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_34 (Conv1D)              (None, None, 16)     784         conv1d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_35 (Conv1D)              (None, None, 16)     784         conv1d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, None, 16)     0           add_8[0][0]                      \n",
            "                                                                 conv1d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_11 (Multiply)          (None, None, 16)     0           conv1d_34[0][0]                  \n",
            "                                                                 conv1d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, None, 16)     0           add_9[0][0]                      \n",
            "                                                                 conv1d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_36 (Conv1D)              (None, None, 16)     272         multiply_11[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, None, 16)     0           add_10[0][0]                     \n",
            "                                                                 conv1d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_37 (Conv1D)              (None, None, 32)     544         add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_38 (Conv1D)              (None, None, 32)     3104        conv1d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_39 (Conv1D)              (None, None, 32)     3104        conv1d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_12 (Multiply)          (None, None, 32)     0           conv1d_38[0][0]                  \n",
            "                                                                 conv1d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_40 (Conv1D)              (None, None, 32)     1056        multiply_12[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_41 (Conv1D)              (None, None, 32)     3104        conv1d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_42 (Conv1D)              (None, None, 32)     3104        conv1d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_13 (Multiply)          (None, None, 32)     0           conv1d_41[0][0]                  \n",
            "                                                                 conv1d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_43 (Conv1D)              (None, None, 32)     1056        multiply_13[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_44 (Conv1D)              (None, None, 32)     3104        conv1d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_45 (Conv1D)              (None, None, 32)     3104        conv1d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_14 (Multiply)          (None, None, 32)     0           conv1d_44[0][0]                  \n",
            "                                                                 conv1d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_46 (Conv1D)              (None, None, 32)     1056        multiply_14[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_47 (Conv1D)              (None, None, 32)     3104        conv1d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_48 (Conv1D)              (None, None, 32)     3104        conv1d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_15 (Multiply)          (None, None, 32)     0           conv1d_47[0][0]                  \n",
            "                                                                 conv1d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_49 (Conv1D)              (None, None, 32)     1056        multiply_15[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_50 (Conv1D)              (None, None, 32)     3104        conv1d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_51 (Conv1D)              (None, None, 32)     3104        conv1d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_16 (Multiply)          (None, None, 32)     0           conv1d_50[0][0]                  \n",
            "                                                                 conv1d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_52 (Conv1D)              (None, None, 32)     1056        multiply_16[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_53 (Conv1D)              (None, None, 32)     3104        conv1d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_54 (Conv1D)              (None, None, 32)     3104        conv1d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_17 (Multiply)          (None, None, 32)     0           conv1d_53[0][0]                  \n",
            "                                                                 conv1d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, None, 32)     0           conv1d_37[0][0]                  \n",
            "                                                                 conv1d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_55 (Conv1D)              (None, None, 32)     1056        multiply_17[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, None, 32)     0           add_12[0][0]                     \n",
            "                                                                 conv1d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_56 (Conv1D)              (None, None, 32)     3104        conv1d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_57 (Conv1D)              (None, None, 32)     3104        conv1d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, None, 32)     0           add_13[0][0]                     \n",
            "                                                                 conv1d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_18 (Multiply)          (None, None, 32)     0           conv1d_56[0][0]                  \n",
            "                                                                 conv1d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, None, 32)     0           add_14[0][0]                     \n",
            "                                                                 conv1d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_58 (Conv1D)              (None, None, 32)     1056        multiply_18[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "add_16 (Add)                    (None, None, 32)     0           add_15[0][0]                     \n",
            "                                                                 conv1d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_59 (Conv1D)              (None, None, 32)     3104        conv1d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_60 (Conv1D)              (None, None, 32)     3104        conv1d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_17 (Add)                    (None, None, 32)     0           add_16[0][0]                     \n",
            "                                                                 conv1d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_19 (Multiply)          (None, None, 32)     0           conv1d_59[0][0]                  \n",
            "                                                                 conv1d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_18 (Add)                    (None, None, 32)     0           add_17[0][0]                     \n",
            "                                                                 conv1d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_61 (Conv1D)              (None, None, 32)     1056        multiply_19[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "add_19 (Add)                    (None, None, 32)     0           add_18[0][0]                     \n",
            "                                                                 conv1d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_62 (Conv1D)              (None, None, 64)     2112        add_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_63 (Conv1D)              (None, None, 64)     12352       conv1d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_64 (Conv1D)              (None, None, 64)     12352       conv1d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_20 (Multiply)          (None, None, 64)     0           conv1d_63[0][0]                  \n",
            "                                                                 conv1d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_65 (Conv1D)              (None, None, 64)     4160        multiply_20[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_66 (Conv1D)              (None, None, 64)     12352       conv1d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_67 (Conv1D)              (None, None, 64)     12352       conv1d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_21 (Multiply)          (None, None, 64)     0           conv1d_66[0][0]                  \n",
            "                                                                 conv1d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_68 (Conv1D)              (None, None, 64)     4160        multiply_21[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_69 (Conv1D)              (None, None, 64)     12352       conv1d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_70 (Conv1D)              (None, None, 64)     12352       conv1d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_22 (Multiply)          (None, None, 64)     0           conv1d_69[0][0]                  \n",
            "                                                                 conv1d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_71 (Conv1D)              (None, None, 64)     4160        multiply_22[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "add_20 (Add)                    (None, None, 64)     0           conv1d_62[0][0]                  \n",
            "                                                                 conv1d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_72 (Conv1D)              (None, None, 64)     12352       conv1d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_73 (Conv1D)              (None, None, 64)     12352       conv1d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_21 (Add)                    (None, None, 64)     0           add_20[0][0]                     \n",
            "                                                                 conv1d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_23 (Multiply)          (None, None, 64)     0           conv1d_72[0][0]                  \n",
            "                                                                 conv1d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_22 (Add)                    (None, None, 64)     0           add_21[0][0]                     \n",
            "                                                                 conv1d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_74 (Conv1D)              (None, None, 64)     4160        multiply_23[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "add_23 (Add)                    (None, None, 64)     0           add_22[0][0]                     \n",
            "                                                                 conv1d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_75 (Conv1D)              (None, None, 128)    8320        add_23[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_76 (Conv1D)              (None, None, 128)    49280       conv1d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_77 (Conv1D)              (None, None, 128)    49280       conv1d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_24 (Multiply)          (None, None, 128)    0           conv1d_76[0][0]                  \n",
            "                                                                 conv1d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_78 (Conv1D)              (None, None, 128)    16512       multiply_24[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "add_24 (Add)                    (None, None, 128)    0           conv1d_75[0][0]                  \n",
            "                                                                 conv1d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "out (Dense)                     (None, None, 11)     1419        add_24[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 323,179\n",
            "Trainable params: 323,179\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model Input shape : (None, None, 3)\n",
            "Model Output shape : (None, None, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QkwmMrLr0OM",
        "colab_type": "code",
        "outputId": "6a308c8c-7315-47d7-9a94-dfdd95b5d866",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "cb_lr_schedule = LearningRateScheduler(lr_schedule)\n",
        "model.fit(train_x,train_y,\n",
        "          epochs = nn_epochs,\n",
        "          callbacks = [cb_lr_schedule, MacroF1(model, val_x, val_y)],\n",
        "          batch_size = nn_batch_size,verbose = 2,\n",
        "          validation_data = (val_x,val_y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "F1 Macro Score: 0.91805\n",
            "54/54 - 7s - loss: 0.1511 - accuracy: 0.9500 - val_loss: 0.1190 - val_accuracy: 0.9574 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "F1 Macro Score: 0.91706\n",
            "54/54 - 7s - loss: 0.1307 - accuracy: 0.9546 - val_loss: 0.1146 - val_accuracy: 0.9581 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "F1 Macro Score: 0.91435\n",
            "54/54 - 7s - loss: 0.1558 - accuracy: 0.9451 - val_loss: 0.1278 - val_accuracy: 0.9529 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "F1 Macro Score: 0.92390\n",
            "54/54 - 7s - loss: 0.1398 - accuracy: 0.9496 - val_loss: 0.1056 - val_accuracy: 0.9615 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "F1 Macro Score: 0.92872\n",
            "54/54 - 7s - loss: 0.1156 - accuracy: 0.9595 - val_loss: 0.0957 - val_accuracy: 0.9638 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "F1 Macro Score: 0.90330\n",
            "54/54 - 7s - loss: 0.1101 - accuracy: 0.9601 - val_loss: 0.1259 - val_accuracy: 0.9521 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "F1 Macro Score: 0.92386\n",
            "54/54 - 7s - loss: 0.1248 - accuracy: 0.9548 - val_loss: 0.0959 - val_accuracy: 0.9637 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "F1 Macro Score: 0.85243\n",
            "54/54 - 7s - loss: 0.1124 - accuracy: 0.9610 - val_loss: 0.2174 - val_accuracy: 0.9195 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "F1 Macro Score: 0.91286\n",
            "54/54 - 7s - loss: 0.2459 - accuracy: 0.9141 - val_loss: 0.1342 - val_accuracy: 0.9512 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "F1 Macro Score: 0.92780\n",
            "54/54 - 7s - loss: 0.1157 - accuracy: 0.9579 - val_loss: 0.0903 - val_accuracy: 0.9649 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "F1 Macro Score: 0.93310\n",
            "54/54 - 7s - loss: 0.1040 - accuracy: 0.9632 - val_loss: 0.0882 - val_accuracy: 0.9660 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "F1 Macro Score: 0.93259\n",
            "54/54 - 7s - loss: 0.0984 - accuracy: 0.9633 - val_loss: 0.0881 - val_accuracy: 0.9658 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "F1 Macro Score: 0.93470\n",
            "54/54 - 7s - loss: 0.1006 - accuracy: 0.9630 - val_loss: 0.0844 - val_accuracy: 0.9671 - lr: 0.0010\n",
            "Epoch 14/100\n",
            "F1 Macro Score: 0.93489\n",
            "54/54 - 7s - loss: 0.0981 - accuracy: 0.9644 - val_loss: 0.0837 - val_accuracy: 0.9672 - lr: 0.0010\n",
            "Epoch 15/100\n",
            "F1 Macro Score: 0.93012\n",
            "54/54 - 7s - loss: 0.0974 - accuracy: 0.9637 - val_loss: 0.0892 - val_accuracy: 0.9645 - lr: 0.0010\n",
            "Epoch 16/100\n",
            "F1 Macro Score: 0.93460\n",
            "54/54 - 7s - loss: 0.0966 - accuracy: 0.9640 - val_loss: 0.0822 - val_accuracy: 0.9675 - lr: 0.0010\n",
            "Epoch 17/100\n",
            "F1 Macro Score: 0.93344\n",
            "54/54 - 7s - loss: 0.0939 - accuracy: 0.9642 - val_loss: 0.0850 - val_accuracy: 0.9664 - lr: 0.0010\n",
            "Epoch 18/100\n",
            "F1 Macro Score: 0.93619\n",
            "54/54 - 7s - loss: 0.0964 - accuracy: 0.9645 - val_loss: 0.0806 - val_accuracy: 0.9681 - lr: 0.0010\n",
            "Epoch 19/100\n",
            "F1 Macro Score: 0.93434\n",
            "54/54 - 7s - loss: 0.0911 - accuracy: 0.9649 - val_loss: 0.0814 - val_accuracy: 0.9678 - lr: 0.0010\n",
            "Epoch 20/100\n",
            "F1 Macro Score: 0.92994\n",
            "54/54 - 7s - loss: 0.0911 - accuracy: 0.9650 - val_loss: 0.0859 - val_accuracy: 0.9653 - lr: 0.0010\n",
            "Epoch 21/100\n",
            "F1 Macro Score: 0.92872\n",
            "54/54 - 7s - loss: 0.0942 - accuracy: 0.9647 - val_loss: 0.0955 - val_accuracy: 0.9625 - lr: 0.0010\n",
            "Epoch 22/100\n",
            "F1 Macro Score: 0.93545\n",
            "54/54 - 7s - loss: 0.0976 - accuracy: 0.9634 - val_loss: 0.0817 - val_accuracy: 0.9675 - lr: 0.0010\n",
            "Epoch 23/100\n",
            "F1 Macro Score: 0.93275\n",
            "54/54 - 7s - loss: 0.0939 - accuracy: 0.9639 - val_loss: 0.0834 - val_accuracy: 0.9667 - lr: 0.0010\n",
            "Epoch 24/100\n",
            "F1 Macro Score: 0.93709\n",
            "54/54 - 7s - loss: 0.0940 - accuracy: 0.9655 - val_loss: 0.0784 - val_accuracy: 0.9688 - lr: 0.0010\n",
            "Epoch 25/100\n",
            "F1 Macro Score: 0.92070\n",
            "54/54 - 7s - loss: 0.1057 - accuracy: 0.9598 - val_loss: 0.0927 - val_accuracy: 0.9632 - lr: 0.0010\n",
            "Epoch 26/100\n",
            "F1 Macro Score: 0.93045\n",
            "54/54 - 7s - loss: 0.1010 - accuracy: 0.9609 - val_loss: 0.0823 - val_accuracy: 0.9669 - lr: 0.0010\n",
            "Epoch 27/100\n",
            "F1 Macro Score: 0.92901\n",
            "54/54 - 7s - loss: 0.0982 - accuracy: 0.9620 - val_loss: 0.0912 - val_accuracy: 0.9639 - lr: 0.0010\n",
            "Epoch 28/100\n",
            "F1 Macro Score: 0.93262\n",
            "54/54 - 7s - loss: 0.0998 - accuracy: 0.9609 - val_loss: 0.0834 - val_accuracy: 0.9670 - lr: 0.0010\n",
            "Epoch 29/100\n",
            "F1 Macro Score: 0.93143\n",
            "54/54 - 7s - loss: 0.0883 - accuracy: 0.9661 - val_loss: 0.0848 - val_accuracy: 0.9663 - lr: 0.0010\n",
            "Epoch 30/100\n",
            "F1 Macro Score: 0.93805\n",
            "54/54 - 7s - loss: 0.0885 - accuracy: 0.9658 - val_loss: 0.0766 - val_accuracy: 0.9693 - lr: 0.0010\n",
            "Epoch 31/100\n",
            "F1 Macro Score: 0.93842\n",
            "54/54 - 7s - loss: 0.0864 - accuracy: 0.9668 - val_loss: 0.0757 - val_accuracy: 0.9698 - lr: 3.3333e-04\n",
            "Epoch 32/100\n",
            "F1 Macro Score: 0.93843\n",
            "54/54 - 7s - loss: 0.0871 - accuracy: 0.9669 - val_loss: 0.0755 - val_accuracy: 0.9698 - lr: 3.3333e-04\n",
            "Epoch 33/100\n",
            "F1 Macro Score: 0.93786\n",
            "54/54 - 7s - loss: 0.0854 - accuracy: 0.9670 - val_loss: 0.0758 - val_accuracy: 0.9696 - lr: 3.3333e-04\n",
            "Epoch 34/100\n",
            "F1 Macro Score: 0.93835\n",
            "54/54 - 7s - loss: 0.0854 - accuracy: 0.9669 - val_loss: 0.0757 - val_accuracy: 0.9697 - lr: 3.3333e-04\n",
            "Epoch 35/100\n",
            "F1 Macro Score: 0.93867\n",
            "54/54 - 7s - loss: 0.0846 - accuracy: 0.9671 - val_loss: 0.0753 - val_accuracy: 0.9698 - lr: 3.3333e-04\n",
            "Epoch 36/100\n",
            "F1 Macro Score: 0.93806\n",
            "54/54 - 7s - loss: 0.0852 - accuracy: 0.9670 - val_loss: 0.0761 - val_accuracy: 0.9694 - lr: 3.3333e-04\n",
            "Epoch 37/100\n",
            "F1 Macro Score: 0.93837\n",
            "54/54 - 7s - loss: 0.0851 - accuracy: 0.9670 - val_loss: 0.0757 - val_accuracy: 0.9697 - lr: 3.3333e-04\n",
            "Epoch 38/100\n",
            "F1 Macro Score: 0.93841\n",
            "54/54 - 7s - loss: 0.0847 - accuracy: 0.9671 - val_loss: 0.0759 - val_accuracy: 0.9697 - lr: 3.3333e-04\n",
            "Epoch 39/100\n",
            "F1 Macro Score: 0.93815\n",
            "54/54 - 7s - loss: 0.0864 - accuracy: 0.9671 - val_loss: 0.0757 - val_accuracy: 0.9696 - lr: 3.3333e-04\n",
            "Epoch 40/100\n",
            "F1 Macro Score: 0.93870\n",
            "54/54 - 7s - loss: 0.0850 - accuracy: 0.9671 - val_loss: 0.0752 - val_accuracy: 0.9699 - lr: 3.3333e-04\n",
            "Epoch 41/100\n",
            "F1 Macro Score: 0.93861\n",
            "54/54 - 7s - loss: 0.0866 - accuracy: 0.9672 - val_loss: 0.0751 - val_accuracy: 0.9698 - lr: 2.0000e-04\n",
            "Epoch 42/100\n",
            "F1 Macro Score: 0.93841\n",
            "54/54 - 7s - loss: 0.0849 - accuracy: 0.9673 - val_loss: 0.0753 - val_accuracy: 0.9698 - lr: 2.0000e-04\n",
            "Epoch 43/100\n",
            "F1 Macro Score: 0.93842\n",
            "54/54 - 7s - loss: 0.0850 - accuracy: 0.9672 - val_loss: 0.0753 - val_accuracy: 0.9698 - lr: 2.0000e-04\n",
            "Epoch 44/100\n",
            "F1 Macro Score: 0.93861\n",
            "54/54 - 7s - loss: 0.0865 - accuracy: 0.9673 - val_loss: 0.0749 - val_accuracy: 0.9700 - lr: 2.0000e-04\n",
            "Epoch 45/100\n",
            "F1 Macro Score: 0.93834\n",
            "54/54 - 7s - loss: 0.0845 - accuracy: 0.9673 - val_loss: 0.0750 - val_accuracy: 0.9698 - lr: 2.0000e-04\n",
            "Epoch 46/100\n",
            "F1 Macro Score: 0.93824\n",
            "54/54 - 7s - loss: 0.0840 - accuracy: 0.9673 - val_loss: 0.0752 - val_accuracy: 0.9699 - lr: 2.0000e-04\n",
            "Epoch 47/100\n",
            "F1 Macro Score: 0.93860\n",
            "54/54 - 7s - loss: 0.0867 - accuracy: 0.9673 - val_loss: 0.0750 - val_accuracy: 0.9700 - lr: 2.0000e-04\n",
            "Epoch 48/100\n",
            "F1 Macro Score: 0.93873\n",
            "54/54 - 7s - loss: 0.0838 - accuracy: 0.9673 - val_loss: 0.0749 - val_accuracy: 0.9700 - lr: 2.0000e-04\n",
            "Epoch 49/100\n",
            "F1 Macro Score: 0.93842\n",
            "54/54 - 7s - loss: 0.0845 - accuracy: 0.9673 - val_loss: 0.0750 - val_accuracy: 0.9700 - lr: 2.0000e-04\n",
            "Epoch 50/100\n",
            "F1 Macro Score: 0.93805\n",
            "54/54 - 7s - loss: 0.0866 - accuracy: 0.9673 - val_loss: 0.0762 - val_accuracy: 0.9695 - lr: 2.0000e-04\n",
            "Epoch 51/100\n",
            "F1 Macro Score: 0.93886\n",
            "54/54 - 7s - loss: 0.0843 - accuracy: 0.9673 - val_loss: 0.0746 - val_accuracy: 0.9701 - lr: 1.4286e-04\n",
            "Epoch 52/100\n",
            "F1 Macro Score: 0.93890\n",
            "54/54 - 7s - loss: 0.0857 - accuracy: 0.9674 - val_loss: 0.0747 - val_accuracy: 0.9700 - lr: 1.4286e-04\n",
            "Epoch 53/100\n",
            "F1 Macro Score: 0.93882\n",
            "54/54 - 7s - loss: 0.0837 - accuracy: 0.9674 - val_loss: 0.0745 - val_accuracy: 0.9701 - lr: 1.4286e-04\n",
            "Epoch 54/100\n",
            "F1 Macro Score: 0.93876\n",
            "54/54 - 7s - loss: 0.0838 - accuracy: 0.9674 - val_loss: 0.0747 - val_accuracy: 0.9702 - lr: 1.4286e-04\n",
            "Epoch 55/100\n",
            "F1 Macro Score: 0.93837\n",
            "54/54 - 7s - loss: 0.0837 - accuracy: 0.9674 - val_loss: 0.0750 - val_accuracy: 0.9699 - lr: 1.4286e-04\n",
            "Epoch 56/100\n",
            "F1 Macro Score: 0.93888\n",
            "54/54 - 7s - loss: 0.0842 - accuracy: 0.9674 - val_loss: 0.0747 - val_accuracy: 0.9701 - lr: 1.4286e-04\n",
            "Epoch 57/100\n",
            "F1 Macro Score: 0.93914\n",
            "54/54 - 7s - loss: 0.0837 - accuracy: 0.9674 - val_loss: 0.0744 - val_accuracy: 0.9702 - lr: 1.4286e-04\n",
            "Epoch 58/100\n",
            "F1 Macro Score: 0.93851\n",
            "54/54 - 7s - loss: 0.0856 - accuracy: 0.9674 - val_loss: 0.0746 - val_accuracy: 0.9700 - lr: 1.4286e-04\n",
            "Epoch 59/100\n",
            "F1 Macro Score: 0.93900\n",
            "54/54 - 7s - loss: 0.0856 - accuracy: 0.9675 - val_loss: 0.0746 - val_accuracy: 0.9701 - lr: 1.4286e-04\n",
            "Epoch 60/100\n",
            "F1 Macro Score: 0.93885\n",
            "54/54 - 7s - loss: 0.0861 - accuracy: 0.9674 - val_loss: 0.0743 - val_accuracy: 0.9702 - lr: 1.4286e-04\n",
            "Epoch 61/100\n",
            "F1 Macro Score: 0.93867\n",
            "54/54 - 7s - loss: 0.0858 - accuracy: 0.9675 - val_loss: 0.0747 - val_accuracy: 0.9700 - lr: 1.1111e-04\n",
            "Epoch 62/100\n",
            "F1 Macro Score: 0.93879\n",
            "54/54 - 7s - loss: 0.0838 - accuracy: 0.9675 - val_loss: 0.0745 - val_accuracy: 0.9701 - lr: 1.1111e-04\n",
            "Epoch 63/100\n",
            "F1 Macro Score: 0.93867\n",
            "54/54 - 7s - loss: 0.0834 - accuracy: 0.9675 - val_loss: 0.0744 - val_accuracy: 0.9701 - lr: 1.1111e-04\n",
            "Epoch 64/100\n",
            "F1 Macro Score: 0.93877\n",
            "54/54 - 7s - loss: 0.0856 - accuracy: 0.9675 - val_loss: 0.0743 - val_accuracy: 0.9701 - lr: 1.1111e-04\n",
            "Epoch 65/100\n",
            "F1 Macro Score: 0.93889\n",
            "54/54 - 7s - loss: 0.0838 - accuracy: 0.9675 - val_loss: 0.0742 - val_accuracy: 0.9701 - lr: 1.1111e-04\n",
            "Epoch 66/100\n",
            "F1 Macro Score: 0.93896\n",
            "54/54 - 7s - loss: 0.0837 - accuracy: 0.9675 - val_loss: 0.0744 - val_accuracy: 0.9701 - lr: 1.1111e-04\n",
            "Epoch 67/100\n",
            "F1 Macro Score: 0.93883\n",
            "54/54 - 7s - loss: 0.0836 - accuracy: 0.9675 - val_loss: 0.0743 - val_accuracy: 0.9702 - lr: 1.1111e-04\n",
            "Epoch 68/100\n",
            "F1 Macro Score: 0.93906\n",
            "54/54 - 7s - loss: 0.0835 - accuracy: 0.9676 - val_loss: 0.0742 - val_accuracy: 0.9702 - lr: 1.1111e-04\n",
            "Epoch 69/100\n",
            "F1 Macro Score: 0.93906\n",
            "54/54 - 7s - loss: 0.0851 - accuracy: 0.9675 - val_loss: 0.0742 - val_accuracy: 0.9702 - lr: 1.1111e-04\n",
            "Epoch 70/100\n",
            "F1 Macro Score: 0.93879\n",
            "54/54 - 7s - loss: 0.0852 - accuracy: 0.9675 - val_loss: 0.0746 - val_accuracy: 0.9701 - lr: 1.1111e-04\n",
            "Epoch 71/100\n",
            "F1 Macro Score: 0.93903\n",
            "54/54 - 7s - loss: 0.0876 - accuracy: 0.9676 - val_loss: 0.0742 - val_accuracy: 0.9702 - lr: 9.0909e-05\n",
            "Epoch 72/100\n",
            "F1 Macro Score: 0.93879\n",
            "54/54 - 7s - loss: 0.0830 - accuracy: 0.9676 - val_loss: 0.0744 - val_accuracy: 0.9702 - lr: 9.0909e-05\n",
            "Epoch 73/100\n",
            "F1 Macro Score: 0.93919\n",
            "54/54 - 7s - loss: 0.0833 - accuracy: 0.9676 - val_loss: 0.0741 - val_accuracy: 0.9702 - lr: 9.0909e-05\n",
            "Epoch 74/100\n",
            "F1 Macro Score: 0.93902\n",
            "54/54 - 7s - loss: 0.0859 - accuracy: 0.9676 - val_loss: 0.0742 - val_accuracy: 0.9702 - lr: 9.0909e-05\n",
            "Epoch 75/100\n",
            "F1 Macro Score: 0.93913\n",
            "54/54 - 7s - loss: 0.0853 - accuracy: 0.9676 - val_loss: 0.0741 - val_accuracy: 0.9702 - lr: 9.0909e-05\n",
            "Epoch 76/100\n",
            "F1 Macro Score: 0.93905\n",
            "54/54 - 7s - loss: 0.0828 - accuracy: 0.9676 - val_loss: 0.0742 - val_accuracy: 0.9702 - lr: 9.0909e-05\n",
            "Epoch 77/100\n",
            "F1 Macro Score: 0.93913\n",
            "54/54 - 7s - loss: 0.0838 - accuracy: 0.9676 - val_loss: 0.0741 - val_accuracy: 0.9702 - lr: 9.0909e-05\n",
            "Epoch 78/100\n",
            "F1 Macro Score: 0.93913\n",
            "54/54 - 7s - loss: 0.0828 - accuracy: 0.9676 - val_loss: 0.0741 - val_accuracy: 0.9703 - lr: 9.0909e-05\n",
            "Epoch 79/100\n",
            "F1 Macro Score: 0.93929\n",
            "54/54 - 7s - loss: 0.0850 - accuracy: 0.9677 - val_loss: 0.0741 - val_accuracy: 0.9703 - lr: 9.0909e-05\n",
            "Epoch 80/100\n",
            "F1 Macro Score: 0.93910\n",
            "54/54 - 7s - loss: 0.0829 - accuracy: 0.9676 - val_loss: 0.0740 - val_accuracy: 0.9703 - lr: 9.0909e-05\n",
            "Epoch 81/100\n",
            "F1 Macro Score: 0.93920\n",
            "54/54 - 7s - loss: 0.0833 - accuracy: 0.9676 - val_loss: 0.0742 - val_accuracy: 0.9702 - lr: 7.6923e-05\n",
            "Epoch 82/100\n",
            "F1 Macro Score: 0.93913\n",
            "54/54 - 7s - loss: 0.0828 - accuracy: 0.9676 - val_loss: 0.0740 - val_accuracy: 0.9703 - lr: 7.6923e-05\n",
            "Epoch 83/100\n",
            "F1 Macro Score: 0.93888\n",
            "54/54 - 7s - loss: 0.0828 - accuracy: 0.9677 - val_loss: 0.0740 - val_accuracy: 0.9703 - lr: 7.6923e-05\n",
            "Epoch 84/100\n",
            "F1 Macro Score: 0.93898\n",
            "54/54 - 7s - loss: 0.0829 - accuracy: 0.9677 - val_loss: 0.0740 - val_accuracy: 0.9702 - lr: 7.6923e-05\n",
            "Epoch 85/100\n",
            "F1 Macro Score: 0.93913\n",
            "54/54 - 7s - loss: 0.0849 - accuracy: 0.9677 - val_loss: 0.0741 - val_accuracy: 0.9703 - lr: 7.6923e-05\n",
            "Epoch 86/100\n",
            "F1 Macro Score: 0.93890\n",
            "54/54 - 7s - loss: 0.0853 - accuracy: 0.9676 - val_loss: 0.0743 - val_accuracy: 0.9701 - lr: 7.6923e-05\n",
            "Epoch 87/100\n",
            "F1 Macro Score: 0.93925\n",
            "54/54 - 7s - loss: 0.0838 - accuracy: 0.9676 - val_loss: 0.0742 - val_accuracy: 0.9702 - lr: 7.6923e-05\n",
            "Epoch 88/100\n",
            "F1 Macro Score: 0.93909\n",
            "54/54 - 7s - loss: 0.0825 - accuracy: 0.9677 - val_loss: 0.0740 - val_accuracy: 0.9702 - lr: 7.6923e-05\n",
            "Epoch 89/100\n",
            "F1 Macro Score: 0.93935\n",
            "54/54 - 7s - loss: 0.0825 - accuracy: 0.9677 - val_loss: 0.0739 - val_accuracy: 0.9703 - lr: 7.6923e-05\n",
            "Epoch 90/100\n",
            "F1 Macro Score: 0.93933\n",
            "54/54 - 7s - loss: 0.0845 - accuracy: 0.9678 - val_loss: 0.0739 - val_accuracy: 0.9703 - lr: 7.6923e-05\n",
            "Epoch 91/100\n",
            "F1 Macro Score: 0.93937\n",
            "54/54 - 7s - loss: 0.0824 - accuracy: 0.9678 - val_loss: 0.0738 - val_accuracy: 0.9704 - lr: 1.0000e-05\n",
            "Epoch 92/100\n",
            "F1 Macro Score: 0.93931\n",
            "54/54 - 7s - loss: 0.0829 - accuracy: 0.9678 - val_loss: 0.0738 - val_accuracy: 0.9704 - lr: 1.0000e-05\n",
            "Epoch 93/100\n",
            "F1 Macro Score: 0.93930\n",
            "54/54 - 7s - loss: 0.0827 - accuracy: 0.9678 - val_loss: 0.0738 - val_accuracy: 0.9703 - lr: 1.0000e-05\n",
            "Epoch 94/100\n",
            "F1 Macro Score: 0.93935\n",
            "54/54 - 7s - loss: 0.0822 - accuracy: 0.9678 - val_loss: 0.0738 - val_accuracy: 0.9704 - lr: 1.0000e-05\n",
            "Epoch 95/100\n",
            "F1 Macro Score: 0.93934\n",
            "54/54 - 7s - loss: 0.0828 - accuracy: 0.9678 - val_loss: 0.0738 - val_accuracy: 0.9704 - lr: 1.0000e-05\n",
            "Epoch 96/100\n",
            "F1 Macro Score: 0.93925\n",
            "54/54 - 7s - loss: 0.0830 - accuracy: 0.9678 - val_loss: 0.0739 - val_accuracy: 0.9704 - lr: 1.0000e-05\n",
            "Epoch 97/100\n",
            "F1 Macro Score: 0.93928\n",
            "54/54 - 7s - loss: 0.0851 - accuracy: 0.9678 - val_loss: 0.0738 - val_accuracy: 0.9703 - lr: 1.0000e-05\n",
            "Epoch 98/100\n",
            "F1 Macro Score: 0.93932\n",
            "54/54 - 7s - loss: 0.0831 - accuracy: 0.9678 - val_loss: 0.0738 - val_accuracy: 0.9704 - lr: 1.0000e-05\n",
            "Epoch 99/100\n",
            "F1 Macro Score: 0.93921\n",
            "54/54 - 7s - loss: 0.0845 - accuracy: 0.9678 - val_loss: 0.0737 - val_accuracy: 0.9703 - lr: 1.0000e-05\n",
            "Epoch 100/100\n",
            "F1 Macro Score: 0.93929\n",
            "54/54 - 7s - loss: 0.0830 - accuracy: 0.9678 - val_loss: 0.0738 - val_accuracy: 0.9704 - lr: 1.0000e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1c042372b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNU-xIU-OqHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "te_preds = model.predict(test_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUxqOglCr0CJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "te_preds = te_preds.reshape(-1, te_preds.shape[-1]) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zUuq8SSEB05",
        "colab_type": "code",
        "outputId": "a1c12a26-3741-403b-b363-67e2e13f1267",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "submission['open_channels'] = np.argmax(te_preds, axis = 1).astype(int)\n",
        "submission.open_channels.value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     1196487\n",
              "1      236118\n",
              "3      118943\n",
              "2      114422\n",
              "6       73656\n",
              "5       66673\n",
              "7       63860\n",
              "4       56297\n",
              "8       42769\n",
              "9       23764\n",
              "10       7011\n",
              "Name: open_channels, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D8I0cdFEDx_",
        "colab_type": "text"
      },
      "source": [
        "###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8DCrNUkU0uY",
        "colab_type": "text"
      },
      "source": [
        "# Submtting the Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-29Qre0cxmv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "round_pred = np.round(np.clip(preds, 0, 10)).astype(int)\n",
        "submission['open_channels'] = round_pred\n",
        "submission['open_channels'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKi3VjLKYoTt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission.to_csv('submission_5_fold_lstm_21.csv', index=False, float_format='%.4f')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ng9ELI8AlTnD",
        "colab_type": "code",
        "outputId": "db93c5c1-b661-4796-ecb1-a9a97da5ae0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "!kaggle competitions submit -c liverpool-ion-switching -f submission_5_fold_lstm_21.csv -m \" \""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "100% 21.0M/21.0M [00:09<00:00, 2.31MB/s]\n",
            "Successfully submitted to University of Liverpool - Ion Switching"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wX5aoY5FUv3C",
        "colab_type": "text"
      },
      "source": [
        "# Feature Importance Scale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suVlLZNZAiZE",
        "colab_type": "text"
      },
      "source": [
        "# ANALYSIS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rn3bDnsFBz2W",
        "colab_type": "text"
      },
      "source": [
        "#### For Train Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luKG_tBwbLS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.head()\n",
        "train2 = train.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOzcRoLqcZup",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for x in train.group.unique():\n",
        "  plt.plot(train[train['group'] == x]['signal'][::100],'.',label=f'signal group {x}')\n",
        "plt.plot(train['group'],label='groups',color='black')\n",
        "for i in range(10):\n",
        "  plt.plot([i*5e5,i*5e5],[-4,12],'r')\n",
        "for i in range(10):\n",
        "  plt.text(i*5e5+2e5,10,str(i+1),size=20)\n",
        "plt.yticks(np.linspace(-4,12,17))\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcyMClBjbLIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "plt.figure(figsize=(19,6))\n",
        "res = 1000\n",
        "plt.plot(range(0,train.shape[0],res),train['open_channels'][0::res])\n",
        "plt.yticks([0,1,2,3,4,5,6,7,8,9,10])\n",
        "for i in range(10):\n",
        "  plt.plot([i*5e5,i*5e5],[0,10],'r')\n",
        "for i in range(10):\n",
        "  plt.text(i*500000+200000,10,str(i+1),size=20)\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_9Cbb8YbK_y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "plt.figure(figsize=(19,5)) \n",
        "res = 1000\n",
        "plt.plot(range(0,train.shape[0],res),train['signal'][0::res])\n",
        "for i in range(10):\n",
        "  plt.plot([i*500000,i*500000],[-4,12],'r')\n",
        "for j in range(10):\n",
        "    plt.plot([j*100000,j*100000],[-4,12],'r:')\n",
        "for i in range(10):\n",
        "  plt.text(i*500000+200000,10,str(i+1),size=20)\n",
        "plt.xticks(np.linspace(0,5e6,11))\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfjwV1bDEG5F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1 = train[train['signal'] == -1.817]\n",
        "df2 = train[train['signal'] == 3.186]\n",
        "df3 = train[train['signal'] == 325]\n",
        "\n",
        "plt.plot(train['signal'][::1000],alpha=1)\n",
        "plt.plot(df1['signal'],alpha=1,label='df1')\n",
        "plt.plot(df2['signal'],alpha=1,label='df2')\n",
        "plt.plot(df3['signal'],alpha=1,label='df3')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oa-jAltv5aYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "slope, intercept, rval, pval, stderr = stats.linregress(train['time'][5e5:6e5],np.array(pd.Series(train['signal'][5e5:6e5].rolling(1000).mean()).replace(np.nan,0)))\n",
        "print(slope,\" \",intercept)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6g7Sfubr6q7j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train2 = train.copy()\n",
        "a = 5e5;b1=6e5;b2=1e6\n",
        "train2.loc[a:b1,'signal'] = train2.loc[a:b1,'signal'] - ((np.ceil(slope*10)/10)*(train2.loc[a:b1,'time']) - 15)\n",
        "plt.figure(figsize=(19,5))\n",
        "plt.plot(train2.loc[a:b2,'signal'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcE0nUZI5aMB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(19,5))\n",
        "plt.title(\"Signals with Drift in Batch 2\",color='white',size=20)\n",
        "plt.plot(train['signal'][::1000])\n",
        "for i in range(10):\n",
        "  plt.plot([i*5e5,i*5e5],[-4,12],'r')\n",
        "\n",
        "plt.figure(figsize=(19,5))\n",
        "plt.title(\"Signals without Drift in Batch 2\",color='white',size=20)\n",
        "plt.plot(train2['signal'][::1000])\n",
        "for i in range(10):\n",
        "  plt.plot([i*5e5,i*5e5],[-4,12],'r')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIn7esT0D17l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(train[train['batch'] == 6]['signal'][::100].rolling(1000).mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZm_jtzEzeUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_c = pd.read_csv('train_clean.csv')\n",
        "train_c = get_batch(train_c)\n",
        "train_c.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHvVAGHCy-5v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.distplot(train[train['batch'] == 6]['signal'],label='not cleaned')\n",
        "sns.distplot(train_c[train_c['batch'] == 6]['signal'],label='cleaned')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNk7Lcm9_rCS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "for i in [0,3,4,5,6]:\n",
        "  sns.distplot(train[train['batch'] == i]['signal'][::1000],label=f'batch {i}')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qowCoCb-xWxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.distplot(train_c['signal'],label='cleaned')\n",
        "sns.distplot(train['signal'],label='not cleaned')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeDXbVAUwHwY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train['drift_dis'] = train['signal'] - train_c['signal']\n",
        "plt.plot(train_c['signal'][::10000])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfVdNC8QwHta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(50,5))\n",
        "sns.scatterplot(train.index,train['open_channels'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vp5TdDb_PQOJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('train_clean.csv')\n",
        "test = pd.read_csv('test_clean.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDRKvnffPQL7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = get_batch(train)\n",
        "test = get_batch(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLgi479APQJZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vals_gauss = signal.gausspulse(np.array(train[train['batch'] == 3]['signal'][::100]))\n",
        "vals_org = train[train['batch'] == 3]['signal'][::100].values\n",
        "vals_chnls = train[train['batch'] == 3]['open_channels'][::100].values\n",
        "plt.plot(vals_org,label='org data')\n",
        "plt.plot(vals_gauss,label='gausspulse')\n",
        "plt.plot(vals_chnls,label='channels')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoQLkDM6eWfs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#vals = np.array(train['signal']) * 0.3\n",
        "plt.plot(train['signal'],'o',label='current')\n",
        "plt.plot(vals,'^',label='volatge')\n",
        "plt.plot(train['open_channels'],label='channels')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yg4lmtXJeWdF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(train['signal'],label='signal')\n",
        "plt.plot(train['open_channels'],label='channels')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyjAg52veWaw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(train['signal'],'+',label='signal')\n",
        "plt.plot(train['open_channels'],'.',label='channels')\n",
        "plt.xticks(np.linspace(0,5e6,21))\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kH9GE1GceWYJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(train['signal'][::1000])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZSiuSDoPQGg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = np.sqrt(((4*1.308*10e-23)*230*10*10e3/10e9))\n",
        "train['c'] = a/train['signal']\n",
        "plt.plot(train['c'],label='noise')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHlQFN2HjrY4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(11):\n",
        "  print(i,\" => \",train[train['batch'] == i]['signal'].mean(),train[train['batch'] == i]['signal'].std())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mqv44gu_jrKM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(train[train['batch'] == 3]['signal'][::100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N58GPan8dLU0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i=7\n",
        "plt.scatter(np.roll(train[train['batch'] == i]['signal'],-1),train[train['batch'] == i]['signal'],s=0.01,label=f'signal {i}')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "print(train[train['batch'] == i]['open_channels'].value_counts(normalize=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTAahx9qitu1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(train[train['batch'] == 7]['signal'],label='signal')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtpsvANiwsYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def smooth(x,window_len=11,window='hanning'):\n",
        "    s=np.r_[x[window_len-1:0:-1],x,x[-2:-window_len-1:-1]]\n",
        "    #print(len(s))\n",
        "    if window == 'flat': #moving average\n",
        "        w=np.ones(window_len,'d')\n",
        "    else:\n",
        "        w=eval('np.'+window+'(window_len)')\n",
        "\n",
        "    y=np.convolve(w/w.sum(),s,mode='valid')\n",
        "    return y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnQHUbkG2tGi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = train.signal.shift(-1) - train.signal"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqNsPARZ9rgU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "0.19*np.sqrt(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WanZvJgX9bzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train['signal'][:10]\n",
        "a[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7V6n1BeN0z5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = np.where(train['signal']<0,train['signal'] + 0.19,train['signal'] - 0.19)\n",
        "plt.plot(train.signal)\n",
        "plt.figure(figsize=(19,5))\n",
        "plt.plot(a[::1000])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klXdC5ybwsVY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(train['signal'][:500].rolling(50).mean(),label='original')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwW365uZDxvK",
        "colab_type": "text"
      },
      "source": [
        "#### For Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIXRxNwdpaoY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test2 = test.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCNrShjE5aHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(20,5))\n",
        "let = ['A','B','C','D','E','F','G','H','I','J']\n",
        "r = test.signal.rolling(30000).mean()\n",
        "plt.plot(test.time.values,r)\n",
        "for i in range(21): plt.plot([500+i*10,500+i*10],[-3,6],'r:')\n",
        "for i in range(5): plt.plot([500+i*50,500+i*50],[-3,6],'r')\n",
        "for k in range(4): plt.text(525+k*50,5.5,str(k+1),size=20)\n",
        "for k in range(10): plt.text(505+k*10,4,let[k],size=16)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uBhZbAro1dg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = 0;b=100000\n",
        "slope, intercept, _, _, _ = stats.linregress(test.loc[test.index[a:b],'time'], test2.signal.values[a:b])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dcm74wUzqXTJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test2.loc[a:b,'signal'] = test2.loc[a:b,'signal'] - (np.round(slope,2)*test2.loc[a:b,'time']) + intercept\n",
        "plt.plot(test2['signal'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7iLbF6-5aDc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(test['signal'].rolling(1000).mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqnledmPB4tv",
        "colab_type": "text"
      },
      "source": [
        "# Garbage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P_apo1V9gLj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "\"\"\"def get_batch_segmented_features(df):\n",
        "\n",
        "  df['whole_signal_lag_1'] = df['signal'].shift(1)\n",
        "  df['whole_signal_lag_2'] = df['signal'].shift(2)\n",
        "  df['whole_signal_lag_3'] = df['signal'].shift(3)\n",
        "  df['whole_signal_lead_1'] = df['signal'].shift(-1)\n",
        "  df['whole_signal_lead_2'] = df['signal'].shift(-2)\n",
        "  df['whole_signal_lead_3'] = df['signal'].shift(-3)\n",
        "\n",
        "  cols_l = ['whole_signal_lag_1','whole_signal_lag_2','whole_signal_lag_3']\n",
        "  cols_r = ['whole_signal_lead_1','whole_signal_lead_2','whole_signal_lead_3']\n",
        "\n",
        "  df['whole_3feat_left_mean'] = df[['whole_signal_lag_1','whole_signal_lag_2','whole_signal_lag_3']].mean(axis=1)\n",
        "  df['whole_3feat_left_min'] = df[['whole_signal_lag_1','whole_signal_lag_2','whole_signal_lag_3']].min(axis=1)\n",
        "  df['whole_3feat_left_max'] = df[['whole_signal_lag_1','whole_signal_lag_2','whole_signal_lag_3']].max(axis=1)\n",
        "  df['whole_3feat_left_std'] = df[['whole_signal_lag_1','whole_signal_lag_2','whole_signal_lag_3']].std(axis=1)\n",
        "  df['whole_3feat_left_var'] = df[['whole_signal_lag_1','whole_signal_lag_2','whole_signal_lag_3']].var(axis=1)\n",
        "  df['whole_3feat_left_median'] = df[['whole_signal_lag_1','whole_signal_lag_2','whole_signal_lag_3']].median(axis=1)\n",
        "\n",
        "  df['whole_3feat_right_mean'] = df[['whole_signal_lead_1','whole_signal_lead_2','whole_signal_lead_3']].mean(axis=1)\n",
        "  df['whole_3feat_right_min'] =  df[['whole_signal_lead_1','whole_signal_lead_2','whole_signal_lead_3']].min(axis=1)\n",
        "  df['whole_3feat_right_max'] = df[['whole_signal_lead_1','whole_signal_lead_2','whole_signal_lead_3']].max(axis=1)\n",
        "  df['whole_3feat_right_std'] = df[['whole_signal_lead_1','whole_signal_lead_2','whole_signal_lead_3']].std(axis=1)\n",
        "  df['whole_3feat_right_var'] = df[['whole_signal_lead_1','whole_signal_lead_2','whole_signal_lead_3']].var(axis=1)\n",
        "  df['whole_3feat_right_median'] = df[['whole_signal_lead_1','whole_signal_lead_2','whole_signal_lead_3']].median(axis=1)\n",
        "\n",
        "  df['whole_3feat_lag_lead_mean'] = df[cols_l + cols_r].mean(axis=1)\n",
        "  df['leag_lead_mean_msignal'] = df['whole_3feat_lag_lead_mean'] - df['signal']\n",
        "\n",
        "  for x in [10,25,50,100,500,1000,10000]:\n",
        "    df['whole_rolling_mean'] = df['signal'].rolling(x,min_periods=1).mean()\n",
        "    df['whole_rolling_median'] = df['signal'].rolling(x,min_periods=1).median()\n",
        "    df['whole_rolling_std'] = df['signal'].rolling(x,min_periods=1).std()\n",
        "    df['whole_rolling_var'] = df['signal'].rolling(x,min_periods=1).var()\n",
        "    df['whole_rolling_min'] = df['signal'].rolling(x,min_periods=1).min()\n",
        "    df['whole_rolling_max'] = df['signal'].rolling(x,min_periods=1).max()\n",
        "\n",
        "  for x in tqdm_notebook(['batch','batch_slices']):\n",
        "      df[f'{x}_mean'] = df.groupby(x)['signal'].transform('mean')\n",
        "      df[f'{x}_std'] = df.groupby(x)['signal'].transform('std')\n",
        "      df[f'{x}_min'] = df.groupby(x)['signal'].transform('min')\n",
        "      df[f'{x}_max'] = df.groupby(x)['signal'].transform('max')\n",
        "      df[f'{x}_variance'] = df.groupby(x)['signal'].transform('var')\n",
        "      df[f'{x}_mean_abs_dev'] = df.groupby(x)['signal'].transform(mean_abs_dev)\n",
        "      df[f'{x}_mean_abs_chg'] = df.groupby(x)['signal'].transform(lambda x:np.mean(np.abs(np.diff(x))))\n",
        "      df[f'{x}_shift_lag_1'] = df.groupby(x)['signal'].transform(lambda x:x.shift(-1))\n",
        "      df[f'{x}_shift_lag_2'] = df.groupby(x)['signal'].transform(lambda x:x.shift(-2))\n",
        "      df[f'{x}_shift_lag_3'] = df.groupby(x)['signal'].transform(lambda x:x.shift(-3))\n",
        "      df[f'{x}_shift_lead_1'] = df.groupby(x)['signal'].transform(lambda x:x.shift(1))\n",
        "      df[f'{x}_shift_lead_2'] = df.groupby(x)['signal'].transform(lambda x:x.shift(2))\n",
        "      df[f'{x}_shift_lead_3'] = df.groupby(x)['signal'].transform(lambda x:x.shift(3))\n",
        "      df[f'{x}_min_max_ratio'] = df[f'{x}_max']/df[f'{x}_min']\n",
        "\n",
        "      for i in df.columns[-14:]:\n",
        "        df[f'{i}_msignal'] = df[i] - df['signal']\n",
        "\n",
        "      df[f'{x}_roll10_q10'] = df.groupby(x)['signal'].transform(lambda x:pd.Series(x).rolling(10,min_periods=1).quantile(0.1))\n",
        "      df[f'{x}_roll10_q20'] = df.groupby(x)['signal'].transform(lambda x:pd.Series(x).rolling(10,min_periods=1).quantile(0.2))\n",
        "      df[f'{x}_roll10_q80'] = df.groupby(x)['signal'].transform(lambda x:pd.Series(x).rolling(10,min_periods=1).quantile(0.8))\n",
        "      df[f'{x}_roll10_q90'] = df.groupby(x)['signal'].transform(lambda x:pd.Series(x).rolling(10,min_periods=1).quantile(0.9))\n",
        "\n",
        "      df[f'{x}_roll20_q10'] = df.groupby(x)['signal'].transform(lambda x:pd.Series(x).rolling(20,min_periods=1).quantile(0.1))\n",
        "      df[f'{x}_roll20_q20'] = df.groupby(x)['signal'].transform(lambda x:pd.Series(x).rolling(20,min_periods=1).quantile(0.2))\n",
        "      df[f'{x}_roll20_q80'] = df.groupby(x)['signal'].transform(lambda x:pd.Series(x).rolling(20,min_periods=1).quantile(0.8))\n",
        "      df[f'{x}_roll20_q90'] = df.groupby(x)['signal'].transform(lambda x:pd.Series(x).rolling(20,min_periods=1).quantile(0.9))\n",
        "\n",
        "      df[f'{x}_roll10_mean'] = df.groupby(x)['signal'].transform(lambda x:pd.Series(x).rolling(10,min_periods=1).quantile(0.5))\n",
        "      df[f'{x}_roll20_mean'] = df.groupby(x)['signal'].transform(lambda x:pd.Series(x).rolling(20,min_periods=1).quantile(0.5))\n",
        "\n",
        "      df[f'{x}_roll10_std'] = df.groupby(x)['signal'].transform(lambda x:pd.Series(x).rolling(10,min_periods=1).std())\n",
        "      df[f'{x}_roll20_std'] = df.groupby(x)['signal'].transform(lambda x:pd.Series(x).rolling(20,min_periods=1).std())\n",
        "      \n",
        "      df[f'{x}_roll10_variance'] = df.groupby(x)['signal'].transform(lambda x:pd.Series(x).rolling(10,min_periods=1).var())\n",
        "      df[f'{x}_roll10_variance'] = df.groupby(x)['signal'].transform(lambda x:pd.Series(x).rolling(20,min_periods=1).var())\n",
        "\n",
        "      df[f'{x}_roll10_min'] = df.groupby(x)['signal'].transform(lambda x:pd.Series(x).rolling(10,min_periods=1).min())\n",
        "      df[f'{x}_roll20_min'] = df.groupby(x)['signal'].transform(lambda x:pd.Series(x).rolling(20,min_periods=1).min())\n",
        "\n",
        "      df[f'{x}_roll10_max'] = df.groupby(x)['signal'].transform(lambda x:pd.Series(x).rolling(10,min_periods=1).max())\n",
        "      df[f'{x}_roll20_max'] = df.groupby(x)['signal'].transform(lambda x:pd.Series(x).rolling(20,min_periods=1).max())\n",
        "\n",
        "      df[f'{x}_roll10_min_max_range'] = df[f'{x}_roll10_max'] - df[f'{x}_roll10_min']\n",
        "      df[f'{x}_roll20_min_max_range'] = df[f'{x}_roll20_max'] - df[f'{x}_roll20_min']\n",
        "      df[f'{x}_roll10_min_max_avg'] = (df[f'{x}_roll10_max'] + df[f'{x}_roll10_min'])/2.0\n",
        "      df[f'{x}_roll20_min_max_avg'] = (df[f'{x}_roll20_max'] + df[f'{x}_roll20_min'])/2.0\n",
        "\n",
        "      df[f'{x}_seg_1'] = df.groupby(x)['signal'].transform(lambda x:np.quantile(x,0.0001))\n",
        "      df[f'{x}_seg_9'] = df.groupby(x)['signal'].transform(lambda x:np.quantile(x,0.9999))\n",
        "      df[f'{x}_range_1to9'] = df[f'{x}_seg_9'] - df[f'{x}_seg_1']\n",
        "      df[f'{x}_seg_5'] = (df[f'{x}_seg_1'] + df[f'{x}_seg_9'])/2.0 \n",
        "      df[f'{x}_seg_3'] = (df[f'{x}_seg_5'] + df[f'{x}_seg_1'])/2.0\n",
        "      df[f'{x}_seg_7'] = (df[f'{x}_seg_9'] + df[f'{x}_seg_5'])/2.0\n",
        "      df[f'{x}_seg_4'] = (df[f'{x}_seg_5'] + df[f'{x}_seg_3'])/2.0\n",
        "      df[f'{x}_seg_2'] = (df[f'{x}_seg_3'] + df[f'{x}_seg_1'])/2.0\n",
        "      df[f'{x}_seg_6'] = (df[f'{x}_seg_5'] + df[f'{x}_seg_7'])/2.0\n",
        "      df[f'{x}_seg_8']  = (df[f'{x}_seg_7'] + df[f'{x}_seg_9'])/2.0\n",
        "\n",
        "  for x in df.columns:\n",
        "    if 'whole' in x:\n",
        "      df[f'{x}_msignal'] = df[x] - df['signal']\n",
        "\n",
        "  return df\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nai5cjVJLveF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "\"\"\"def get_group_channel_agg(train,test):\n",
        "  groups = train.group.unique().tolist()\n",
        "  groups.sort()\n",
        "  group_max = {}\n",
        "  group_min = {}\n",
        "  group_mean = {}\n",
        "  group_std = {}\n",
        "  group_var = {}\n",
        "  for i in tqdm_notebook(groups):\n",
        "    group_mean[i] = {}\n",
        "    group_mean[i].update(train[train['group'] == i].groupby('open_channels')['signal'].mean().to_dict())\n",
        "\n",
        "    group_max[i] = {}\n",
        "    group_max[i].update(train[train['group'] == i].groupby('open_channels')['signal'].max().to_dict())\n",
        "\n",
        "    group_min[i] = {}\n",
        "    group_min[i].update(train[train['group'] == i].groupby('open_channels')['signal'].min().to_dict())\n",
        "\n",
        "    group_std[i] = {}\n",
        "    group_std[i].update(train[train['group'] == i].groupby('open_channels')['signal'].std().to_dict())\n",
        "\n",
        "    group_var[i] = {}\n",
        "    group_var[i].update(train[train['group'] == i].groupby('open_channels')['signal'].var().to_dict())\n",
        "\n",
        "  for df in tqdm_notebook([train,test]):\n",
        "    for x in groups:\n",
        "      df[f'group_{x}_channel_sig_mean'] = df['group'].map(group_mean[x])\n",
        "      df[f'group_{x}_channel_sig_max'] = df['group'].map(group_max[x])\n",
        "      df[f'group_{x}_channel_sig_min'] = df['group'].map(group_min[x])\n",
        "      df[f'group_{x}_channel_sig_std'] = df['group'].map(group_std[x])\n",
        "      df[f'group_{x}_channel_sig_var'] = df['group'].map(group_mean[x])\n",
        "\n",
        "  return train, test\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Whw7T4R-foD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "\"\"\"for x in tqdm_notebook(train.columns[6:57]):\n",
        "  train[f'{x}_msignal'] = train[x] - train['signal']\n",
        "\n",
        "for x in tqdm_notebook(train.columns[6:57]):\n",
        "  test[f'{x}_msignal'] = test[x] - test['signal']\n",
        "\n",
        "for x in tqdm_notebook(train.columns[57:108]):\n",
        "  train[f'{x}_msignal'] = train[x] - train['signal']\n",
        "\n",
        "for x in tqdm_notebook(train.columns[57:108]):\n",
        "  test[f'{x}_msignal'] = test[x] - test['signal']\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoiqMIRQbK-S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"def get_features(df):\n",
        "    df['rolling_signal_mean'] = df['signal'].rolling(10,min_periods=1).mean()\n",
        "    df['rolling_signal_median'] = df['signal'].rolling(10,min_periods=1).median()\n",
        "\n",
        "    for x in tqdm_notebook(['batch','batch_slices2']):\n",
        "      sig_mean = df.groupby(x)['signal'].mean().to_dict()\n",
        "      sig_median = df.groupby(x)['signal'].median().to_dict()\n",
        "      sig_std = df.groupby(x)['signal'].std().to_dict()\n",
        "      sig_max = df.groupby(x)['signal'].max().to_dict()\n",
        "      sig_min = df.groupby(x)['signal'].min().to_dict()\n",
        "      sig_mean_abs_diff = df.groupby(x)['signal'].apply(lambda x: np.mean(np.absolute(np.diff(x)))).to_dict()\n",
        "      sig_mean_non_abs_diff = df.groupby(x)['signal'].apply(lambda x:np.mean(np.diff(x))).to_dict()\n",
        "      sig_skew = df.groupby(x)['signal'].skew().to_dict()\n",
        "      sig_kurt = df.groupby(x)['signal'].apply(pd.DataFrame.kurt).to_dict()\n",
        "      sig_kurt_zscore = {x:y[0] for (x,y) in df.groupby(x)['signal'].apply(stats.kurtosistest).to_dict().items()}\n",
        "      sig_kurt_pvalue = {x:y[0] for (x,y) in df.groupby(x)['signal'].apply(stats.kurtosistest).to_dict().items()}\n",
        "      sig_sem = df.groupby(x)['signal'].apply(stats.sem).to_dict()\n",
        "      sig_skew_zscore = {x:y[0] for (x,y) in df.groupby(x)['signal'].apply(stats.skewtest).to_dict().items()}\n",
        "      sig_skew_pvalue = {x:y[1] for (x,y) in df.groupby(x)['signal'].apply(stats.skewtest).to_dict().items()}\n",
        "      sig_zscore_mean = {x:np.mean(y) for (x,y) in df.groupby(x)['signal'].apply(stats.zscore).to_dict().items()}\n",
        "      sig_zscore_max = {x:np.max(y) for (x,y) in df.groupby(x)['signal'].apply(stats.zscore).to_dict().items()}\n",
        "      sig_zscore_min = {x:np.min(y) for (x,y) in df.groupby(x)['signal'].apply(stats.zscore).to_dict().items()}\n",
        "      sig_zscore_std = {x:np.std(y) for (x,y) in df.groupby(x)['signal'].apply(stats.zscore).to_dict().items()}\n",
        "      sig_zscore_var = {x:np.var(y) for (x,y) in df.groupby(x)['signal'].apply(stats.zscore).to_dict().items()}\n",
        "      sig_zscore_median = {x:np.median(y) for (x,y) in df.groupby(x)['signal'].apply(stats.zscore).to_dict().items()}\n",
        "      sig_relfreq_freq_mean = {x:np.mean(y[0]) for (x,y) in df.groupby(x)['signal'].apply(stats.relfreq).to_dict().items()}\n",
        "      sig_relfreq_freq_median = {x:np.mean(y[0]) for (x,y) in df.groupby(x)['signal'].apply(stats.relfreq).to_dict().items()}\n",
        "      sig_relfreq_freq_max = {x:np.mean(y[0]) for (x,y) in df.groupby(x)['signal'].apply(stats.relfreq).to_dict().items()}\n",
        "      sig_relfreq_freq_min = {x:np.mean(y[0]) for (x,y) in df.groupby(x)['signal'].apply(stats.relfreq).to_dict().items()}\n",
        "      sig_relfreq_freq_std = {x:np.mean(y[0]) for (x,y) in df.groupby(x)['signal'].apply(stats.relfreq).to_dict().items()}\n",
        "      sig_relfreq_freq_var = {x:np.mean(y[0]) for (x,y) in df.groupby(x)['signal'].apply(stats.relfreq).to_dict().items()}\n",
        "      sig_relfreq_lower_limit = {x:y[1] for (x,y) in df.groupby(x)['signal'].apply(stats.relfreq).to_dict().items()}\n",
        "      sig_relfreq_binsize = {x:y[2] for (x,y) in df.groupby(x)['signal'].apply(stats.relfreq).to_dict().items()}\n",
        "\n",
        "      sig_variance = df.groupby(x)['signal'].var().to_dict()\n",
        "\n",
        "      if x is not 'batch':\n",
        "        sig_mad = df.groupby(x)['signal'].apply(mean_abs_dev).to_dict()\n",
        "        sig_sum = df.groupby(x)['signal'].sum().to_dict()\n",
        "        df[f'{x}_sig_mad'] = df[x].map(sig_mad)\n",
        "        df[f'{x}_sig_sum'] = df[x].map(sig_sum)\n",
        "\n",
        "      df[f'{x}_sig_mean'] =  df[x].map(sig_mean)\n",
        "      df[f'{x}_sig_median'] =  df[x].map(sig_median)\n",
        "      df[f'{x}_sig_std'] = df[x].map(sig_std)\n",
        "      df[f'{x}_sig_max'] =  df[x].map(sig_max)\n",
        "      df[f'{x}_sig_min'] =  df[x].map(sig_min)\n",
        "      df[f'{x}_sig_abs_max'] = 0 - df[f'{x}_sig_min']\n",
        "      df[f'{x}_sig_abs_min'] = 0 - df[f'{x}_sig_max']\n",
        "      df[f'{x}_sig_mean_abs_diff'] = df[x].map(sig_mean_abs_diff)\n",
        "      df[f'{x}_sig_mean_non_abs_diff'] = df[x].map(sig_mean_non_abs_diff)\n",
        "      df[f'{x}_range'] = df[f'{x}_sig_max'] - df[f'{x}_sig_min']\n",
        "      df[f'{x}_max_by_min'] = df[f'{x}_sig_max'] / df[f'{x}_sig_min']\n",
        "      df[f'{x}_abs_min_max_avg'] = (df[f'{x}_sig_abs_max'] + df[f'{x}_sig_abs_min'])/2.0\n",
        "      df[f'{x}_min_max_avg'] = (df[f'{x}_sig_max'] + df[f'{x}_sig_min'])/2.0\n",
        "      df[f'{x}_sig_shift_pos'] = df.groupby(x)['signal'].shift()\n",
        "      df[f'{x}_sig_shift_neg'] = df.groupby(x)['signal'].shift(-1)\n",
        "      df[f'{x}_max_to_abs_min_diff'] = df[f'{x}_sig_max'] - np.absolute(df[f'{x}_sig_min'])\n",
        "      df[f'{x}_sig_kurtosis'] = df[x].map(sig_kurt)\n",
        "      df[f'{x}_sig_skew'] = df[x].map(sig_skew)\n",
        "      df[f'{x}_sig_kurt_zscore'] = df[x].map(sig_kurt_zscore)\n",
        "      df[f'{x}_sig_kurt_pvalue'] = df[x].map(sig_kurt_pvalue)\n",
        "      df[f'{x}_sig_skew_zscore'] = df[x].map(sig_skew_zscore)\n",
        "      df[f'{x}_sig_skew_pvalue'] = df[x].map(sig_skew_pvalue)\n",
        "      df[f'{x}_sig_sem'] = df[x].map(sig_sem)\n",
        "      df[f'{x}_sig_zscore_mean'] = df[x].map(sig_zscore_mean)\n",
        "      df[f'{x}_sig_zscore_min'] = df[x].map(sig_zscore_min)\n",
        "      df[f'{x}_sig_zscore_max'] = df[x].map(sig_zscore_max)\n",
        "      df[f'{x}_sig_zscore_median'] = df[x].map(sig_zscore_median)\n",
        "      df[f'{x}_sig_zscore_std'] = df[x].map(sig_zscore_std)\n",
        "      df[f'{x}_sig_zscore_var'] = df[x].map(sig_zscore_var)\n",
        "      df[f'{x}_sig_relfreq_freq_mean'] = df[x].map(sig_relfreq_freq_mean)\n",
        "      df[f'{x}_sig_relfreq_freq_min'] = df[x].map(sig_relfreq_freq_min)\n",
        "      df[f'{x}_sig_relfreq_freq_max'] = df[x].map(sig_relfreq_freq_max)\n",
        "      df[f'{x}_sig_relfreq_freq_std'] = df[x].map(sig_relfreq_freq_std)\n",
        "      df[f'{x}_sig_relfreq_freq_median'] = df[x].map(sig_relfreq_freq_median)\n",
        "      df[f'{x}_sig_relfreq_freq_var'] = df[x].map(sig_relfreq_freq_var)\n",
        "      df[f'{x}sig_relfreq_lower_limit'] = df[x].map(sig_relfreq_lower_limit)\n",
        "      df[f'{x}_sig_relfreq_binsize'] = df[x].map(sig_relfreq_binsize)\n",
        "\n",
        "      \n",
        "      if x is not 'batch':\n",
        "        df[f'{x}_abs_min_range_from_mean'] = df[f'{x}_sig_abs_min'] - df[f'{x}_sig_mean']\n",
        "        df[f'{x}_abs_max_range_from_mean'] = df[f'{x}_sig_abs_max'] - df[f'{x}_sig_mean']\n",
        "        df[f'{x}_abs_min_range_from_mad'] = df[f'{x}_sig_abs_min'] - df[f'{x}_sig_mad']\n",
        "        df[f'{x}_abs_max_range_from_mad'] = df[f'{x}_sig_abs_max'] - df[f'{x}_sig_mad']\n",
        "\n",
        "\n",
        "    df['signal_rolling_10_sum'] = df['signal'].rolling(10,min_periods=1).sum()\n",
        "    df['signal_rolling_10_mean'] = df['signal'].rolling(10,min_periods=1).mean()\n",
        "    df['signal_shift_pos'] = df['signal'].shift()\n",
        "    df['signal_shift_neg'] = df['signal'].shift(-1)\n",
        "\n",
        "    for c in [c for c in df.columns if c not in ['time','signal','open_channels','batch_index','batch','batch_slices2','batch_slices']]:\n",
        "      df[f'{c}_msignal'] = df[c] - df['signal']\n",
        "    return df\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}